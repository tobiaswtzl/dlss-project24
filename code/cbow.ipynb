{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBoW-Model\n",
    "\n",
    "Implemented with:\n",
    "- Adam\n",
    "- Early-Stopping, Dropout, L2 Reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/tobiaswtzl/dlss-project24/main/data/preprocessed/comments.csv'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'token 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'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = StringIO(response.text)\n",
    "\n",
    "comments = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "#Adding all comments for generating the vocabulary. If not a error occurs when tokens missing\n",
    "total_comments_list = comments[\"cleaned\"].dropna().astype(str).tolist()\n",
    "\n",
    "train_list = train_df[\"cleaned\"].dropna().astype(str).tolist()\n",
    "val_list = val_df[\"cleaned\"].dropna().astype(str).tolist()\n",
    "test_list = test_df[\"cleaned\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>try</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>equivalency</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14815</th>\n",
       "      <td>exorbitant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14816</th>\n",
       "      <td>commercially</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14817</th>\n",
       "      <td>lcoe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14818</th>\n",
       "      <td>troublesome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14819</th>\n",
       "      <td>fraudulent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14820 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Count\n",
       "0              nice     20\n",
       "1               try     97\n",
       "2               but   1350\n",
       "3             false     42\n",
       "4       equivalency      5\n",
       "...             ...    ...\n",
       "14815    exorbitant      1\n",
       "14816  commercially      1\n",
       "14817          lcoe      1\n",
       "14818   troublesome      1\n",
       "14819    fraudulent      1\n",
       "\n",
       "[14820 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure each entry is a string and split each sentence into words\n",
    "total_corpus = [doc.split() for doc in total_comments_list]\n",
    "corpus_train = [doc.split() for doc in train_list]\n",
    "corpus_val = [doc.split() for doc in val_list]\n",
    "corpus_test = [doc.split() for doc in test_list]\n",
    "\n",
    "# Create a vocabulary: count occurrences of each word\n",
    "vocab = defaultdict(int)\n",
    "for sentence in total_corpus:\n",
    "    for word in sentence:\n",
    "        vocab[word] += 1\n",
    "\n",
    "# Remove infrequent words from the vocabulary\n",
    "min_count = 1\n",
    "vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Create DataFrame from vocabulary\n",
    "vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "context_size = 2\n",
    "embedding_dim = 50\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4  # L2 regularization factor\n",
    "patience = 5  # Patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create context-target pairs\n",
    "def create_context_target_pairs(text, context_size):\n",
    "    pairs = []\n",
    "    for sentence in text:\n",
    "        for i in range(context_size, len(sentence) - context_size):\n",
    "            context = sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "            target = sentence[i]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "train_pairs = create_context_target_pairs(corpus_train, context_size)\n",
    "val_pairs = create_context_target_pairs(corpus_val, context_size)\n",
    "test_pairs = create_context_target_pairs(corpus_test, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader definition\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, word_to_index):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.pairs[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_index[word] for word in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n",
    "\n",
    "train_dataset = Word2VecDataset(train_pairs, word_to_index)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = Word2VecDataset(val_pairs, word_to_index)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = Word2VecDataset(test_pairs, word_to_index)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 7.8542, Validation Loss: 7.2085\n",
      "Epoch 2/50, Training Loss: 7.0404, Validation Loss: 6.9796\n",
      "Epoch 3/50, Training Loss: 6.8645, Validation Loss: 6.8621\n",
      "Epoch 4/50, Training Loss: 6.7610, Validation Loss: 6.7855\n",
      "Epoch 5/50, Training Loss: 6.6923, Validation Loss: 6.7309\n",
      "Epoch 6/50, Training Loss: 6.6425, Validation Loss: 6.6910\n",
      "Epoch 7/50, Training Loss: 6.6049, Validation Loss: 6.6620\n",
      "Epoch 8/50, Training Loss: 6.5775, Validation Loss: 6.6396\n",
      "Epoch 9/50, Training Loss: 6.5569, Validation Loss: 6.6209\n",
      "Epoch 10/50, Training Loss: 6.5375, Validation Loss: 6.6073\n",
      "Epoch 11/50, Training Loss: 6.5249, Validation Loss: 6.5963\n",
      "Epoch 12/50, Training Loss: 6.5132, Validation Loss: 6.5864\n"
     ]
    }
   ],
   "source": [
    "# CBOW model definition\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embeds = self.embeddings(context)\n",
    "        combined = torch.mean(embeds, dim=1)\n",
    "        out = self.linear1(self.dropout(combined))\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# Initialize model, loss function, and optimizer with L2 regularization\n",
    "vocab_size = len(word_to_index)\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Lists to store loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for context_idxs, target_idx in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, target_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(total_loss / len(train_dataloader))\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context_idxs, target_idx in val_dataloader:\n",
    "            log_probs = model(context_idxs)\n",
    "            loss = loss_function(log_probs, target_idx)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_losses[-1]:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Display learned word vectors\n",
    "word_embeddings = model.embeddings.weight.data.numpy()\n",
    "for word, idx in word_to_index.items():\n",
    "    print(f\"{word}: {word_embeddings[idx]}\")\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, loss_function):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for context_idxs, target_idx in dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
