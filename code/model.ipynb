{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GKOEX6sL72o8"
      },
      "outputs": [],
      "source": [
        "from github import Github\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from random import shuffle, randint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hKPu-5aT72pA"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/tobiaswtzl/dlss-project24/main/data/preprocessed/comments.csv'\n",
        "\n",
        "headers = {\n",
        "    'Authorization': 'token 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'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "data = StringIO(response.text)\n",
        "\n",
        "comments = pd.read_csv(data)\n",
        "comments = comments[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n4VcRWtS72pB"
      },
      "outputs": [],
      "source": [
        "train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "train_list = train_df[\"cleaned\"].dropna().astype(str).tolist()\n",
        "val_list = val_df[\"cleaned\"].dropna().astype(str).tolist()\n",
        "test_list = test_df[\"cleaned\"].dropna().astype(str).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hfOcQfmD72pC",
        "outputId": "f3a989b7-0fa9-430a-9daf-82837a574b67"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>they</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>just</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>say</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>some</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>innane</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5808</th>\n",
              "      <td>default</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5809</th>\n",
              "      <td>disproportionately</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5810</th>\n",
              "      <td>represent</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5811</th>\n",
              "      <td>managing</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5812</th>\n",
              "      <td>sheesh</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5813 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Word  Count\n",
              "0                   they    255\n",
              "1                   just    120\n",
              "2                    say     39\n",
              "3                   some     68\n",
              "4                 innane      1\n",
              "...                  ...    ...\n",
              "5808             default      1\n",
              "5809  disproportionately      1\n",
              "5810           represent      1\n",
              "5811            managing      1\n",
              "5812              sheesh      1\n",
              "\n",
              "[5813 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensure each entry is a string and split each sentence into words\n",
        "corpus_train = [doc.split() for doc in train_list]\n",
        "corpus_val = [doc.split() for doc in val_list]\n",
        "corpus_test = [doc.split() for doc in test_list]\n",
        "\n",
        "# Create a vocabulary: count occurrences of each word\n",
        "vocab = defaultdict(int)\n",
        "for sentence in corpus_train:\n",
        "    for word in sentence:\n",
        "        vocab[word] += 1\n",
        "\n",
        "# Remove infrequent words from the vocabulary\n",
        "min_count = 1\n",
        "vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
        "\n",
        "# Create word to index and index to word mappings\n",
        "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Create DataFrame from vocabulary\n",
        "vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
        "vocab_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K-Pqhhu-72pE"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "vocab_size = 10000\n",
        "embed_size = 300\n",
        "learning_rate = 0.001\n",
        "window_size = 5\n",
        "neg_samples = 10\n",
        "dropout_rate = 0.5\n",
        "patience = 5\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "l2_lambda = 0.001\n",
        "\n",
        "W1 = np.random.randn(vocab_size, embed_size) * 0.01\n",
        "W2 = np.random.randn(embed_size, vocab_size) * 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vP4_a-Ee72pF"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clipping to prevent overflow\n",
        "\n",
        "def apply_dropout(z, dropout_rate):\n",
        "    mask = np.random.binomial(1, 1 - dropout_rate, size=z.shape)\n",
        "    return z * mask / (1 - dropout_rate)\n",
        "\n",
        "def get_negative_samples(target_idx, vocab_size, neg_samples):\n",
        "    negatives = []\n",
        "    while len(negatives) < neg_samples:\n",
        "        sample = np.random.randint(0, vocab_size)\n",
        "        if sample != target_idx:\n",
        "            negatives.append(sample)\n",
        "    return negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kv3cz_1j72pG"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(corpus, W1, W2, word_to_index, window_size, neg_samples, l2_lambda):\n",
        "    loss = 0\n",
        "    for sentence in corpus:\n",
        "        sentence_indices = [word_to_index[word] for word in sentence if word in word_to_index]\n",
        "        for center_pos in range(len(sentence_indices)):\n",
        "            center_word_idx = sentence_indices[center_pos]\n",
        "            context_indices = list(range(max(0, center_pos - window_size), min(len(sentence_indices), center_pos + window_size + 1)))\n",
        "            context_indices.remove(center_pos)\n",
        "\n",
        "            for context_pos in context_indices:\n",
        "                context_word_idx = sentence_indices[context_pos]\n",
        "                negative_samples = get_negative_samples(center_word_idx, len(word_to_index), neg_samples)\n",
        "                sample_indices = [context_word_idx] + negative_samples\n",
        "\n",
        "                for sample_idx in sample_indices:\n",
        "                    label = 1 if sample_idx == context_word_idx else 0\n",
        "                    z = np.dot(W1[center_word_idx], W2[:, sample_idx])\n",
        "                    prediction = sigmoid(z)\n",
        "                    loss += -np.log(prediction + 1e-10) if label == 1 else -np.log(1 - prediction + 1e-10)  # Adding small constant to prevent log(0)\n",
        "\n",
        "    # L2 Regularization\n",
        "    l2_loss = l2_lambda * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "    return loss + l2_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "_YDrm-3t72pH",
        "outputId": "9389c204-39da-485d-afbf-61b0c18cbc8e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 82\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_history, val_loss_history\n\u001b[1;32m---> 82\u001b[0m train_loss_history, val_loss_history \u001b[38;5;241m=\u001b[39m train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs, batch_size, l2_lambda)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Plot the loss history\u001b[39;00m\n\u001b[0;32m     85\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss_history, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[8], line 49\u001b[0m, in \u001b[0;36mtrain_word2vec\u001b[1;34m(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs, batch_size, l2_lambda)\u001b[0m\n\u001b[0;32m     47\u001b[0m m_hat_W1 \u001b[38;5;241m=\u001b[39m m_W1[center_word_idx] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     48\u001b[0m v_hat_W1 \u001b[38;5;241m=\u001b[39m v_W1[center_word_idx] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 49\u001b[0m W1[center_word_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m_hat_W1 \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39msqrt(v_hat_W1) \u001b[38;5;241m+\u001b[39m epsilon)\n\u001b[0;32m     51\u001b[0m m_W2[:, sample_idx] \u001b[38;5;241m=\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m m_W2[:, sample_idx] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1) \u001b[38;5;241m*\u001b[39m grad_W2\n\u001b[0;32m     52\u001b[0m v_W2[:, sample_idx] \u001b[38;5;241m=\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m v_W2[:, sample_idx] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2) \u001b[38;5;241m*\u001b[39m (grad_W2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs=50, batch_size=32, l2_lambda=0.001):\n",
        "    vocab_size = len(word_to_index)\n",
        "    train_loss_history = []  # List to store training loss at each epoch\n",
        "    val_loss_history = []  # List to store validation loss at each epoch\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss\n",
        "    no_improvement_epochs = 0  # Count epochs with no improvement\n",
        "\n",
        "    # Adam optimizer variables\n",
        "    m_W1, v_W1 = np.zeros_like(W1), np.zeros_like(W1)\n",
        "    m_W2, v_W2 = np.zeros_like(W2), np.zeros_like(W2)\n",
        "    beta1, beta2 = 0.9, 0.999\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        shuffle(corpus_train)  # Shuffle the corpus at the beginning of each epoch\n",
        "        train_loss = 0  # Initialize training loss for the epoch\n",
        "        batch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for sentence in corpus_train:\n",
        "            sentence_indices = [word_to_index[word] for word in sentence if word in word_to_index]\n",
        "            for center_pos in range(len(sentence_indices)):\n",
        "                center_word_idx = sentence_indices[center_pos]\n",
        "                context_indices = list(range(max(0, center_pos - window_size), min(len(sentence_indices), center_pos + window_size + 1)))\n",
        "                context_indices.remove(center_pos)\n",
        "\n",
        "                for context_pos in context_indices:\n",
        "                    context_word_idx = sentence_indices[context_pos]\n",
        "                    negative_samples = get_negative_samples(center_word_idx, vocab_size, neg_samples)\n",
        "                    sample_indices = [context_word_idx] + negative_samples\n",
        "\n",
        "                    for sample_idx in sample_indices:\n",
        "                        label = 1 if sample_idx == context_word_idx else 0\n",
        "                        z = np.dot(W1[center_word_idx], W2[:, sample_idx])\n",
        "                        z = apply_dropout(z, dropout_rate)  # Apply dropout\n",
        "                        prediction = sigmoid(z)\n",
        "                        error = label - prediction\n",
        "                        batch_loss += -np.log(prediction + 1e-10) if label == 1 else -np.log(1 - prediction + 1e-10)  # Adding small constant to prevent log(0)\n",
        "\n",
        "                        # Gradient update for weights with clipping and L2 regularization\n",
        "                        grad_W1 = learning_rate * (error * W2[:, sample_idx] + l2_lambda * W1[center_word_idx])\n",
        "                        grad_W2 = learning_rate * (error * W1[center_word_idx] + l2_lambda * W2[:, sample_idx])\n",
        "\n",
        "                        # Adam updates\n",
        "                        m_W1[center_word_idx] = beta1 * m_W1[center_word_idx] + (1 - beta1) * grad_W1\n",
        "                        v_W1[center_word_idx] = beta2 * v_W1[center_word_idx] + (1 - beta2) * (grad_W1 ** 2)\n",
        "                        m_hat_W1 = m_W1[center_word_idx] / (1 - beta1 ** (epoch + 1))\n",
        "                        v_hat_W1 = v_W1[center_word_idx] / (1 - beta2 ** (epoch + 1))\n",
        "                        W1[center_word_idx] += m_hat_W1 / (np.sqrt(v_hat_W1) + epsilon)\n",
        "\n",
        "                        m_W2[:, sample_idx] = beta1 * m_W2[:, sample_idx] + (1 - beta1) * grad_W2\n",
        "                        v_W2[:, sample_idx] = beta2 * v_W2[:, sample_idx] + (1 - beta2) * (grad_W2 ** 2)\n",
        "                        m_hat_W2 = m_W2[:, sample_idx] / (1 - beta1 ** (epoch + 1))\n",
        "                        v_hat_W2 = v_W2[:, sample_idx] / (1 - beta2 ** (epoch + 1))\n",
        "                        W2[:, sample_idx] += m_hat_W2 / (np.sqrt(v_hat_W2) + epsilon)\n",
        "\n",
        "                # Accumulate loss for each batch\n",
        "                batch_count += 1\n",
        "                if batch_count == batch_size:\n",
        "                    train_loss += batch_loss / batch_count\n",
        "                    batch_loss = 0\n",
        "                    batch_count = 0\n",
        "\n",
        "        train_loss_history.append(train_loss)  # Append training loss for the current epoch\n",
        "        val_loss = calculate_loss(corpus_val, W1, W2, word_to_index, window_size, neg_samples, l2_lambda)\n",
        "        val_loss_history.append(val_loss)  # Append validation loss for the current epoch\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improvement_epochs = 0\n",
        "        else:\n",
        "            no_improvement_epochs += 1\n",
        "            if no_improvement_epochs >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "    return train_loss_history, val_loss_history\n",
        "\n",
        "train_loss_history, val_loss_history = train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs, batch_size, l2_lambda)\n",
        "\n",
        "# Plot the loss history\n",
        "plt.plot(train_loss_history, label='Training Loss')\n",
        "plt.plot(val_loss_history, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hve2N_aU72pI",
        "outputId": "c8f527b8-4cb6-4cb5-a3b5-d6c7aa0baa46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector for 'love': [ 632.3732819  -633.27161628 -631.36130629  633.79216797 -631.2661538\n",
            " -631.29054505 -632.1291488   632.84136825 -631.25651639  632.71765065\n",
            "  632.54329235  631.16959243  633.45661188  633.78401374 -633.51973249\n",
            " -633.46714929 -632.31690307 -633.47623913  632.21367251  633.34798295\n",
            "  633.43824307  633.7511362  -633.30515941  632.79675585  632.759291\n",
            " -631.98761615  632.35947985  632.32696914 -633.47292761 -632.11616634\n",
            " -633.31473216 -631.26702077  632.50529146 -626.14637459  633.44448076\n",
            " -633.51743789  631.94822247  626.03582568 -631.75603269  632.56626453\n",
            " -631.14617778 -632.86087649 -632.64333343  632.52082363  632.74758635\n",
            " -631.59211876  632.77809596  632.49950361 -631.31952806  631.35954387\n",
            " -632.10740645  632.72521974  632.94882313 -632.89872175 -632.85545464\n",
            "  632.87771541 -632.86904279  632.88401365  631.20924529  632.86840385\n",
            "  631.98464902  633.15744071  633.58100799  633.34562819 -632.21782814\n",
            "  633.01688175 -631.98787212 -632.86890913  631.92608218 -632.82135736\n",
            "  633.17655466  633.04676575  633.36722827 -632.63320292  632.26299797\n",
            " -632.81501763 -631.84918388 -632.34657796  631.5824462   632.78233636\n",
            " -631.89692451 -629.99731571 -631.80319748 -632.13034285  633.10018085\n",
            "  626.55436936 -632.84593842  633.00446668  632.44311521 -632.76551161\n",
            " -632.79996396 -632.47422871  632.68058967  632.79695197 -632.93705289\n",
            "  632.27532771  632.7806461  -631.68351165 -632.56698341 -632.89491174\n",
            " -633.50831281 -631.74093486  631.38173493  627.00954186  632.60491595\n",
            " -632.7628129  -632.41533053 -633.26287654 -633.84374172 -633.16040934\n",
            "  632.63370076 -631.5945327   633.85535994 -631.29130667  632.53920332\n",
            "  631.82483269  632.61593245  633.28887037  632.17435696 -631.43155025\n",
            "  631.52478844  633.3204566   633.22076895  632.18782289  633.25075323\n",
            "  632.65006984 -629.86694656 -632.72829271  632.90952589 -631.47332052\n",
            " -632.47689668  630.18250613  632.82378071  633.33516432 -633.404504\n",
            " -632.87809036  633.40109979  633.06775153 -631.66165621  632.66999346\n",
            "  632.99481852 -632.84490258 -633.22146022  626.19959999 -631.77966815\n",
            " -633.52656021 -632.66473781  633.03954046  633.65477718 -632.50957208\n",
            " -633.47001299  632.60052545 -632.16158279  632.31606084 -633.28597406\n",
            " -631.03789054 -632.82437591 -632.69933771 -632.8167885   633.24071009\n",
            " -632.73667872 -633.33420096 -631.01768926 -632.30335058  633.79388148\n",
            " -632.78104034 -633.53842577 -633.37819341  632.20104259  632.53010131\n",
            "  633.27783832  633.51447411 -633.56232059 -633.67379668 -632.27623299\n",
            "  632.94296014  633.44567653  633.87376655 -631.42859352 -632.45663531\n",
            "  632.57268051 -632.7399637   633.11490216 -631.37481134  632.58009463\n",
            "  633.41749465 -632.6532941  -632.96297782 -633.39279607  630.81014275\n",
            " -633.4608993   632.25248727 -633.4831698  -627.26630409 -631.73802924\n",
            " -632.89109901  632.91944412  631.34813378  633.41855996  627.08816592\n",
            " -631.22944841 -632.60213357  631.11615546  633.77101476 -633.34466025\n",
            "  633.63777004 -631.03640672  632.77008601 -632.70062842 -632.40155343\n",
            " -633.04786483  632.58374633 -631.58698388  631.45127377  632.26071921\n",
            "  629.80186845  633.24802834 -632.02259541 -633.53940602  633.01296225\n",
            "  632.9636363   632.59400525 -631.90463754 -630.46586804  632.21593592\n",
            " -633.01170102  633.47156866  632.45596658  632.89038232 -632.80386141\n",
            " -632.53356314  632.18350783  632.67514499  633.45144485  632.86304892\n",
            " -632.9068183  -633.30289145 -632.20292382  633.44808643  632.15878177\n",
            "  632.77981656  633.45509669 -633.4803527  -633.10121067 -633.47804496\n",
            " -633.56132221 -632.72523158 -631.71933823  632.78801125  632.57287752\n",
            "  632.77512189 -632.18504065  633.38208142 -631.665307    631.34110837\n",
            "  633.08410944  632.09335874 -632.63978111 -632.67657953  631.67410119\n",
            " -633.16543097  632.87492318 -633.2379324   632.72098291  632.48737938\n",
            " -631.41821342  631.12685024 -633.14835944  632.50906722 -633.14521554\n",
            " -632.98849773 -633.35669996  633.39624247  633.44959399 -633.87880934\n",
            " -632.98892751  633.56120813 -633.33162427 -633.52568923  633.29616313\n",
            "  632.52300644  632.52672517 -633.03995807 -632.53056735 -632.18908121\n",
            " -633.03644022 -632.94987485 -632.70344383  632.78220517  632.65830756\n",
            " -632.94979394 -631.40365032  632.90446134  631.41265402 -632.88432406\n",
            "  632.56815704  632.80839413 -627.34135437  633.36437549  632.99628634]\n"
          ]
        }
      ],
      "source": [
        "# Get the word vector for a specific word\n",
        "def get_word_vector(word):\n",
        "    return W1[word_to_index[word]]\n",
        "\n",
        "# Test the word vector retrieval\n",
        "word = \"love\"\n",
        "print(f\"Vector for '{word}': {get_word_vector(word)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAwzyYqe72pJ",
        "outputId": "dea56399-87ec-41c9-ee23-8be5862b68d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words similar to 'love': [('circle', 0.9999996831122959), ('include', 0.9999996687704977), ('process', 0.9999996620265303), ('army', 0.999999640841594), ('nationalist', 0.9999996380582445)]\n"
          ]
        }
      ],
      "source": [
        "# Function to find similar words to a given word\n",
        "def find_similar_words(word, top_n=5):\n",
        "    word_vector = get_word_vector(word)\n",
        "    similarities = {}\n",
        "    for other_word in word_to_index:\n",
        "        if other_word == word:\n",
        "            continue\n",
        "        other_word_vector = get_word_vector(other_word)\n",
        "        # Cosine similarity between word vectors\n",
        "        similarity = np.dot(word_vector, other_word_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_word_vector))\n",
        "        similarities[other_word] = similarity\n",
        "    # Sort words by similarity and return the top_n most similar words\n",
        "    similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return similar_words[:top_n]\n",
        "\n",
        "# Find and print similar words to \"love\"\n",
        "similar_words = find_similar_words(\"love\")\n",
        "print(f\"Words similar to 'love': {similar_words}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
