{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/tobiaswtzl/dlss-project24/main/data/preprocessed/comments.csv'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'token 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'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = StringIO(response.text)\n",
    "\n",
    "comments = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_list = train_df[\"cleaned\"].dropna().astype(str).tolist()\n",
    "val_list = val_df[\"cleaned\"].dropna().astype(str).tolist()\n",
    "test_list = test_df[\"cleaned\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>here</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>3006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12308</th>\n",
       "      <td>qje</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12309</th>\n",
       "      <td>huntington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12310</th>\n",
       "      <td>exhaustion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>jel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>occasionally</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12313 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Count\n",
       "0             first    109\n",
       "1                no    394\n",
       "2               one    409\n",
       "3              here    166\n",
       "4                is   3006\n",
       "...             ...    ...\n",
       "12308           qje      4\n",
       "12309    huntington      1\n",
       "12310    exhaustion      1\n",
       "12311           jel      1\n",
       "12312  occasionally      1\n",
       "\n",
       "[12313 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure each entry is a string and split each sentence into words\n",
    "corpus_train = [doc.split() for doc in train_list]\n",
    "corpus_val = [doc.split() for doc in val_list]\n",
    "corpus_test = [doc.split() for doc in test_list]\n",
    "\n",
    "# Create a vocabulary: count occurrences of each word\n",
    "vocab = defaultdict(int)\n",
    "for sentence in corpus_train:\n",
    "    for word in sentence:\n",
    "        vocab[word] += 1\n",
    "\n",
    "# Remove infrequent words from the vocabulary\n",
    "min_count = 1\n",
    "vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Create DataFrame from vocabulary\n",
    "vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "vocab_size = len(vocab)  # Number of unique words in the vocabulary\n",
    "embedding_dim = 100  # Dimensionality of the word vectors\n",
    "learning_rate = 0.01  # Learning rate for gradient descent\n",
    "window_size = 2  # Context window size\n",
    "neg_samples = 5  # Number of negative samples\n",
    "dropout_rate = 0.3  # Dropout rate\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Initialize weights: input-to-hidden and hidden-to-output\n",
    "W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
    "W2 = np.random.uniform(-0.8, 0.8, (embedding_dim, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Function to get negative samples for a given target word\n",
    "def get_negative_samples(target, vocab_size, count):\n",
    "    negatives = []\n",
    "    while len(negatives) < count:\n",
    "        neg = randint(0, vocab_size - 1)\n",
    "        if neg != target:  # Ensure the negative sample is not the target word\n",
    "            negatives.append(neg)\n",
    "    return negatives\n",
    "\n",
    "# Function to apply dropout\n",
    "def apply_dropout(vec, rate):\n",
    "    mask = np.random.binomial(1, 1 - rate, size=vec.shape)\n",
    "    return vec * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate loss for a dataset\n",
    "def calculate_loss(corpus, W1, W2, word_to_index, window_size, neg_samples):\n",
    "    loss = 0\n",
    "    for sentence in corpus:\n",
    "        sentence_indices = [word_to_index[word] for word in sentence if word in word_to_index]\n",
    "        for center_pos in range(len(sentence_indices)):\n",
    "            center_word_idx = sentence_indices[center_pos]\n",
    "            context_indices = list(range(max(0, center_pos - window_size), min(len(sentence_indices), center_pos + window_size + 1)))\n",
    "            context_indices.remove(center_pos)\n",
    "            \n",
    "            for context_pos in context_indices:\n",
    "                context_word_idx = sentence_indices[context_pos]\n",
    "                negative_samples = get_negative_samples(center_word_idx, vocab_size, neg_samples)\n",
    "                sample_indices = [context_word_idx] + negative_samples\n",
    "\n",
    "                for sample_idx in sample_indices:\n",
    "                    label = 1 if sample_idx == context_word_idx else 0\n",
    "                    z = np.dot(W1[center_word_idx], W2[:, sample_idx])\n",
    "                    prediction = sigmoid(z)\n",
    "                    loss += -np.log(prediction) if label == 1 else -np.log(1 - prediction)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jannik Wirtheim\\AppData\\Local\\Temp\\ipykernel_11852\\760245316.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "  train_loss += -np.log(prediction) if label == 1 else -np.log(1 - prediction)\n",
      "C:\\Users\\Jannik Wirtheim\\AppData\\Local\\Temp\\ipykernel_11852\\1933298095.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\Jannik Wirtheim\\AppData\\Local\\Temp\\ipykernel_11852\\2498614118.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += -np.log(prediction) if label == 1 else -np.log(1 - prediction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: inf, Val Loss: inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_history, val_loss_history\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Train the Word2Vec model with early stopping and dropout, and get the loss history\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m train_loss_history, val_loss_history \u001b[38;5;241m=\u001b[39m train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Plot the loss history\u001b[39;00m\n\u001b[0;32m     59\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss_history, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 27\u001b[0m, in \u001b[0;36mtrain_word2vec\u001b[1;34m(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample_idx \u001b[38;5;129;01min\u001b[39;00m sample_indices:\n\u001b[0;32m     26\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_idx \u001b[38;5;241m==\u001b[39m context_word_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W1[center_word_idx], W2[:, sample_idx])\n\u001b[0;32m     28\u001b[0m     z \u001b[38;5;241m=\u001b[39m apply_dropout(z, dropout_rate)  \u001b[38;5;66;03m# Apply dropout\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m sigmoid(z)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training function for the Word2Vec model with early stopping and dropout\n",
    "def train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs=50):\n",
    "    vocab_size = len(word_to_index)\n",
    "    train_loss_history = []  # List to store training loss at each epoch\n",
    "    val_loss_history = []  # List to store validation loss at each epoch\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "    no_improvement_epochs = 0  # Count epochs with no improvement\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        shuffle(corpus_train)  # Shuffle the corpus at the beginning of each epoch\n",
    "        train_loss = 0  # Initialize training loss for the epoch\n",
    "        \n",
    "        for sentence in corpus_train:\n",
    "            sentence_indices = [word_to_index[word] for word in sentence if word in word_to_index]\n",
    "            for center_pos in range(len(sentence_indices)):\n",
    "                center_word_idx = sentence_indices[center_pos]\n",
    "                context_indices = list(range(max(0, center_pos - window_size), min(len(sentence_indices), center_pos + window_size + 1)))\n",
    "                context_indices.remove(center_pos)\n",
    "                \n",
    "                for context_pos in context_indices:\n",
    "                    context_word_idx = sentence_indices[context_pos]\n",
    "                    negative_samples = get_negative_samples(center_word_idx, vocab_size, neg_samples)\n",
    "                    sample_indices = [context_word_idx] + negative_samples\n",
    "\n",
    "                    for sample_idx in sample_indices:\n",
    "                        label = 1 if sample_idx == context_word_idx else 0\n",
    "                        z = np.dot(W1[center_word_idx], W2[:, sample_idx])\n",
    "                        z = apply_dropout(z, dropout_rate)  # Apply dropout\n",
    "                        prediction = sigmoid(z)\n",
    "                        error = label - prediction\n",
    "                        train_loss += -np.log(prediction) if label == 1 else -np.log(1 - prediction)\n",
    "\n",
    "                        # Gradient update for weights\n",
    "                        W1[center_word_idx] += learning_rate * error * W2[:, sample_idx]\n",
    "                        W2[:, sample_idx] += learning_rate * error * W1[center_word_idx]\n",
    "\n",
    "        train_loss_history.append(train_loss)  # Append training loss for the current epoch\n",
    "        val_loss = calculate_loss(corpus_val, W1, W2, word_to_index, window_size, neg_samples)\n",
    "        val_loss_history.append(val_loss)  # Append validation loss for the current epoch\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            if no_improvement_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Train the Word2Vec model with early stopping and dropout, and get the loss history\n",
    "train_loss_history, val_loss_history = train_word2vec(corpus_train, corpus_val, word_to_index, index_to_word, W1, W2, learning_rate, window_size, neg_samples, dropout_rate, patience, epochs=100)\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love': [ 0.32967642  0.20962584 -0.19999234  0.50746454 -0.36895608  0.56973287\n",
      " -0.46194968 -0.23281486 -0.07418715  0.04077017  0.52202598  0.08918705\n",
      "  0.72039143 -0.35664342 -0.39387216  0.78018162 -0.41540798  0.11002259\n",
      " -0.80068919  0.68296547 -0.35265098 -0.73629822  0.48870112  0.41729333\n",
      "  0.57746912  0.05634591 -0.67157719  0.30858189  0.35732776  0.30165978\n",
      "  0.55094717  0.2157749   0.21713246  0.34449331 -0.65459133  0.64912084\n",
      " -0.01911289  0.30888308 -0.21792916 -0.3106169  -0.31569687  0.01967885\n",
      " -0.84389507 -0.727317   -0.56332271 -0.17525195  0.12464249 -0.15059182\n",
      " -0.67299858  0.69055936 -0.34205184  0.19363645 -0.40013541 -0.1302929\n",
      " -0.72010926 -0.4204297  -0.17319406 -0.78521319  0.24166465 -0.4359429\n",
      " -0.51811008  0.14873075  0.39492503 -0.19526978  0.30324878  0.25534403\n",
      " -0.14140323 -0.54894935 -0.18607595 -0.23945178 -0.20192939 -0.63649034\n",
      " -0.61284265 -0.65769112 -0.17445822 -0.09196754 -0.4425369   0.44645479\n",
      " -0.57548338  0.39394191  0.20151794  0.07762715  0.74032105  0.70221602\n",
      "  0.76157822 -0.56357326  0.61907384 -0.43075887 -0.32238386 -0.51520695\n",
      "  0.18047423 -0.27194548  0.0147035   0.16260411 -0.23732884  0.82908873\n",
      " -0.3148045   0.27022305 -0.05371202  0.32672249]\n"
     ]
    }
   ],
   "source": [
    "# Get the word vector for a specific word\n",
    "def get_word_vector(word):\n",
    "    return W1[word_to_index[word]]\n",
    "\n",
    "# Test the word vector retrieval\n",
    "word = \"love\"\n",
    "print(f\"Vector for '{word}': {get_word_vector(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'love': [('learning', 0.26791816655337114), ('cool', 0.20000573087514323), ('brown', 0.16993345921666628), ('fox', 0.12007165035471216), ('processing', 0.1160159784002131)]\n"
     ]
    }
   ],
   "source": [
    "# Function to find similar words to a given word\n",
    "def find_similar_words(word, top_n=5):\n",
    "    word_vector = get_word_vector(word)\n",
    "    similarities = {}\n",
    "    for other_word in word_to_index:\n",
    "        if other_word == word:\n",
    "            continue\n",
    "        other_word_vector = get_word_vector(other_word)\n",
    "        # Cosine similarity between word vectors\n",
    "        similarity = np.dot(word_vector, other_word_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_word_vector))\n",
    "        similarities[other_word] = similarity\n",
    "    # Sort words by similarity and return the top_n most similar words\n",
    "    similar_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return similar_words[:top_n]\n",
    "\n",
    "# Find and print similar words to \"love\"\n",
    "similar_words = find_similar_words(\"love\")\n",
    "print(f\"Words similar to 'love': {similar_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
