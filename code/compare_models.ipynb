{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load SpaCy model for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(current_wd) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_embeddings_dict(df):\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']  # Assuming the word column is named 'word'\n",
    "        vector = row.iloc[3:].to_numpy(dtype=np.float32)  # Convert remaining columns to numpy array\n",
    "        embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def compute_similarities(word_pairs, embeddings):\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            vec1 = embeddings[word1].reshape(1, -1)\n",
    "            vec2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "        else:\n",
    "            similarity = 0  # Handle OOV words\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_word(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_\n",
    "\n",
    "# Function to predict the fourth word in an analogy\n",
    "def predict_analogy_word(word_a, word_b, word_c, word_to_vec):\n",
    "    # Lemmatize the words to match the vocabulary\n",
    "    word_a = lemmatize_word(word_a)\n",
    "    word_b = lemmatize_word(word_b)\n",
    "    word_c = lemmatize_word(word_c)\n",
    "\n",
    "    # Retrieve the vectors for the words\n",
    "    vec_a = word_to_vec.get(word_a)\n",
    "    vec_b = word_to_vec.get(word_b)\n",
    "    vec_c = word_to_vec.get(word_c)\n",
    "\n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return None\n",
    "\n",
    "    # Calculate the target vector: vec_b - vec_a + vec_c\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "\n",
    "    # Find the closest word to the target vector\n",
    "    best_word = None\n",
    "    best_similarity = float('inf')\n",
    "    \n",
    "    for word, vec in word_to_vec.items():\n",
    "        if word not in {word_a, word_b, word_c}:\n",
    "            similarity = cosine(target_vec, vec)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_word = word\n",
    "    \n",
    "    return best_word\n",
    "\n",
    "def evaluate_analogy_dataset(analogy_df, word_to_vec):\n",
    "    correct = 0\n",
    "    total = len(analogy_df)\n",
    "\n",
    "    for index, row in analogy_df.iterrows():\n",
    "        word_a = row['Word 1']\n",
    "        word_b = row['Word 2']\n",
    "        word_c = row['Word 3']\n",
    "        expected_word_d = row['Expected Word']\n",
    "\n",
    "        predicted_word_d = predict_analogy_word(word_a, word_b, word_c, word_to_vec)\n",
    "\n",
    "        # Compare the predicted word with the expected word\n",
    "        if predicted_word_d == lemmatize_word(expected_word_d):\n",
    "            correct += 1\n",
    "        \n",
    "        #print(f\"Analogy: {word_a} is to {word_b} as {word_c} is to {predicted_word_d} (Expected: {expected_word_d})\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow_df = pd.read_csv(local_path + \"output/embeddings/embeddings_CBOW_total_posts.csv\")\n",
    "embeddings_skipgram_df = pd.read_csv(local_path + \"output/embeddings/embeddings_CBOW_posts_genz.csv\")\n",
    "glove_cbow_df = pd.read_csv(local_path + \"output/embeddings/glove_embeddings.csv\")\n",
    "bge_cbow_df = pd.read_csv(local_path + \"output/embeddings/bge_embeddings.csv\")\n",
    "\n",
    "## to dict\n",
    "skipgram_embeddings = df_to_embeddings_dict(embeddings_skipgram_df)\n",
    "cbow_embeddings = df_to_embeddings_dict(embeddings_cbow_df)\n",
    "bge_embeddings = df_to_embeddings_dict(bge_cbow_df)\n",
    "glove_embeddings = df_to_embeddings_dict(glove_cbow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim353_df = pd.read_csv(local_path + \"data/external_data/wordsim353crowd.csv\")\n",
    "\n",
    "## split up into list of word pairs and list of scores\n",
    "word_pairs = wordsim353_df[['Word 1', 'Word 2']].values.tolist()\n",
    "human_scores = wordsim353_df['Human (Mean)'].values\n",
    "\n",
    "wordsim353_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_similarities = compute_similarities(word_pairs, cbow_embeddings)\n",
    "correlation_cbow = spearmanr(cbow_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (CBOW): {correlation_cbow:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_similarities = compute_similarities(word_pairs, skipgram_embeddings)\n",
    "correlation_skipgram = spearmanr(skipgram_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (Skip-gram): {correlation_skipgram:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_similarities = compute_similarities(word_pairs, glove_embeddings)\n",
    "correlation_glove = spearmanr(glove_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (GloVe): {correlation_glove:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_similarities = compute_similarities(word_pairs, bge_embeddings)\n",
    "correlation_bge = spearmanr(bge_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (BGE): {correlation_bge:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame of analogies\n",
    "data_climate_change = {\n",
    "    'Word 1': ['coal', 'fossil', 'emission', 'climate', 'carbon', 'global', 'electric', 'renewable', 'methane', 'solar',\n",
    "               'carbon', 'temperature', 'ice', 'deforestation', 'dioxide', 'sea', 'wind', 'greenhouse', 'sustainability', 'pollution'],\n",
    "    'Word 2': ['fossil', 'energy', 'reduction', 'warming', 'dioxide', 'warming', 'vehicle', 'energy', 'gas', 'power',\n",
    "               'dioxide', 'rise', 'melt', 'reduction', 'gas', 'level', 'turbine', 'emissions', 'policy', 'impact'],\n",
    "    'Word 3': ['solar', 'renewable', 'pollution', 'environment', 'methane', 'cooling', 'car', 'wind', 'CO2', 'wind',\n",
    "               'carbon', 'impact', 'melting', 'forestation', 'gas', 'carbon', 'turbine', 'mitigation', 'climate', 'change'],\n",
    "    'Expected Word': ['renewable', 'resource', 'control', 'sustainability', 'gas', 'cooling', 'car', 'turbine', 'greenhouse', 'energy',\n",
    "                      'emission', 'rise', 'melt', 'deforestation', 'gas', 'level', 'wind', 'policy', 'impact', 'policy']}\n",
    "\n",
    "analogy_df_climate_change = pd.DataFrame(data_climate_change)\n",
    "\n",
    "data_reddit = {\n",
    "    'Word 1': ['OP', 'thread', 'TL;DR', 'upvote', 'troll', 'mod', 'AMA', 'lurker', 'NSFW', 'flair',\n",
    "               'comment', 'karma', 'subreddit', 'post', 'reply', 'ban', 'meme', 'user', 'admin', 'tag'],\n",
    "    'Word 2': ['post', 'discussion', 'summary', 'downvote', 'bait', 'admin', 'Q&A', 'reader', 'SFW', 'label',\n",
    "               'reply', 'points', 'community', 'thread', 'comment', 'ban', 'GIF', 'user', 'moderator', 'badge'],\n",
    "    'Word 3': ['comment', 'reply', 'context', 'upvote', 'spam', 'user', 'ask', 'reader', 'explicit', 'flair',\n",
    "               'upvote', 'comment', 'thread', 'discussion', 'report', 'image', 'moderator', 'poster', 'sub', 'message'],\n",
    "    'Expected Word': ['reply', 'discussion', 'summary', 'downvote', 'bait', 'admin', 'Q&A', 'lurker', 'SFW', 'tag',\n",
    "                      'comment', 'points', 'subreddit', 'post', 'reply', 'ban', 'sticker', 'user', 'admin', 'flair']}\n",
    "\n",
    "analogy_df_reddit = pd.DataFrame(data_reddit)\n",
    "\n",
    "data_politics_climate = {\n",
    "    'Word 1': ['EPA', 'Paris Agreement', 'Biden', 'UN', 'Congress', 'carbon tax', 'renewable energy', 'climate bill', 'COP26', 'Green New Deal',\n",
    "               'regulation', 'emissions', 'legislation', 'government', 'policy', 'administration', 'carbon footprint', 'international', 'president', 'senator'],\n",
    "    'Word 2': ['regulation', 'international accord', 'administration', 'global body', 'legislature', 'carbon pricing', 'clean energy', 'policy', 'summit', 'policy',\n",
    "               'rules', 'treaty', 'law', 'leadership', 'initiative', 'impact', 'effort', 'negotiation', 'leader', 'law'],\n",
    "    'Word 3': ['Paris Agreement', 'UN', 'Biden', 'G7', 'senator', 'cap-and-trade', 'climate action', 'agenda', 'COP21', 'climate legislation',\n",
    "               'treaty', 'agreement', 'regulation', 'administration', 'program', 'target', 'campaign', 'deal', 'conference', 'bill'],\n",
    "    'Expected Word': ['international accord', 'agreement', 'administration', 'global body', 'congress', 'carbon pricing', 'clean energy', 'policy', 'summit', 'policy',\n",
    "                      'regulation', 'treaty', 'law', 'leadership', 'initiative', 'impact', 'effort', 'negotiation', 'leader', 'law']\n",
    "}\n",
    "\n",
    "analogy_df_politics_climate = pd.DataFrame(data_politics_climate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_cbow_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_climate_change:.2%}\")\n",
    "\n",
    "## reddit related words:\n",
    "accuracy_cbow_reddit = evaluate_analogy_dataset(analogy_df_reddit, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_reddit:.2%}\")\n",
    "\n",
    "## politics related words:\n",
    "accuracy_cbow_politics = evaluate_analogy_dataset(analogy_df_politics_climate, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_politics:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_glove_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, glove_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (GloVe - Climate Change): {accuracy_glove_climate_change:.2%}\")\n",
    "\n",
    "## Reddit related words:\n",
    "accuracy_glove_reddit = evaluate_analogy_dataset(analogy_df_reddit, glove_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (GloVe - Reddit): {accuracy_glove_reddit:.2%}\")\n",
    "\n",
    "## Politics/Climate related words:\n",
    "accuracy_glove_politics_climate = evaluate_analogy_dataset(analogy_df_politics_climate, glove_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (GloVe - Politics/Climate): {accuracy_glove_politics_climate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_bge_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, bge_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (BGE - Climate Change): {accuracy_bge_climate_change:.2%}\")\n",
    "\n",
    "## Reddit related words:\n",
    "accuracy_bge_reddit = evaluate_analogy_dataset(analogy_df_reddit, bge_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (BGE - Reddit): {accuracy_bge_reddit:.2%}\")\n",
    "\n",
    "## Politics/Climate related words:\n",
    "accuracy_bge_politics_climate = evaluate_analogy_dataset(analogy_df_politics_climate, bge_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy (BGE - Politics/Climate): {accuracy_bge_politics_climate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_skipram_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipram_climate_change:.2%}\")\n",
    "#\n",
    "### reddit related words:\n",
    "accuracy_skipgram_reddit = evaluate_analogy_dataset(analogy_df_reddit, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipgram_reddit:.2%}\")\n",
    "#\n",
    "### politics related words:\n",
    "accuracy_skipgram_politics = evaluate_analogy_dataset(analogy_df_politics_climate, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipgram_politics:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
