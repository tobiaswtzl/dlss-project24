{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  d:\\dlss-project24/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load SpaCy model for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(current_wd) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_embeddings_dict(df):\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']  # Assuming the word column is named 'word'\n",
    "        vector = row.iloc[3:].to_numpy(dtype=np.float32)  # Convert remaining columns to numpy array\n",
    "        embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def compute_similarities(word_pairs, embeddings):\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            vec1 = embeddings[word1].reshape(1, -1)\n",
    "            vec2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "        else:\n",
    "            similarity = 0  # Handle OOV words\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_word(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_\n",
    "\n",
    "# Function to predict the fourth word in an analogy\n",
    "def predict_analogy_word(word_a, word_b, word_c, word_to_vec):\n",
    "    # Lemmatize the words to match the vocabulary\n",
    "    word_a = lemmatize_word(word_a)\n",
    "    word_b = lemmatize_word(word_b)\n",
    "    word_c = lemmatize_word(word_c)\n",
    "\n",
    "    # Retrieve the vectors for the words\n",
    "    vec_a = word_to_vec.get(word_a)\n",
    "    vec_b = word_to_vec.get(word_b)\n",
    "    vec_c = word_to_vec.get(word_c)\n",
    "\n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return None\n",
    "\n",
    "    # Calculate the target vector: vec_b - vec_a + vec_c\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "\n",
    "    # Find the closest word to the target vector\n",
    "    best_word = None\n",
    "    best_similarity = float('inf')\n",
    "    \n",
    "    for word, vec in word_to_vec.items():\n",
    "        if word not in {word_a, word_b, word_c}:\n",
    "            similarity = cosine(target_vec, vec)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_word = word\n",
    "    \n",
    "    return best_word\n",
    "\n",
    "def predict_analogy_word(word_a, word_b, word_c, word_to_vec):\n",
    "    vec_a = word_to_vec.get(word_a)\n",
    "    vec_b = word_to_vec.get(word_b)\n",
    "    vec_c = word_to_vec.get(word_c)\n",
    "    \n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate the analogy vector: vec_b - vec_a + vec_c\n",
    "    vec_d = vec_b - vec_a + vec_c\n",
    "    \n",
    "    # Find the closest word to vec_d\n",
    "    closest_word = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for word, vec in word_to_vec.items():\n",
    "        distance = np.linalg.norm(vec - vec_d)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = word\n",
    "            \n",
    "    return closest_word\n",
    "\n",
    "\n",
    "def evaluate_analogy_dataset(analogy_df, word_to_vec):\n",
    "    correct = 0\n",
    "    total = len(analogy_df)\n",
    "\n",
    "    for index, row in analogy_df.iterrows():\n",
    "        word_a = row['Word 1']\n",
    "        word_b = row['Word 2']\n",
    "        word_c = row['Word 3']\n",
    "        expected_word_d = row['Expected Word']\n",
    "\n",
    "        predicted_word_d = predict_analogy_word(word_a, word_b, word_c, word_to_vec)\n",
    "\n",
    "        # Compare the predicted word with the expected word\n",
    "        if predicted_word_d == lemmatize_word(expected_word_d):\n",
    "            correct += 1\n",
    "        \n",
    "        #print(f\"Analogy: {word_a} is to {word_b} as {word_c} is to {predicted_word_d} (Expected: {expected_word_d})\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_analogy_dataset(analogy_df, word_to_vec):\n",
    "    correct = 0\n",
    "    total = len(analogy_df)\n",
    "\n",
    "    for index, row in analogy_df.iterrows():\n",
    "        word_a = row['Word 1']\n",
    "        word_b = row['Word 2']\n",
    "        word_c = row['Word 3']\n",
    "        expected_word_d = row['Expected Word']\n",
    "\n",
    "        predicted_word_d = predict_analogy_word(word_a, word_b, word_c, word_to_vec)\n",
    "\n",
    "        # Compare the predicted word with the expected word\n",
    "        if predicted_word_d == lemmatize_word(expected_word_d):\n",
    "            correct += 1\n",
    "        \n",
    "        # Optionally, print details for debugging\n",
    "        # print(f\"Analogy: {word_a} is to {word_b} as {word_c} is to {predicted_word_d} (Expected: {expected_word_d})\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow_df = pd.read_csv(local_path + \"output/embeddings/embeddings_CBOW_total_posts.csv\")\n",
    "embeddings_skipgram_df = pd.read_csv(local_path + \"output/embeddings/embeddings_CBOW_posts_genz.csv\")\n",
    "glove_cbow_df = pd.read_csv(local_path + \"output/embeddings/glove_embeddings.csv\")\n",
    "bge_cbow_df = pd.read_csv(local_path + \"output/embeddings/bge_embeddings.csv\")\n",
    "\n",
    "## fix order so function workd for all dfs\n",
    "glove_cbow_df = glove_cbow_df[glove_cbow_df.columns[:1].tolist() + ['word'] + glove_cbow_df.columns[1:].drop('word').tolist()]\n",
    "bge_cbow_df = bge_cbow_df[bge_cbow_df.columns[:1].tolist() + ['word'] + bge_cbow_df.columns[1:].drop('word').tolist()]\n",
    "\n",
    "## to dict\n",
    "skipgram_embeddings = df_to_embeddings_dict(embeddings_skipgram_df)\n",
    "cbow_embeddings = df_to_embeddings_dict(embeddings_cbow_df)\n",
    "bge_embeddings = df_to_embeddings_dict(bge_cbow_df)\n",
    "glove_embeddings = df_to_embeddings_dict(glove_cbow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim353_df = pd.read_csv(local_path + \"data/external_data/wordsim353crowd.csv\")\n",
    "\n",
    "## split up into list of word pairs and list of scores\n",
    "word_pairs = wordsim353_df[['Word 1', 'Word 2']].values.tolist()\n",
    "human_scores = wordsim353_df['Human (Mean)'].values\n",
    "\n",
    "wordsim353_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_similarities = compute_similarities(word_pairs, cbow_embeddings)\n",
    "correlation_cbow = spearmanr(cbow_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (CBOW): {correlation_cbow:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_similarities = compute_similarities(word_pairs, skipgram_embeddings)\n",
    "correlation_skipgram = spearmanr(skipgram_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (Skip-gram): {correlation_skipgram:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_similarities = compute_similarities(word_pairs, glove_embeddings)\n",
    "correlation_glove = spearmanr(glove_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (GloVe): {correlation_glove:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_similarities = compute_similarities(word_pairs, bge_embeddings)\n",
    "correlation_bge = spearmanr(bge_similarities, human_scores).correlation\n",
    "print(f\"Spearman Correlation (BGE): {correlation_bge:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word2vec_analogies = {\n",
    "    'Word 1': ['sustainable', 'park', 'pollution', 'scientist', 'carbon', \n",
    "               'leader', 'earth', 'wealth', 'fire', 'drought',\n",
    "               'global', 'emission', 'research', 'climate', 'city',\n",
    "               'temperature', 'development', 'tree', 'planet'],\n",
    "    'Word 2': ['development', 'bike lane', 'environment', 'experiment', 'dioxide', \n",
    "               'country', 'orbit', 'privilege', 'forest', 'water',\n",
    "               'warming', 'atmosphere', 'discovery', 'change', 'capital',\n",
    "               'weather', 'progress', 'bicycle', 'solar system'],\n",
    "    'Word 3': ['renewable', 'tree', 'waste', 'researcher', 'oxygen', \n",
    "               'nation', 'rotation', 'poverty', 'fuel', 'famine',\n",
    "               'cooling', 'pollutant', 'innovation', 'adaptation', 'village',\n",
    "               'precipitation', 'growth', 'shade', 'sun'],\n",
    "    'Expected Word': ['energy', 'bicycle', 'disposal', 'analysis', 'air', \n",
    "                      'state', 'spin', 'disadvantaged ', 'firewood', 'food',\n",
    "                      'heating', 'contaminant', 'invention', 'mitigation', 'town',\n",
    "                      'rain', 'advancement', 'canopy', 'orbit']\n",
    "}\n",
    "\n",
    "\n",
    "data_word2vec_analogies_df = pd.DataFrame(data_word2vec_analogies)\n",
    "\n",
    "accuracy_cbow_data_word2vec_analogies = evaluate_analogy_dataset(data_word2vec_analogies_df, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_data_word2vec_analogies:.2%}\")\n",
    "\n",
    "\n",
    "accuracy_skipgram_data_word2vec_analogies = evaluate_analogy_dataset(data_word2vec_analogies_df, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipgram_data_word2vec_analogies:.2%}\")\n",
    "\n",
    "accuracy_glove_data_word2vec_analogies = evaluate_analogy_dataset(data_word2vec_analogies_df, glove_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_glove_data_word2vec_analogies:.2%}\")\n",
    "\n",
    "accuracy_bge_data_word2vec_analogies = evaluate_analogy_dataset(data_word2vec_analogies_df, bge_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_bge_data_word2vec_analogies:.2%}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
