{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Kaggle's post dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "## install english spacy model (if not already installed) and load\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data in, drop non relevant cols, can be later merged on id\n",
    "posts = pd.read_csv(\"data/raw/total_posts.csv\")[[\"id\", \"title\", \"selftext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by performing the following steps:\n",
    "    1. Detects the language of the text using a language detection library.\n",
    "    2. Filters out non-English text; returns (pd.NA, pd.NA) if the detected language is not English ('en').\n",
    "    3. Processes the text using spaCy in batch mode:\n",
    "       - Converts the text to lowercase.\n",
    "       - Tokenizes the text.\n",
    "       - Filters out stop words, URLs, and non-alphabetic tokens.\n",
    "       - Creates a cleaned version of the text without stop words and URLs.\n",
    "       - Creates a lemmatized version of the cleaned text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two strings:\n",
    "        - cleaned (str): The cleaned version of the input text.\n",
    "        - lemmatized (str): The lemmatized version of the cleaned text.\n",
    "        Returns (pd.NA, pd.NA) if the language is not English or if an error occurs during processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the text is deleted, removed, empty, or None\n",
    "        if text in ['[deleted]', '[removed]', '', None]:\n",
    "            return \"\", \"\"\n",
    "\n",
    "        # Convert text to lowercase before language detection\n",
    "        text = text.lower()\n",
    "\n",
    "        # Detect the language of the text\n",
    "        lang = detect(text)\n",
    "\n",
    "        # Return NA if the detected language is not English\n",
    "        if lang != 'en':\n",
    "            return pd.NA, pd.NA\n",
    "\n",
    "        # Replace specific HTML junk (e.g., '&gt;')\n",
    "        text = text.replace('&gt;', ' ')\n",
    "\n",
    "        # Substitute Reddit usernames (e.g., 'u/username') with a placeholder\n",
    "        text = re.sub(r\"u/\\w+\", \"username\", text)\n",
    "\n",
    "        # Process the text using spaCy in batch mode, disabling NER and parser to save time\n",
    "        doc = list(nlp.pipe([text], disable=[\"ner\", \"parser\"]))[0]\n",
    "\n",
    "        # Initialize a list to store cleaned tokens\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Include only alphabetic tokens or punctuation ('?' and '!') and exclude URLs\n",
    "            if (token.is_alpha or token.text in ['?', '!']) and not token.like_url:\n",
    "                cleaned_tokens.append(token)\n",
    "\n",
    "        # Join the cleaned tokens into a single string\n",
    "        cleaned = ' '.join([token.text for token in cleaned_tokens])\n",
    "        # Join the lemmatized forms of the tokens into a single string\n",
    "        lemmatized = ' '.join([token.lemma_ for token in cleaned_tokens])\n",
    "\n",
    "        return cleaned, lemmatized\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return empty strings in case of any errors during processing\n",
    "        return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'title' column of the posts DataFrame\n",
    "# The 'preprocess_text' function returns a tuple, so we unpack it into two new columns: 'title_cleaned' and 'title_lemmatized'\n",
    "posts[['title_cleaned', 'title_lemmatized']] = posts['title'].apply(lambda x: pd.Series(preprocess_text(x)))\n",
    "\n",
    "# Apply the preprocessing function to the 'selftext' column of the posts DataFrame\n",
    "# Similarly, unpack the tuple returned by 'preprocess_text' into 'selftext_cleaned' and 'selftext_lemmatized'\n",
    "posts[['selftext_cleaned', 'selftext_lemmatized']] = posts['selftext'].apply(lambda x: pd.Series(preprocess_text(x)))\n",
    "\n",
    "# Clean the 'selftext' column by replacing NaN values and markers of deleted/removed text with an empty string\n",
    "posts[\"selftext\"] = posts[\"selftext\"].apply(lambda x: \"\" if pd.isna(x) or x in ['[deleted]', '[removed]'] else x)\n",
    "\n",
    "# Combine the 'title' and 'selftext' columns into a new column 'title_and_text'\n",
    "posts[\"title_and_text\"] = posts[\"title\"] + \" \" + posts[\"selftext\"]\n",
    "\n",
    "# Combine the cleaned versions of 'title' and 'selftext' into 'title_and_text_cleaned'\n",
    "posts[\"title_and_text_cleaned\"] = posts[\"title_cleaned\"] + \" \" + posts[\"selftext_cleaned\"]\n",
    "\n",
    "# Combine the lemmatized versions of 'title' and 'selftext' into 'title_and_text_lemmatized'\n",
    "posts[\"title_and_text_lemmatized\"] = posts[\"title_lemmatized\"] + \" \" + posts[\"selftext_lemmatized\"]\n",
    "\n",
    "# Drop any rows that contain NaN values after preprocessing\n",
    "posts = posts.dropna()\n",
    "\n",
    "# Reset the index of the DataFrame after dropping rows, ensuring that the index is sequential\n",
    "posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the preprocessed DataFrame to a CSV file\n",
    "posts.to_csv(\"data/preprocessed/total_posts.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
