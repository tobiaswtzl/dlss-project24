{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of different Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors, normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the post data\n",
    "preprocessed_posts = pd.read_csv('c:/Users/Jannik Wirtheim/OneDrive/Dokumente/Privat/Bildung/M. Sc. Social and Economic Data Science/2. Semester/Deep Learning for Social Sciences/Project/data/preprocessed_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy post data\n",
    "df_bpe = preprocessed_posts.copy()\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer_bpe = Tokenizer(models.BPE())\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_bpe[\"title_and_text_lemmatized\"].tolist()\n",
    "tokenizer_bpe.train_from_iterator(texts, trainer)\n",
    "vocab_bpe = tokenizer_bpe.get_vocab()\n",
    "vocab_size_bpe = len(vocab_bpe)\n",
    "\n",
    "# Tokenize the column and add tokens as a new column\n",
    "df_bpe['tokens'] = df_bpe[\"title_and_text_lemmatized\"].apply(lambda x: tokenizer_bpe.encode(x).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy post data\n",
    "df_wp = preprocessed_posts.copy()\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer_wp = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))\n",
    "\n",
    "# Normalize the text (optional but recommended)\n",
    "tokenizer_wp.normalizer = normalizers.BertNormalizer()\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer_wp.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_wp['title_and_text_lemmatized'].tolist()\n",
    "tokenizer_wp.train_from_iterator(texts, trainer)\n",
    "vocab_wordpiece = tokenizer_wp.get_vocab()\n",
    "vocab_size_wordpiece = len(vocab_wordpiece)\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_wp['tokens'] = df_wp['title_and_text_lemmatized'].apply(lambda x: tokenizer_wp.encode(x).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy post data\n",
    "df_spacy = preprocessed_posts.copy()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_spacy(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the given text using the Spacy library.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens extracted from the text, excluding punctuation and whitespace.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# Apply the tokenize function to the \"cleaned\" column\n",
    "df_spacy['tokens'] = df_spacy['title_and_text_lemmatized'].apply(tokenize_spacy)\n",
    "\n",
    "# Build the vocabulary\n",
    "# Flatten the list of tokens and create a set of unique tokens\n",
    "vocab = set(token for tokens in df_spacy['tokens'] for token in tokens)\n",
    "\n",
    "# Calculate the length of the vocabulary\n",
    "vocab_size_spacy= len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy post data\n",
    "df_unigram = preprocessed_posts.copy()\n",
    "\n",
    "# Initialize the CountVectorizer for unigram (default)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the \"cleaned\" column and transform the text to a term-document matrix\n",
    "X = vectorizer.fit_transform(df_unigram['title_and_text_lemmatized'])\n",
    "\n",
    "# Get the feature names (i.e., the vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Length of the vocabulary\n",
    "vocab_size_unigram = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenizer  Vocab Size\n",
      "0        BPE       10000\n",
      "1    Unigram       14797\n",
      "2  WordPiece       10000\n",
      "3      spaCy       14818\n"
     ]
    }
   ],
   "source": [
    "# Collect the results\n",
    "results = {\n",
    "    \"Tokenizer\": [\"BPE\", \"Unigram\", \"WordPiece\", \"spaCy\"],\n",
    "    \"Vocab Size\": [vocab_size_bpe, vocab_size_unigram, vocab_size_wordpiece, vocab_size_spacy]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
