{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to treat # and other punctuation\n",
    "- Implement BPE\n",
    "- Implement WordPiece\n",
    "- Implement Spacy\n",
    "- Implement Unigram\n",
    "- Compare size of vocab\n",
    "- Compare performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: mit cleaned testen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_bpe = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_wp = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_spacy = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_unigram = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "dataframes = [df_comments_bpe, df_comments_wp, df_comments_spacy, df_comments_unigram, df_comments]\n",
    "\n",
    "for df in dataframes:\n",
    "    df['cleaned'] = df['cleaned'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer_bpe = Tokenizer(models.BPE())\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_comments_bpe[\"cleaned\"].tolist()\n",
    "tokenizer_bpe.train_from_iterator(texts, trainer)\n",
    "vocab_bpe = tokenizer_bpe.get_vocab()\n",
    "vocab_size_bpe = len(vocab_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[nice, try, but, false, equivalency, is, still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[i, slightly, torn, on, this, the, idea, of, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[article, cont, ents, as, always, south, austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[what, is, the, current, out, look, for, ocean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[the, usual, reason, their, policies, most, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[it, not, deep, though, it, is, a, surface, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[hello, unfortunately, your, submission, do, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[new, conventional, nuclear, plants, ex, orbit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[just, look, through, your, comment, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   nan   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0                                                 [nan]  \n",
       "1     [nice, try, but, false, equivalency, is, still...  \n",
       "2     [i, slightly, torn, on, this, the, idea, of, n...  \n",
       "3     [article, cont, ents, as, always, south, austr...  \n",
       "4     [what, is, the, current, out, look, for, ocean...  \n",
       "...                                                 ...  \n",
       "1995  [the, usual, reason, their, policies, most, cl...  \n",
       "1996  [it, not, deep, though, it, is, a, surface, le...  \n",
       "1997  [hello, unfortunately, your, submission, do, b...  \n",
       "1998  [new, conventional, nuclear, plants, ex, orbit...  \n",
       "1999  [just, look, through, your, comment, history, ...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the column and add tokens as a new column\n",
    "df_comments_bpe['tokens'] = df_comments_bpe['cleaned'].apply(lambda x: tokenizer_bpe.encode(x).tokens)\n",
    "\n",
    "# Display the dataframe with tokens\n",
    "df_comments_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unl_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, normalizers\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer_wp = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))\n",
    "\n",
    "# Normalize the text (optional but recommended)\n",
    "tokenizer_wp.normalizer = normalizers.BertNormalizer()\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer_wp.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_comments_wp['cleaned'].tolist()\n",
    "tokenizer_wp.train_from_iterator(texts, trainer)\n",
    "vocab_wordpiece = tokenizer_wp.get_vocab()\n",
    "vocab_size_wordpiece = len(vocab_wordpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[nice, try, but, false, equivalency, is, still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[i, slightly, torn, on, this, the, idea, of, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[article, content, ##s, as, always, south, aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[what, is, the, current, out, ##lo, ##ok, for,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[the, usual, reason, their, policies, most, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[it, not, deep, though, it, is, a, surface, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[hello, unfortunately, your, submission, do, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[new, conventional, nuclear, plants, ex, ##or,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[just, look, through, your, comment, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   nan   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0                                                 [nan]  \n",
       "1     [nice, try, but, false, equivalency, is, still...  \n",
       "2     [i, slightly, torn, on, this, the, idea, of, n...  \n",
       "3     [article, content, ##s, as, always, south, aus...  \n",
       "4     [what, is, the, current, out, ##lo, ##ok, for,...  \n",
       "...                                                 ...  \n",
       "1995  [the, usual, reason, their, policies, most, cl...  \n",
       "1996  [it, not, deep, though, it, is, a, surface, le...  \n",
       "1997  [hello, unfortunately, your, submission, do, b...  \n",
       "1998  [new, conventional, nuclear, plants, ex, ##or,...  \n",
       "1999  [just, look, through, your, comment, history, ...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the dataframe\n",
    "df_comments_wp['tokens'] = df_comments_wp['cleaned'].apply(lambda x: tokenizer_wp.encode(x).tokens)\n",
    "\n",
    "# Display the dataframe with tokens\n",
    "df_comments_wp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_spacy(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the given text using the Spacy library.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens extracted from the text, excluding punctuation and whitespace.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# Apply the tokenize function to the \"cleaned\" column\n",
    "df_comments_spacy['tokens'] = df_comments_spacy['cleaned'].apply(tokenize_spacy)\n",
    "\n",
    "# Build the vocabulary\n",
    "# Flatten the list of tokens and create a set of unique tokens\n",
    "vocab = set(token for tokens in df_comments_spacy['tokens'] for token in tokens)\n",
    "\n",
    "# Calculate the length of the vocabulary\n",
    "vocab_size_spacy= len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_spacy['tokens'] = df_comments_spacy['cleaned'].apply(tokenize_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize the CountVectorizer for unigram (default)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the \"cleaned\" column and transform the text to a term-document matrix\n",
    "X = vectorizer.fit_transform(df_comments_unigram['cleaned'])\n",
    "\n",
    "# Get the feature names (i.e., the vocabulary)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Length of the vocabulary\n",
    "vocab_size_unigram = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenizer  Vocab Size\n",
      "0        BPE       10000\n",
      "1    Unigram       14797\n",
      "2  WordPiece       10000\n",
      "3      spaCy       14818\n"
     ]
    }
   ],
   "source": [
    "# Collect the results\n",
    "results = {\n",
    "    \"Tokenizer\": [\"BPE\", \"Unigram\", \"WordPiece\", \"spaCy\"],\n",
    "    \"Vocab Size\": [vocab_size_bpe, vocab_size_unigram, vocab_size_wordpiece, vocab_size_spacy]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
