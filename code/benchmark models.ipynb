{"cells":[{"cell_type":"markdown","source":["WIR MÜSSEN MODELLE ÜBER PIPLINE LADEN SONST SIND DIESE ZU GROß"],"metadata":{"id":"FciH_oYMg0kA"}},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","from io import StringIO\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","from transformers import pipeline"],"metadata":{"id":"avbz52RPlOij","executionInfo":{"status":"ok","timestamp":1723734744158,"user_tz":-120,"elapsed":17849,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DONQzCOth7ph","executionInfo":{"status":"ok","timestamp":1723734744784,"user_tz":-120,"elapsed":647,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}}},"outputs":[],"source":["# URL of the CSV file containing sample comments\n","url = \"https://raw.githubusercontent.com/tobiaswtzl/dlss-project24/main/data/preprocessed/comments.csv\"\n","\n","# Headers for the HTTP request, including an authorization token\n","headers = {\"Authorization\": \"token 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\"}\n","\n","# Sending a GET request to the specified URL with the provided headers\n","response = requests.get(url, headers=headers)\n","\n","# Creating a StringIO object from the response text to simulate a file-like object\n","data = StringIO(response.text)\n","\n","# Reading the CSV data into a pandas DataFrame\n","comments = pd.read_csv(data)"]},{"cell_type":"code","source":["comments = comments[:10]"],"metadata":{"id":"JhL0kLbymDuY","executionInfo":{"status":"ok","timestamp":1723734744784,"user_tz":-120,"elapsed":10,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hKv1lDIch7ph","executionInfo":{"status":"ok","timestamp":1723734744785,"user_tz":-120,"elapsed":10,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}}},"outputs":[],"source":["# Splitting the data into train, validation, and test sets\n","train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","#Adding all comments for generating the vocabulary. If not an error occurs when tokens missing\n","total_comments_list = comments[\"lemmatized\"].dropna().astype(str).tolist()\n","\n","train_list = train_df[\"lemmatized\"].dropna().astype(str).tolist()\n","val_list = val_df[\"lemmatized\"].dropna().astype(str).tolist()\n","test_list = test_df[\"lemmatized\"].dropna().astype(str).tolist()\n","\n","# Ensure each entry is a string and split each sentence into words\n","total_corpus = [doc.split() for doc in total_comments_list]\n","corpus_train = [doc.split() for doc in train_list]\n","corpus_val = [doc.split() for doc in val_list]\n","corpus_test = [doc.split() for doc in test_list]\n","\n","# Create a vocabulary: count occurrences of each word\n","vocab = defaultdict(int)\n","for sentence in total_corpus:\n","    for word in sentence:\n","        vocab[word] += 1\n","\n","min_count = 6\n","\n","# Remove infrequent words from the vocabulary\n","vocab = {word: count for word, count in vocab.items() if count >= min_count}\n","\n","# Create word to index and index to word mappings\n","word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n","index_to_word = {idx: word for word, idx in word_to_index.items()}\n","\n","# Create DataFrame from vocabulary\n","vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n","\n","vocab_set = set(vocab.keys())\n","\n","def filter_corpus(corpus, vocab_set):\n","    return [[word for word in doc if word in vocab_set] for doc in corpus]\n","\n","filtered_total_corpus = filter_corpus(total_corpus, vocab_set)\n","filtered_corpus_train = filter_corpus(corpus_train, vocab_set)\n","filtered_corpus_val = filter_corpus(corpus_val, vocab_set)\n","filtered_corpus_test = filter_corpus(corpus_test, vocab_set)"]},{"cell_type":"code","source":["#FUNCTIONS\n","# Prepare the text for the model\n","def tokenize_corpus(corpus, tokenizer, max_length=512):\n","    \"\"\"\n","    Tokenizes the corpus using the provided tokenizer.\n","\n","    Args:\n","    - corpus: List of lists, where each sublist is a list of words.\n","    - tokenizer: The tokenizer to use.\n","    - max_length: Maximum length of tokens per input.\n","\n","    Returns:\n","    - tokenized_corpus: List of tokenized inputs.\n","    \"\"\"\n","    tokenized_corpus = []\n","    for sentence in corpus:\n","        sentence_str = \" \".join(sentence)\n","        tokenized_input = tokenizer(\n","            sentence_str,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","        tokenized_corpus.append(tokenized_input)\n","    return tokenized_corpus\n","\n","\n","def run_model_on_corpus(tokenized_corpus, model, device):\n","    \"\"\"\n","    Passes the tokenized inputs to the model and returns the outputs.\n","\n","    Args:\n","    - tokenized_corpus: List of tokenized inputs.\n","    - model: The model to run.\n","    - device: The device (CPU/GPU) to run the model on.\n","\n","    Returns:\n","    - outputs: List of model outputs.\n","    \"\"\"\n","    outputs = []\n","    model.to(device)  # Move model to the specified device\n","    for tokenized_input in tokenized_corpus:\n","        tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}  # Move input tensors to device\n","        with torch.no_grad():\n","            output = model(**tokenized_input)\n","            outputs.append(output)\n","    return outputs\n","\n","\n","# FUNCTION: Prepare the text for the model using a pipeline\n","def run_model_on_corpus_with_pipeline(corpus, model_pipeline):\n","    \"\"\"\n","    Passes the corpus through the pipeline and returns the outputs.\n","\n","    Args:\n","    - corpus: List of lists, where each sublist is a list of words.\n","    - model_pipeline: The pipeline to run.\n","\n","    Returns:\n","    - outputs: List of model outputs.\n","    \"\"\"\n","    outputs = []\n","    for sentence in corpus:\n","        sentence_str = \" \".join(sentence)\n","        output = model_pipeline(sentence_str)\n","        outputs.append(output)\n","    return outputs"],"metadata":{"id":"Zjqs_ielTnAN","executionInfo":{"status":"ok","timestamp":1723734744785,"user_tz":-120,"elapsed":9,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58lDREzuh7pi"},"source":["And dunzhang/stella_en_1.5B_v5 which is the second best but with less memory needed.\n","\n","https://huggingface.co/dunzhang/stella_en_1.5B_v5"]},{"cell_type":"code","source":["# Load the pipeline\n","stella_pipeline = pipeline(\"feature-extraction\", model=\"dunzhang/stella_en_1.5B_v5\", tokenizer=\"dunzhang/stella_en_1.5B_v5\", device=0 if torch.cuda.is_available() else -1)\n","\n","# Run the model on the filtered training data using the pipeline\n","train_outputs_stella = run_model_on_corpus_with_pipeline(filtered_corpus_train, stella_pipeline)\n","\n","# If you want to extract embeddings or further process the outputs, you can do so here.\n","# For example, the outputs will be in a list of numpy arrays or tensors depending on the pipeline.\n","train_outputs_stella"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1jLg6wyGXS9k0CL3k5NRPlatCgZQ3IhLG"},"id":"BSbLmsLbgOAP","executionInfo":{"status":"ok","timestamp":1723734812494,"user_tz":-120,"elapsed":67717,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}},"outputId":"0159405a-62fb-4207-e85d-129f2bec6112"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["#Implemented the model directly. But size is to big for free colab\n","'''\n","# Load the tokenizer and model\n","tokenizer_stella = AutoTokenizer.from_pretrained(\"dunzhang/stella_en_1.5B_v5\")\n","model_stella = AutoModel.from_pretrained(\"dunzhang/stella_en_1.5B_v5\")\n","\n","# Tokenize the filtered corpus\n","tokenized_train = tokenize_corpus(filtered_corpus_train, tokenizer_stella)\n","tokenized_val = tokenize_corpus(filtered_corpus_val, tokenizer_stella)\n","tokenized_test = tokenize_corpus(filtered_corpus_test, tokenizer_stella)\n","\n","# Move the model to the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_stella.to(device)\n","\n","# Run the model on the tokenized training data\n","train_outputs = run_model_on_corpus(tokenized_train, model_stella, device)\n","\n","# If you want to extract embeddings or further process the outputs, you can do so here.\n","# For example, to get the last hidden state:\n","train_hidden_states = [output.last_hidden_state for output in train_outputs]\n","\n","train_outputs\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"aJx52bFeUFsY","executionInfo":{"status":"ok","timestamp":1723734812529,"user_tz":-120,"elapsed":83,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}},"outputId":"f69d1f81-f3be-4fcb-98fe-c2062b5fa290"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Load the tokenizer and model\\ntokenizer_stella = AutoTokenizer.from_pretrained(\"dunzhang/stella_en_1.5B_v5\")\\nmodel_stella = AutoModel.from_pretrained(\"dunzhang/stella_en_1.5B_v5\")\\n\\n# Tokenize the filtered corpus\\ntokenized_train = tokenize_corpus(filtered_corpus_train, tokenizer_stella)\\ntokenized_val = tokenize_corpus(filtered_corpus_val, tokenizer_stella)\\ntokenized_test = tokenize_corpus(filtered_corpus_test, tokenizer_stella)\\n\\n# Move the model to the GPU if available\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel_stella.to(device)\\n\\n# Run the model on the tokenized training data\\ntrain_outputs = run_model_on_corpus(tokenized_train, model_stella, device)\\n\\n# If you want to extract embeddings or further process the outputs, you can do so here.\\n# For example, to get the last hidden state:\\ntrain_hidden_states = [output.last_hidden_state for output in train_outputs]\\n\\ntrain_outputs\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"YtgfEAKWh7ph"},"source":["Usage of BAAI/bge-en-icl which is the number one NLP model on the leaderboard.\n","\n","https://huggingface.co/BAAI/bge-en-icl"]},{"cell_type":"code","source":["# Load the pipeline for the \"BAAI/bge-en-icl\" model\n","bge_pipeline = pipeline(\"feature-extraction\", model=\"BAAI/bge-en-icl\", tokenizer=\"BAAI/bge-en-icl\", device=0 if torch.cuda.is_available() else -1)\n","\n","# Run the model on the filtered training data using the pipeline\n","train_outputs_bge = run_model_on_corpus_with_pipeline(filtered_corpus_train, bge_pipeline)\n","\n","# If you want to extract embeddings or further process the outputs, you can do so here.\n","# For example, the outputs will be in a list of numpy arrays or tensors depending on the pipeline.\n","\n","train_outputs_bge  # This will contain the model outputs, typically embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"hgdKCzCNg5yc","executionInfo":{"status":"error","timestamp":1723735197483,"user_tz":-120,"elapsed":538,"user":{"displayName":"Jannik Wirtheim","userId":"09249189617833245480"}},"outputId":"58ad5f2b-ee14-4398-d313-20084f4459e6"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pipeline' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-183fe45e10f9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the pipeline for the \"BAAI/bge-en-icl\" model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbge_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature-extraction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BAAI/bge-en-icl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BAAI/bge-en-icl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the model on the filtered training data using the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_outputs_bge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_on_corpus_with_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_corpus_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbge_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rq9QunIHh7pi"},"outputs":[],"source":["#Implemented the model directly. But size is to big for free colab\n","'''\n","# Load the tokenizer and model\n","tokenizer_bge = AutoTokenizer.from_pretrained(\"BAAI/bge-en-icl\")\n","model_bge = AutoModel.from_pretrained(\"BAAI/bge-en-icl\")\n","\n","# Tokenize the filtered corpus\n","tokenized_train = tokenize_corpus(filtered_corpus_train, tokenizer_bge)\n","tokenized_val = tokenize_corpus(filtered_corpus_val, tokenizer_bge)\n","tokenized_test = tokenize_corpus(filtered_corpus_test, tokenizer_bge)\n","\n","# Move the model to the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_bge.to(device)\n","\n","# Run the model on the tokenized training data\n","train_outputs = run_model_on_corpus(tokenized_train, model_bge)\n","\n","# If you want to extract embeddings or further process the outputs, you can do so here.\n","# For example, to get the last hidden state:\n","train_hidden_states = [output.last_hidden_state for output in train_outputs]\n","'''"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}