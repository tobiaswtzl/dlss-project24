{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  c:\\Users\\wirth/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy's English tokenizer and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Collect words and find most common words in each cluster\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.isalnum() and token.text.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_words_from_cluster(df, cluster_number):\n",
    "    texts = df[df['cluster'] == cluster_number]['title_and_text_lemmatized']\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        words.extend(tokens)\n",
    "    return words\n",
    "\n",
    "def get_embeddings_dict(df):\n",
    "    embeddings_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        embedding = row.drop(['Unnamed: 0', 'word']).values.astype(float)\n",
    "        embeddings_dict[word] = embedding\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_text_embedding(text, embeddings_dict):\n",
    "    words = text.split()  # Simple tokenization, adjust as needed\n",
    "    word_embeddings = [embeddings_dict.get(word) for word in words if embeddings_dict.get(word) is not None]\n",
    "    if word_embeddings:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(len(next(iter(embeddings_dict.values()))))  # Default to zero vector\n",
    "    \n",
    "def get_top_words(words, num_common=10):\n",
    "    word_counts = Counter(words)\n",
    "    return [word for word, freq in word_counts.most_common(num_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster\n",
    "num_clusters = 5  # Adjust based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "df_text = pd.read_csv(\"data/preprocessed/total_posts.csv\")[[\"id\", \"title_and_text_lemmatized\"]]\n",
    "df_embeddings  = pd.read_csv(\"data/embeddings_best_model/embeddings_CBOW_total_posts.csv\")\n",
    "embeddings_dict = get_embeddings_dict(df_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in cluster 0:\n",
      "['climate', 'change', 'people', 'think', 'believe', 'like', 'know', 'real', 'cause', 'trump']\n",
      "Top 10 words in cluster 1:\n",
      "['climate', 'change', 'world', 'new', 'year', 'people', 'global', 'r', 'energy', 'time']\n",
      "Top 10 words in cluster 2:\n",
      "['climate', 'change', 'trump', 'new', 'world', 'news', 'report', 'fight', 'global', 'study']\n",
      "Top 10 words in cluster 3:\n",
      "['climate', 'change', 'global', 'world', 'real', 'fight', 'scientist', 'cause', 'new', 'trump']\n",
      "Top 10 words in cluster 4:\n",
      "['climate', 'change', 'fight', 'world', 'trump', 'new', 'need', 'help', 'combat', 'want']\n"
     ]
    }
   ],
   "source": [
    "## prepare\n",
    "# Step 2: Aggregate embeddings for each text\n",
    "df_text['embedding'] = df_text['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "# Convert the aggregated embeddings into an array for clustering\n",
    "X = np.vstack(df_text['embedding'].values)\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "df_text['cluster'] = clusters\n",
    "\n",
    "top_words_per_cluster = {}\n",
    "for cluster_num in range(num_clusters):\n",
    "    words_in_cluster = get_words_from_cluster(df_text, cluster_num)\n",
    "    top_words_per_cluster[cluster_num] = get_top_words(words_in_cluster)\n",
    "    print(f\"Top 10 words in cluster {cluster_num}:\")\n",
    "    print(top_words_per_cluster[cluster_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen\n",
      "askwomen\n"
     ]
    }
   ],
   "source": [
    "list_finetuning_models = list(range(2010, 2023)) \n",
    "list_finetuning_models = [\"askmen\", \"askwomen\"]\n",
    "\n",
    "for subgroup in list_finetuning_models:\n",
    "    print(subgroup)\n",
    "\n",
    "    data  = pd.read_csv(f\"data/preprocessed/posts_{subgroup}.csv\")\n",
    "    df_embeddings  = pd.read_csv(f\"data/embeddings_year_and_reddits/embeddings_CBOW_posts_{subgroup}.csv\")\n",
    "    embeddings_dict = get_embeddings_dict(df_embeddings)\n",
    "    \n",
    "    ## prepare\n",
    "    # Step 2: Aggregate embeddings for each text\n",
    "    df_text['embedding'] = df_text['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "    # Convert the aggregated embeddings into an array for clustering\n",
    "    X = np.vstack(df_text['embedding'].values)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    df_text['cluster'] = clusters\n",
    "\n",
    "    cluster_words = []\n",
    "    for cluster_num in range(num_clusters):\n",
    "        words_in_cluster = get_words_from_cluster(df_text, cluster_num)\n",
    "        top_words = get_top_words(words_in_cluster)\n",
    "        cluster_words.append({'cluster': cluster_num, 'top_10_words': top_words})\n",
    "        \n",
    "    os.makedirs(f\"data/output/topics/\", exist_ok=True)\n",
    "    df_top_words = pd.DataFrame(cluster_words)\n",
    "    df_top_words.to_csv(f\"data/output/topics/topics_{subgroup}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
