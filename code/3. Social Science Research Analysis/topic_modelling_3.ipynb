{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  d:\\dlss-project24/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy's English tokenizer and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Collect words and find most common words in each cluster\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.isalnum() and token.text.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_words_from_cluster(df, cluster_number):\n",
    "    texts = df[df['cluster'] == cluster_number]['title_and_text_lemmatized']\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        words.extend(tokens)\n",
    "    return words\n",
    "\n",
    "def get_embeddings_dict(df):\n",
    "    embeddings_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        embedding = row.drop(['Unnamed: 0', 'word']).values.astype(float)\n",
    "        embeddings_dict[word] = embedding\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_text_embedding(text, embeddings_dict):\n",
    "    words = text.split()  # Simple tokenization, adjust as needed\n",
    "    word_embeddings = [embeddings_dict.get(word) for word in words if embeddings_dict.get(word) is not None]\n",
    "    if word_embeddings:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(len(next(iter(embeddings_dict.values()))))  # Default to zero vector\n",
    "    \n",
    "def get_top_words(words, num_common=10):\n",
    "    word_counts = Counter(words)\n",
    "    return [word for word, freq in word_counts.most_common(num_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster\n",
    "num_clusters = 3  # Adjust based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "   cluster                                       top_10_words\n",
      "0        0  [global, new, scientist, warming, fight, repor...\n",
      "1        1  [global, warming, science, world, scientist, e...\n",
      "2        2  [science, world, global, scientist, new, peopl...\n"
     ]
    }
   ],
   "source": [
    "list_finetuning_models = list(range(2010, 2011)) \n",
    "\n",
    "for subgroup in list_finetuning_models:\n",
    "    print(subgroup)\n",
    "\n",
    "    data  = pd.read_csv(local_path + f\"data/preprocessed/posts_{subgroup}.csv\")\n",
    "    df_embeddings  = pd.read_csv(local_path + f\"output/embeddings/yearly_embeddings/embeddings_CBOW_posts_{subgroup}.csv\")\n",
    "    embeddings_dict = get_embeddings_dict(df_embeddings)\n",
    "    \n",
    "    ## prepare\n",
    "    # Step 2: Aggregate embeddings for each text\n",
    "    data['embedding'] = data['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "    # Convert the aggregated embeddings into an array for clustering\n",
    "    X = np.vstack(data['embedding'].values)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    data['cluster'] = clusters\n",
    "\n",
    "    cluster_words = []\n",
    "    for cluster_num in range(num_clusters):\n",
    "        words_in_cluster = get_words_from_cluster(data, cluster_num)\n",
    "        top_words = get_top_words(words_in_cluster)\n",
    "        cluster_words.append({'cluster': cluster_num, 'top_10_words': top_words})\n",
    "        \n",
    "    os.makedirs(f\"data/output/topics/\", exist_ok=True)\n",
    "    df_top_words = pd.DataFrame(cluster_words)\n",
    "    df_top_words['top_10_words'] = df_top_words['top_10_words'].apply(lambda x: [word for word in x if word not in ['climate', 'change']])\n",
    "    df_top_words.to_csv(local_path + f\"output/topic_modelling/topics_{subgroup}.csv\")\n",
    "    print(df_top_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
