{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  d:\\dlss-project24/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy's English tokenizer and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing, converting to lowercase, and removing stop words and non-alphanumeric tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of processed tokens.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)  # Tokenize and process the text using a natural language processing model (e.g., SpaCy)\n",
    "    \n",
    "    # Convert tokens to lowercase, remove stop words and non-alphanumeric tokens\n",
    "    tokens = [token.text.lower() for token in doc if token.text.isalnum() and token.text.lower() not in stop_words]\n",
    "    \n",
    "    return tokens  # Return the list of processed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_cluster(df, cluster_number):\n",
    "    \"\"\"\n",
    "    Retrieves all words from the texts in a specific cluster.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing text data and cluster assignments.\n",
    "        cluster_number (int): The cluster number for which to extract words.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of words from the specified cluster.\n",
    "    \"\"\"\n",
    "    # Select texts belonging to the specified cluster\n",
    "    texts = df[df['cluster'] == cluster_number]['title_and_text_lemmatized']\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    # Process each text and collect words\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)  # Preprocess the text to extract tokens\n",
    "        words.extend(tokens)  # Add the tokens to the list of words\n",
    "    \n",
    "    return words  # Return the list of words for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_dict(df):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping words to their corresponding embeddings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing words and their embeddings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are words and the values are embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    # Iterate over each row in the DataFrame to build the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']  # Extract the word\n",
    "        # Extract the embedding vector, excluding unnecessary columns\n",
    "        embedding = row.drop(['Unnamed: 0', 'word']).values.astype(float)\n",
    "        embeddings_dict[word] = embedding  # Add the word and its embedding to the dictionary\n",
    "    \n",
    "    return embeddings_dict  # Return the dictionary of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Computes the average embedding for a given text based on the word embeddings.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which to compute the embedding.\n",
    "        embeddings_dict (dict): A dictionary mapping words to their embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The average embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    # Simple tokenization, splitting text into words\n",
    "    words = text.split()  \n",
    "    \n",
    "    # Get the embedding for each word, if it exists in the embeddings dictionary\n",
    "    word_embeddings = [embeddings_dict.get(word) for word in words if embeddings_dict.get(word) is not None]\n",
    "    \n",
    "    if word_embeddings:\n",
    "        # Compute the mean of the embeddings to get the text's embedding\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if no embeddings were found for any words\n",
    "        return np.zeros(len(next(iter(embeddings_dict.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(words, num_common=10):\n",
    "    \"\"\"\n",
    "    Identifies the most common words in a list of words.\n",
    "\n",
    "    Args:\n",
    "        words (list of str): The list of words to analyze.\n",
    "        num_common (int): The number of top common words to return.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of the most common words.\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)  # Count the frequency of each word\n",
    "    return [word for word, freq in word_counts.most_common(num_common)]  # Return the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of clusters\n",
    "num_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n",
      "   cluster                                       top_10_words\n",
      "0        0  [global, new, scientist, warming, fight, repor...\n",
      "1        1  [global, warming, science, world, scientist, e...\n",
      "2        2  [science, world, global, scientist, new, peopl...\n"
     ]
    }
   ],
   "source": [
    "# Create a list containing a single year, 2010, as the range is exclusive of the upper bound\n",
    "list_finetuning_models = list(range(2010, 2011))\n",
    "\n",
    "# Iterate over the list of years (in this case, it will only be 2010)\n",
    "for subgroup in list_finetuning_models:\n",
    "    print(subgroup)  # Print the current year\n",
    "\n",
    "    # Load the preprocessed data for the current year from a CSV file\n",
    "    data = pd.read_csv(local_path + f\"data/preprocessed/posts_{subgroup}.csv\")\n",
    "    \n",
    "    # Load the precomputed embeddings for the posts in the current year\n",
    "    df_embeddings = pd.read_csv(local_path + f\"output/embeddings/yearly_embeddings/embeddings_CBOW_posts_{subgroup}.csv\")\n",
    "    \n",
    "    # Convert the embeddings dataframe into a dictionary for easier lookup\n",
    "    embeddings_dict = get_embeddings_dict(df_embeddings)\n",
    "    \n",
    "    ## Prepare the data for clustering\n",
    "    # Step 2: Aggregate embeddings for each text by applying the `get_text_embedding` function\n",
    "    data['embedding'] = data['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "    \n",
    "    # Convert the list of embeddings into a 2D array, where each row corresponds to a text\n",
    "    X = np.vstack(data['embedding'].values)\n",
    "\n",
    "    # Perform KMeans clustering on the aggregated embeddings\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Assign the resulting cluster labels to the data\n",
    "    data['cluster'] = clusters\n",
    "\n",
    "    # Initialize a list to store the top words for each cluster\n",
    "    cluster_words = []\n",
    "    \n",
    "    # Iterate over each cluster\n",
    "    for cluster_num in range(num_clusters):\n",
    "        # Get all words from texts that belong to the current cluster\n",
    "        words_in_cluster = get_words_from_cluster(data, cluster_num)\n",
    "        \n",
    "        # Get the top words in the current cluster\n",
    "        top_words = get_top_words(words_in_cluster)\n",
    "        \n",
    "        # Store the cluster number and its top words in the list\n",
    "        cluster_words.append({'cluster': cluster_num, 'top_10_words': top_words})\n",
    "        \n",
    "    # Ensure the output directory for topics exists\n",
    "    os.makedirs(f\"data/output/topics/\", exist_ok=True)\n",
    "    \n",
    "    # Convert the list of top words per cluster into a DataFrame\n",
    "    df_top_words = pd.DataFrame(cluster_words)\n",
    "    \n",
    "    # Filter out the words 'climate' and 'change' from the top words list for each cluster\n",
    "    df_top_words['top_10_words'] = df_top_words['top_10_words'].apply(lambda x: [word for word in x if word not in ['climate', 'change']])\n",
    "    \n",
    "    # Save the top words per cluster to a CSV file\n",
    "    df_top_words.to_csv(local_path + f\"output/topic_modelling/topics_{subgroup}.csv\")\n",
    "    \n",
    "    # Print the DataFrame of top words\n",
    "    print(df_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
