{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load SpaCy's English tokenizer and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.isalnum() and token.text.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_words_from_cluster(df, cluster_number):\n",
    "    texts = df[df['cluster'] == cluster_number]['title_and_text_lemmatized']\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        words.extend(tokens)\n",
    "    return words\n",
    "\n",
    "def get_embeddings_dict(df):\n",
    "    embeddings_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        embedding = row.drop(['Unnamed: 0', 'word']).values.astype(float)\n",
    "        embeddings_dict[word] = embedding\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_text_embedding(text, embeddings_dict):\n",
    "    words = text.split()  # Simple tokenization, adjust as needed\n",
    "    word_embeddings = [embeddings_dict.get(word) for word in words if embeddings_dict.get(word) is not None]\n",
    "    if word_embeddings:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(len(next(iter(embeddings_dict.values()))))  # Default to zero vector\n",
    "    \n",
    "def get_top_words(words, num_common=10):\n",
    "    word_counts = Counter(words)\n",
    "    return [word for word, freq in word_counts.most_common(num_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster  \n",
    "num_clusters = 3  # Adjust based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_finetuning_models = list(range(2010, 2011)) \n",
    "\n",
    "for subgroup in list_finetuning_models:\n",
    "    print(subgroup)\n",
    "\n",
    "    data  = pd.read_csv(local_path + f\"data/preprocessed/posts_{subgroup}.csv\")\n",
    "    df_embeddings  = pd.read_csv(local_path + f\"output/embeddings/yearly_embeddings/embeddings_CBOW_posts_{subgroup}.csv\")\n",
    "    embeddings_dict = get_embeddings_dict(df_embeddings)\n",
    "    \n",
    "    ## prepare\n",
    "    # Step 2: Aggregate embeddings for each text\n",
    "    data['embedding'] = data['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "    # Convert the aggregated embeddings into an array for clustering\n",
    "    X = np.vstack(data['embedding'].values)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    data['cluster'] = clusters\n",
    "\n",
    "    cluster_words = []\n",
    "    for cluster_num in range(num_clusters):\n",
    "        words_in_cluster = get_words_from_cluster(data, cluster_num)\n",
    "        top_words = get_top_words(words_in_cluster)\n",
    "        cluster_words.append({'cluster': cluster_num, 'top_10_words': top_words})\n",
    "        \n",
    "    os.makedirs(f\"data/output/topics/\", exist_ok=True)\n",
    "    df_top_words = pd.DataFrame(cluster_words)\n",
    "    df_top_words['top_10_words'] = df_top_words['top_10_words'].apply(lambda x: [word for word in x if word not in ['climate', 'change']])\n",
    "    df_top_words.to_csv(local_path + f\"output/topic_modelling/topics_{subgroup}.csv\")\n",
    "    print(df_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Original data\n",
    "data = {\n",
    "    'Year': [2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012, 2013, 2013, 2013, 2014, 2014, 2014, 2015, 2015, 2015, \n",
    "             2016, 2016, 2016, 2017, 2017, 2017, 2018, 2018, 2018, 2019, 2019, 2019, 2020, 2020, 2020, 2021, 2021, 2021, \n",
    "             2022, 2022, 2022],\n",
    "    'Topic': ['Climate Change and Scientific Reporting', 'Global Energy and Environmental Science', \n",
    "              'Global Impact of Climate Change', 'Climate Change and Scientific Reporting', \n",
    "              'Causes and Scientific Study of Global Warming', 'Climate Change and Scientific Reporting', \n",
    "              'Global Impact and Scientific Analysis of Hurricanes', 'Scientific Studies on Global Causes and Weather Events', \n",
    "              'Global Warming and Scientific Understanding', 'Global Warming and Scientific Studies', \n",
    "              'Scientific Action Plans and Climate Change', 'Climate Action and Denial in Global Reports', \n",
    "              'Global Warming and Its Impact on People', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and Denial in Global Reports', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and Global Leadership', 'Climate Action and Global Leadership', \n",
    "              'Climate Change and Scientific Reporting', 'Climate Action and Global Leadership', \n",
    "              'Climate Action and Global Leadership', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action and Global Leadership', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action and Public Awareness', \n",
    "              'Global Action and Leadership', 'Climate Action and Global Protest', \n",
    "              'Global Action and Leadership', 'Climate Action and U.S. Leadership', 'Covid', \n",
    "              'Climate Action', 'Global Leadership and Climate Action', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action', \n",
    "              'Global Leadership and Climate Action', 'Climate Action and U.S. Leadership']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping original topics to 5 overarching topics\n",
    "topic_mapping = {\n",
    "    'Climate Change and Scientific Reporting': 'Climate Change and Scientific Reporting',\n",
    "    'Global Warming and Scientific Studies': 'Climate Change and Scientific Reporting',\n",
    "    'Global Impact of Climate Change': 'Climate Change and Scientific Reporting',\n",
    "    'Causes and Scientific Study of Global Warming': 'Climate Change and Scientific Reporting',\n",
    "    'Global Warming and Scientific Understanding': 'Climate Change and Scientific Reporting',\n",
    "    'Global Energy and Environmental Science': 'Climate Change and Scientific Reporting',\n",
    "    \n",
    "    'Climate Action and Global Leadership': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and U.S. Leadership': 'Climate Action and Global Leadership',\n",
    "    'Global Leadership and Climate Action': 'Climate Action and Global Leadership',\n",
    "    'Scientific Action Plans and Climate Change': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and Public Awareness': 'Climate Action and Global Leadership',\n",
    "    'Global Action and Leadership': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and Global Protest': 'Climate Action and Global Leadership',\n",
    "        \n",
    "    'Covid': 'Catastrophes',\n",
    "    'Global Impact and Scientific Analysis of Hurricanes': 'Catastrophes',\n",
    "    'Scientific Studies on Global Causes and Weather Events': 'Catastrophes',\n",
    "}\n",
    "\n",
    "# Applying the mapping\n",
    "df['Overarching Topic'] = df['Topic'].map(topic_mapping)\n",
    "\n",
    "# Count occurrences of each overarching topic by year\n",
    "topic_trend = df.groupby(['Year', 'Overarching Topic']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the trends over time\n",
    "plt.figure(figsize=(14, 8))\n",
    "topic_trend.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.xlabel('')  # Increase the font size of the x-axis label\n",
    "plt.ylabel('Frequency', fontsize=25)  # Increase the font size of the y-axis label\n",
    "plt.xticks(range(topic_trend.index.min(), topic_trend.index.max() + 1, 1), size = 20)  # X-axis steps of 1 year\n",
    "plt.yticks(range(0, int(topic_trend.values.max()) + 2, 1), size = 25)  # Y-axis steps of 1\n",
    "plt.legend(title='Topic', bbox_to_anchor=(0.5, 0.975), loc='upper center', ncol=3, fontsize=15, title_fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(local_path + \"plots/topics_over_time.jpg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
