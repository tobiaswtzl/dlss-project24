{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the code is running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True  # If this block runs, we are on Colab\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    # Mount Google Drive to access files\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False  # If an ImportError occurs, we are not on Colab\n",
    "    # Get the current working directory\n",
    "    current_wd = os.getcwd()\n",
    "    # Move one directory up to go to the main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "# Print the determined local path\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load SpaCy's English tokenizer and stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by tokenizing it, converting to lowercase, \n",
    "    removing stop words, and retaining only alphanumeric tokens.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of processed tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "    # Lowercase, filter out stop words and non-alphanumeric tokens\n",
    "    tokens = [token.text.lower() for token in doc if token.text.isalnum() and token.text.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def get_words_from_cluster(df, cluster_number):\n",
    "    \"\"\"\n",
    "    Extracts and preprocesses words from the texts in a specified cluster.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing the text data with a 'cluster' column.\n",
    "    cluster_number (int): The cluster number to filter the texts.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of preprocessed words from the specified cluster.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to get texts belonging to the specified cluster\n",
    "    texts = df[df['cluster'] == cluster_number]['title_and_text_lemmatized']\n",
    "    words = []\n",
    "    # Preprocess each text and accumulate the words\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        words.extend(tokens)\n",
    "    return words\n",
    "\n",
    "def get_embeddings_dict(df):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping words to their corresponding embeddings.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing word embeddings with a 'word' column.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are words and values are their embeddings (numpy arrays).\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    # Iterate through each row to build the embeddings dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']\n",
    "        # Extract the embedding by dropping unnecessary columns and converting to float\n",
    "        embedding = row.drop(['Unnamed: 0', 'word']).values.astype(float)\n",
    "        embeddings_dict[word] = embedding\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_text_embedding(text, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Computes the average embedding for a given text based on word embeddings.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be embedded.\n",
    "    embeddings_dict (dict): Dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The average embedding of the text, or a zero vector if no words are found.\n",
    "    \"\"\"\n",
    "    # Tokenize the text (simple tokenization)\n",
    "    words = text.split()  \n",
    "    # Retrieve the embeddings for each word in the text\n",
    "    word_embeddings = [embeddings_dict.get(word) for word in words if embeddings_dict.get(word) is not None]\n",
    "    if word_embeddings:\n",
    "        # Return the average of the word embeddings\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if no word embeddings are found\n",
    "        return np.zeros(len(next(iter(embeddings_dict.values()))))  # Default to zero vector\n",
    "\n",
    "def get_top_words(words, num_common=10):\n",
    "    \"\"\"\n",
    "    Identifies the most common words from a list of words.\n",
    "\n",
    "    Args:\n",
    "    words (list): A list of words.\n",
    "    num_common (int, optional): The number of top common words to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of the most common words.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    # Return the top 'num_common' most common words\n",
    "    return [word for word, freq in word_counts.most_common(num_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust number of clusters  \n",
    "num_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models for fine-tuning, specified by years (in this case, just 2010)\n",
    "list_finetuning_models = list(range(2010, 2011)) \n",
    "\n",
    "# Iterate over each model (year) in the list\n",
    "for subgroup in list_finetuning_models:\n",
    "    print(subgroup)\n",
    "\n",
    "    # Load the preprocessed data for the specific year\n",
    "    data  = pd.read_csv(local_path + f\"data/preprocessed/posts_{subgroup}.csv\")\n",
    "    \n",
    "    # Load the embeddings for the specific year\n",
    "    df_embeddings  = pd.read_csv(local_path + f\"output/embeddings/yearly_embeddings/embeddings_CBOW_posts_{subgroup}.csv\")\n",
    "    \n",
    "    # Create a dictionary mapping words to their embeddings\n",
    "    embeddings_dict = get_embeddings_dict(df_embeddings)\n",
    "    \n",
    "    ## Prepare the data for clustering\n",
    "    # Step 2: Aggregate embeddings for each text by computing the average embedding\n",
    "    data['embedding'] = data['title_and_text_lemmatized'].apply(lambda text: get_text_embedding(text, embeddings_dict))\n",
    "    \n",
    "    # Convert the list of embeddings into a 2D array suitable for clustering\n",
    "    X = np.vstack(data['embedding'].values)\n",
    "\n",
    "    # Perform K-means clustering on the embeddings\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Assign the cluster labels to the DataFrame\n",
    "    data['cluster'] = clusters\n",
    "\n",
    "    cluster_words = []\n",
    "    # For each cluster, extract the most common words\n",
    "    for cluster_num in range(num_clusters):\n",
    "        # Get the words associated with the current cluster\n",
    "        words_in_cluster = get_words_from_cluster(data, cluster_num)\n",
    "        \n",
    "        # Identify the top 10 most common words in the cluster\n",
    "        top_words = get_top_words(words_in_cluster)\n",
    "        \n",
    "        # Append the cluster number and its top words to the list\n",
    "        cluster_words.append({'cluster': cluster_num, 'top_10_words': top_words})\n",
    "        \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(f\"data/output/topics/\", exist_ok=True)\n",
    "    \n",
    "    # Create a DataFrame from the list of cluster words\n",
    "    df_top_words = pd.DataFrame(cluster_words)\n",
    "    \n",
    "    # Filter out generic words such as 'climate' and 'change' from the top words\n",
    "    df_top_words['top_10_words'] = df_top_words['top_10_words'].apply(lambda x: [word for word in x if word not in ['climate', 'change']])\n",
    "    \n",
    "    # Save the top words for each cluster to a CSV file\n",
    "    df_top_words.to_csv(local_path + f\"output/topic_modelling/topics_{subgroup}.csv\")\n",
    "    \n",
    "    # Print the DataFrame with the top words for inspection\n",
    "    print(df_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "data = {\n",
    "    'Year': [2010, 2010, 2010, 2011, 2011, 2011, 2012, 2012, 2012, 2013, 2013, 2013, 2014, 2014, 2014, 2015, 2015, 2015, \n",
    "             2016, 2016, 2016, 2017, 2017, 2017, 2018, 2018, 2018, 2019, 2019, 2019, 2020, 2020, 2020, 2021, 2021, 2021, \n",
    "             2022, 2022, 2022],\n",
    "    'Topic': ['Climate Change and Scientific Reporting', 'Global Energy and Environmental Science', \n",
    "              'Global Impact of Climate Change', 'Climate Change and Scientific Reporting', \n",
    "              'Causes and Scientific Study of Global Warming', 'Climate Change and Scientific Reporting', \n",
    "              'Global Impact and Scientific Analysis of Hurricanes', 'Scientific Studies on Global Causes and Weather Events', \n",
    "              'Global Warming and Scientific Understanding', 'Global Warming and Scientific Studies', \n",
    "              'Scientific Action Plans and Climate Change', 'Climate Action and Denial in Global Reports', \n",
    "              'Global Warming and Its Impact on People', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and Denial in Global Reports', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and Global Leadership', 'Climate Action and Global Leadership', \n",
    "              'Climate Change and Scientific Reporting', 'Climate Action and Global Leadership', \n",
    "              'Climate Action and Global Leadership', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Change and Scientific Reporting', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action and Global Leadership', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action and Public Awareness', \n",
    "              'Global Action and Leadership', 'Climate Action and Global Protest', \n",
    "              'Global Action and Leadership', 'Climate Action and U.S. Leadership', 'Covid', \n",
    "              'Climate Action', 'Global Leadership and Climate Action', \n",
    "              'Climate Action and U.S. Leadership', 'Climate Action', \n",
    "              'Global Leadership and Climate Action', 'Climate Action and U.S. Leadership']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping original topics to 5 overarching topics\n",
    "topic_mapping = {\n",
    "    'Climate Change and Scientific Reporting': 'Climate Change and Scientific Reporting',\n",
    "    'Global Warming and Scientific Studies': 'Climate Change and Scientific Reporting',\n",
    "    'Global Impact of Climate Change': 'Climate Change and Scientific Reporting',\n",
    "    'Causes and Scientific Study of Global Warming': 'Climate Change and Scientific Reporting',\n",
    "    'Global Warming and Scientific Understanding': 'Climate Change and Scientific Reporting',\n",
    "    'Global Energy and Environmental Science': 'Climate Change and Scientific Reporting',\n",
    "    \n",
    "    'Climate Action and Global Leadership': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and U.S. Leadership': 'Climate Action and Global Leadership',\n",
    "    'Global Leadership and Climate Action': 'Climate Action and Global Leadership',\n",
    "    'Scientific Action Plans and Climate Change': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and Public Awareness': 'Climate Action and Global Leadership',\n",
    "    'Global Action and Leadership': 'Climate Action and Global Leadership',\n",
    "    'Climate Action and Global Protest': 'Climate Action and Global Leadership',\n",
    "        \n",
    "    'Covid': 'Catastrophes',\n",
    "    'Global Impact and Scientific Analysis of Hurricanes': 'Catastrophes',\n",
    "    'Scientific Studies on Global Causes and Weather Events': 'Catastrophes',\n",
    "}\n",
    "\n",
    "# Applying the mapping\n",
    "df['Overarching Topic'] = df['Topic'].map(topic_mapping)\n",
    "\n",
    "# Count occurrences of each overarching topic by year\n",
    "topic_trend = df.groupby(['Year', 'Overarching Topic']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the trends over time\n",
    "plt.figure(figsize=(14, 8))\n",
    "topic_trend.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.xlabel('')  # Increase the font size of the x-axis label\n",
    "plt.ylabel('Frequency', fontsize=25)  # Increase the font size of the y-axis label\n",
    "plt.xticks(range(topic_trend.index.min(), topic_trend.index.max() + 1, 1), size = 20)  # X-axis steps of 1 year\n",
    "plt.yticks(range(0, int(topic_trend.values.max()) + 2, 1), size = 25)  # Y-axis steps of 1\n",
    "plt.legend(title='Topic', bbox_to_anchor=(0.5, 0.975), loc='upper center', ncol=3, fontsize=15, title_fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(local_path + \"plots/topics_over_time.jpg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
