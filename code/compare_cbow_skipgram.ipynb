{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  d:\\dlss-project24/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(current_wd) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_embeddings_dict(df):\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = row['word']  # Assuming the word column is named 'word'\n",
    "        vector = row.iloc[3:].to_numpy(dtype=np.float32)  # Convert remaining columns to numpy array\n",
    "        embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def compute_similarities(word_pairs, embeddings):\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            vec1 = embeddings[word1].reshape(1, -1)\n",
    "            vec2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "        else:\n",
    "            similarity = 0  # Handle OOV words\n",
    "        similarities.append(similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cbow_df = pd.read_csv(local_path + \"data/embeddings/embeddings_CBOW_sampled_100k_all_text.csv\")\n",
    "embeddings_skipgram_df = pd.read_csv(local_path + \"data/embeddings/embeddings_skipgram_sampled_100k_all_text.csv\")\n",
    "\n",
    "## to dict\n",
    "skipgram_embeddings = df_to_embeddings_dict(embeddings_skipgram_df)\n",
    "cbow_embeddings = df_to_embeddings_dict(embeddings_cbow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (Mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission</td>\n",
       "      <td>ticket</td>\n",
       "      <td>5.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>4.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>6.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "      <td>2.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>news</td>\n",
       "      <td>7.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>weapon</td>\n",
       "      <td>secret</td>\n",
       "      <td>2.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>5.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>news</td>\n",
       "      <td>1.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>wood</td>\n",
       "      <td>forest</td>\n",
       "      <td>7.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>word</td>\n",
       "      <td>similarity</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1      Word 2  Human (Mean)\n",
       "0       admission      ticket        5.5360\n",
       "1         alcohol   chemistry        4.1250\n",
       "2        aluminum       metal        6.6250\n",
       "3    announcement      effort        2.0625\n",
       "4    announcement        news        7.1875\n",
       "..            ...         ...           ...\n",
       "348        weapon      secret        2.5000\n",
       "349       weather    forecast        5.4375\n",
       "350     Wednesday        news        1.1250\n",
       "351          wood      forest        7.9375\n",
       "352          word  similarity        0.8125\n",
       "\n",
       "[353 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim353_df = pd.read_csv(local_path + \"data/external_data/wordsim353crowd.csv\")\n",
    "\n",
    "## split up into list of word pairs and list of scores\n",
    "word_pairs = wordsim353_df[['Word 1', 'Word 2']].values.tolist()\n",
    "human_scores = wordsim353_df['Human (Mean)'].values\n",
    "\n",
    "wordsim353_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation (Skip-gram): 0.05\n",
      "Spearman Correlation (CBOW): 0.05\n"
     ]
    }
   ],
   "source": [
    "skipgram_similarities = compute_similarities(word_pairs, skipgram_embeddings)\n",
    "cbow_similarities = compute_similarities(word_pairs, cbow_embeddings)\n",
    "\n",
    "## calculate correlation\n",
    "correlation_skipgram = spearmanr(skipgram_similarities, human_scores).correlation\n",
    "correlation_cbow = spearmanr(cbow_similarities, human_scores).correlation\n",
    "\n",
    "# Step 5: Output the Results\n",
    "print(f\"Spearman Correlation (Skip-gram): {correlation_skipgram:.2f}\")\n",
    "print(f\"Spearman Correlation (CBOW): {correlation_cbow:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to lemmatize words\n",
    "def lemmatize_word(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_\n",
    "\n",
    "# Function to predict the fourth word in an analogy\n",
    "def predict_analogy_word(word_a, word_b, word_c, word_to_vec):\n",
    "    # Lemmatize the words to match the vocabulary\n",
    "    word_a = lemmatize_word(word_a)\n",
    "    word_b = lemmatize_word(word_b)\n",
    "    word_c = lemmatize_word(word_c)\n",
    "\n",
    "    # Retrieve the vectors for the words\n",
    "    vec_a = word_to_vec.get(word_a)\n",
    "    vec_b = word_to_vec.get(word_b)\n",
    "    vec_c = word_to_vec.get(word_c)\n",
    "\n",
    "    if vec_a is None or vec_b is None or vec_c is None:\n",
    "        return None\n",
    "\n",
    "    # Calculate the target vector: vec_b - vec_a + vec_c\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "\n",
    "    # Find the closest word to the target vector\n",
    "    best_word = None\n",
    "    best_similarity = float('inf')\n",
    "    \n",
    "    for word, vec in word_to_vec.items():\n",
    "        if word not in {word_a, word_b, word_c}:\n",
    "            similarity = cosine(target_vec, vec)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_word = word\n",
    "    \n",
    "    return best_word\n",
    "\n",
    "def evaluate_analogy_dataset(analogy_df, word_to_vec):\n",
    "    correct = 0\n",
    "    total = len(analogy_df)\n",
    "\n",
    "    for index, row in analogy_df.iterrows():\n",
    "        word_a = row['Word 1']\n",
    "        word_b = row['Word 2']\n",
    "        word_c = row['Word 3']\n",
    "        expected_word_d = row['Expected Word']\n",
    "\n",
    "        predicted_word_d = predict_analogy_word(word_a, word_b, word_c, word_to_vec)\n",
    "\n",
    "        # Compare the predicted word with the expected word\n",
    "        if predicted_word_d == lemmatize_word(expected_word_d):\n",
    "            correct += 1\n",
    "        \n",
    "        print(f\"Analogy: {word_a} is to {word_b} as {word_c} is to {predicted_word_d} (Expected: {expected_word_d})\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Example DataFrame of analogies\n",
    "data_climate_change = {\n",
    "    'Word 1': ['coal', 'fossil', 'emission', 'climate', 'carbon', 'global', 'electric', 'renewable', 'methane', 'solar',\n",
    "               'carbon', 'temperature', 'ice', 'deforestation', 'dioxide', 'sea', 'wind', 'greenhouse', 'sustainability', 'pollution'],\n",
    "    'Word 2': ['fossil', 'energy', 'reduction', 'warming', 'dioxide', 'warming', 'vehicle', 'energy', 'gas', 'power',\n",
    "               'dioxide', 'rise', 'melt', 'reduction', 'gas', 'level', 'turbine', 'emissions', 'policy', 'impact'],\n",
    "    'Word 3': ['solar', 'renewable', 'pollution', 'environment', 'methane', 'cooling', 'car', 'wind', 'CO2', 'wind',\n",
    "               'carbon', 'impact', 'melting', 'forestation', 'gas', 'carbon', 'turbine', 'mitigation', 'climate', 'change'],\n",
    "    'Expected Word': ['renewable', 'resource', 'control', 'sustainability', 'gas', 'cooling', 'car', 'turbine', 'greenhouse', 'energy',\n",
    "                      'emission', 'rise', 'melt', 'deforestation', 'gas', 'level', 'wind', 'policy', 'impact', 'policy']}\n",
    "\n",
    "analogy_df_climate_change = pd.DataFrame(data_climate_change)\n",
    "\n",
    "data_reddit = {\n",
    "    'Word 1': ['OP', 'thread', 'TL;DR', 'upvote', 'troll', 'mod', 'AMA', 'lurker', 'NSFW', 'flair',\n",
    "               'comment', 'karma', 'subreddit', 'post', 'reply', 'ban', 'meme', 'user', 'admin', 'tag'],\n",
    "    'Word 2': ['post', 'discussion', 'summary', 'downvote', 'bait', 'admin', 'Q&A', 'reader', 'SFW', 'label',\n",
    "               'reply', 'points', 'community', 'thread', 'comment', 'ban', 'GIF', 'user', 'moderator', 'badge'],\n",
    "    'Word 3': ['comment', 'reply', 'context', 'upvote', 'spam', 'user', 'ask', 'reader', 'explicit', 'flair',\n",
    "               'upvote', 'comment', 'thread', 'discussion', 'report', 'image', 'moderator', 'poster', 'sub', 'message'],\n",
    "    'Expected Word': ['reply', 'discussion', 'summary', 'downvote', 'bait', 'admin', 'Q&A', 'lurker', 'SFW', 'tag',\n",
    "                      'comment', 'points', 'subreddit', 'post', 'reply', 'ban', 'sticker', 'user', 'admin', 'flair']}\n",
    "\n",
    "analogy_df_reddit = pd.DataFrame(data_reddit)\n",
    "\n",
    "data_politics_climate = {\n",
    "    'Word 1': ['EPA', 'Paris Agreement', 'Biden', 'UN', 'Congress', 'carbon tax', 'renewable energy', 'climate bill', 'COP26', 'Green New Deal',\n",
    "               'regulation', 'emissions', 'legislation', 'government', 'policy', 'administration', 'carbon footprint', 'international', 'president', 'senator'],\n",
    "    'Word 2': ['regulation', 'international accord', 'administration', 'global body', 'legislature', 'carbon pricing', 'clean energy', 'policy', 'summit', 'policy',\n",
    "               'rules', 'treaty', 'law', 'leadership', 'initiative', 'impact', 'effort', 'negotiation', 'leader', 'law'],\n",
    "    'Word 3': ['Paris Agreement', 'UN', 'Biden', 'G7', 'senator', 'cap-and-trade', 'climate action', 'agenda', 'COP21', 'climate legislation',\n",
    "               'treaty', 'agreement', 'regulation', 'administration', 'program', 'target', 'campaign', 'deal', 'conference', 'bill'],\n",
    "    'Expected Word': ['international accord', 'agreement', 'administration', 'global body', 'congress', 'carbon pricing', 'clean energy', 'policy', 'summit', 'policy',\n",
    "                      'regulation', 'treaty', 'law', 'leadership', 'initiative', 'impact', 'effort', 'negotiation', 'leader', 'law']\n",
    "}\n",
    "\n",
    "analogy_df_politics_climate = pd.DataFrame(data_politics_climate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dlss-project24\\venv\\Lib\\site-packages\\scipy\\spatial\\distance.py:647: RuntimeWarning: divide by zero encountered in divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: coal is to fossil as solar is to oddball (Expected: renewable)\n",
      "Analogy: fossil is to energy as renewable is to antler (Expected: resource)\n",
      "Analogy: emission is to reduction as pollution is to antler (Expected: control)\n",
      "Analogy: climate is to warming as environment is to synagogue (Expected: sustainability)\n",
      "Analogy: carbon is to dioxide as methane is to antler (Expected: gas)\n",
      "Analogy: global is to warming as cooling is to synagogue (Expected: cooling)\n",
      "Analogy: electric is to vehicle as car is to synagogue (Expected: car)\n",
      "Analogy: renewable is to energy as wind is to synagogue (Expected: turbine)\n",
      "Analogy: methane is to gas as CO2 is to None (Expected: greenhouse)\n",
      "Analogy: solar is to power as wind is to antler (Expected: energy)\n",
      "Analogy: carbon is to dioxide as carbon is to antler (Expected: emission)\n",
      "Analogy: temperature is to rise as impact is to antler (Expected: rise)\n",
      "Analogy: ice is to melt as melting is to antler (Expected: melt)\n",
      "Analogy: deforestation is to reduction as forestation is to synagogue (Expected: deforestation)\n",
      "Analogy: dioxide is to gas as gas is to synagogue (Expected: gas)\n",
      "Analogy: sea is to level as carbon is to antler (Expected: level)\n",
      "Analogy: wind is to turbine as turbine is to synagogue (Expected: wind)\n",
      "Analogy: greenhouse is to emissions as mitigation is to synagogue (Expected: policy)\n",
      "Analogy: sustainability is to policy as climate is to shedding (Expected: impact)\n",
      "Analogy: pollution is to impact as change is to synagogue (Expected: policy)\n",
      "\n",
      "Analogy Task Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_cbow_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_climate_change:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: OP is to post as comment is to synagogue (Expected: reply)\n",
      "Analogy: thread is to discussion as reply is to reus (Expected: discussion)\n",
      "Analogy: TL;DR is to summary as context is to None (Expected: summary)\n",
      "Analogy: upvote is to downvote as upvote is to synagogue (Expected: downvote)\n",
      "Analogy: troll is to bait as spam is to shedding (Expected: bait)\n",
      "Analogy: mod is to admin as user is to synagogue (Expected: admin)\n",
      "Analogy: AMA is to Q&A as ask is to None (Expected: Q&A)\n",
      "Analogy: lurker is to reader as reader is to antler (Expected: lurker)\n",
      "Analogy: NSFW is to SFW as explicit is to None (Expected: SFW)\n",
      "Analogy: flair is to label as flair is to synagogue (Expected: tag)\n",
      "Analogy: comment is to reply as upvote is to antler (Expected: comment)\n",
      "Analogy: karma is to points as comment is to synagogue (Expected: points)\n",
      "Analogy: subreddit is to community as thread is to synagogue (Expected: subreddit)\n",
      "Analogy: post is to thread as discussion is to antler (Expected: post)\n",
      "Analogy: reply is to comment as report is to synagogue (Expected: reply)\n",
      "Analogy: ban is to ban as image is to antler (Expected: ban)\n",
      "Analogy: meme is to GIF as moderator is to None (Expected: sticker)\n",
      "Analogy: user is to user as poster is to synagogue (Expected: user)\n",
      "Analogy: admin is to moderator as sub is to antler (Expected: admin)\n",
      "Analogy: tag is to badge as message is to synagogue (Expected: flair)\n",
      "\n",
      "Analogy Task Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## reddit related words:\n",
    "accuracy_cbow_reddit = evaluate_analogy_dataset(analogy_df_reddit, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_reddit:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: EPA is to regulation as Paris Agreement is to None (Expected: international accord)\n",
      "Analogy: Paris Agreement is to international accord as UN is to None (Expected: agreement)\n",
      "Analogy: Biden is to administration as Biden is to None (Expected: administration)\n",
      "Analogy: UN is to global body as G7 is to None (Expected: global body)\n",
      "Analogy: Congress is to legislature as senator is to None (Expected: congress)\n",
      "Analogy: carbon tax is to carbon pricing as cap-and-trade is to synagogue (Expected: carbon pricing)\n",
      "Analogy: renewable energy is to clean energy as climate action is to synagogue (Expected: clean energy)\n",
      "Analogy: climate bill is to policy as agenda is to synagogue (Expected: policy)\n",
      "Analogy: COP26 is to summit as COP21 is to None (Expected: summit)\n",
      "Analogy: Green New Deal is to policy as climate legislation is to None (Expected: policy)\n",
      "Analogy: regulation is to rules as treaty is to shedding (Expected: regulation)\n",
      "Analogy: emissions is to treaty as agreement is to synagogue (Expected: treaty)\n",
      "Analogy: legislation is to law as regulation is to antler (Expected: law)\n",
      "Analogy: government is to leadership as administration is to antler (Expected: leadership)\n",
      "Analogy: policy is to initiative as program is to shedding (Expected: initiative)\n",
      "Analogy: administration is to impact as target is to antler (Expected: impact)\n",
      "Analogy: carbon footprint is to effort as campaign is to shedding (Expected: effort)\n",
      "Analogy: international is to negotiation as deal is to reus (Expected: negotiation)\n",
      "Analogy: president is to leader as conference is to synagogue (Expected: leader)\n",
      "Analogy: senator is to law as bill is to synagogue (Expected: law)\n",
      "\n",
      "Analogy Task Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## reddit related words:\n",
    "accuracy_cbow_reddit = evaluate_analogy_dataset(analogy_df_politics_climate, cbow_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_cbow_reddit:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: coal is to fossil as solar is to oddball (Expected: renewable)\n",
      "Analogy: fossil is to energy as renewable is to antler (Expected: resource)\n",
      "Analogy: emission is to reduction as pollution is to antler (Expected: control)\n",
      "Analogy: climate is to warming as environment is to synagogue (Expected: sustainability)\n",
      "Analogy: carbon is to dioxide as methane is to antler (Expected: gas)\n",
      "Analogy: global is to warming as cooling is to synagogue (Expected: cooling)\n",
      "Analogy: electric is to vehicle as car is to synagogue (Expected: car)\n",
      "Analogy: renewable is to energy as wind is to synagogue (Expected: turbine)\n",
      "Analogy: methane is to gas as CO2 is to None (Expected: greenhouse)\n",
      "Analogy: solar is to power as wind is to antler (Expected: energy)\n",
      "Analogy: carbon is to dioxide as carbon is to antler (Expected: emission)\n",
      "Analogy: temperature is to rise as impact is to antler (Expected: rise)\n",
      "Analogy: ice is to melt as melting is to antler (Expected: melt)\n",
      "Analogy: deforestation is to reduction as forestation is to synagogue (Expected: deforestation)\n",
      "Analogy: dioxide is to gas as gas is to synagogue (Expected: gas)\n",
      "Analogy: sea is to level as carbon is to antler (Expected: level)\n",
      "Analogy: wind is to turbine as turbine is to synagogue (Expected: wind)\n",
      "Analogy: greenhouse is to emissions as mitigation is to synagogue (Expected: policy)\n",
      "Analogy: sustainability is to policy as climate is to shedding (Expected: impact)\n",
      "Analogy: pollution is to impact as change is to synagogue (Expected: policy)\n",
      "\n",
      "Analogy Task Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## Climate Change related words:\n",
    "accuracy_skipram_climate_change = evaluate_analogy_dataset(analogy_df_climate_change, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipram_climate_change:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy: OP is to comment as post is to ask (Expected: OP)\n",
      "Analogy: thread is to discussion as reply is to difficult (Expected: discussion)\n",
      "Analogy: TL;DR is to summary as context is to None (Expected: summary)\n",
      "Analogy: upvote is to downvote as upvote is to None (Expected: downvote)\n",
      "Analogy: troll is to bait as spam is to None (Expected: bait)\n",
      "Analogy: mod is to admin as user is to None (Expected: admin)\n",
      "Analogy: AMA is to Q&A as ask is to None (Expected: Q&A)\n",
      "Analogy: lurker is to poster as reader is to None (Expected: lurker)\n",
      "Analogy: NSFW is to SFW as explicit is to None (Expected: SFW)\n",
      "Analogy: flair is to tag as label is to None (Expected: tag)\n",
      "\n",
      "Analogy Task Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## reddit related words:\n",
    "accuracy_skipgram_reddit = evaluate_analogy_dataset(analogy_df_reddit, skipgram_embeddings)\n",
    "print(f\"\\nAnalogy Task Accuracy: {accuracy_skipgram_reddit:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
