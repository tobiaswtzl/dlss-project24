{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to treat # and other punctuation\n",
    "- Implement BPE\n",
    "- Implement WordPiece\n",
    "- Implement Spacy\n",
    "- Implement Unigram\n",
    "- Compare size of vocab\n",
    "- Compare performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_bpe = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_wp = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_spacy = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments_unigram = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")\n",
    "df_comments = pd.read_csv(\"/Users/fabianmahner/dlss-project24/data/preprocessed/comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_comments_bpe[\"body\"].tolist()\n",
    "tokenizer.train_from_iterator(texts, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https, ://, www, ., pen, medi, ain, c, ., com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[Nice, try, ,, but, false, equivalency, is, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[I, ', m, slightly, torn, on, this, ,, the, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[*, Artic, le, cont, ents, as, always, :, *, &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[What, is, the, current, out, look, for, ocean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[The, usual, reason, -, their, policies, most,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[it, ', s, not, deep, though, ,, it, is, a, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[**, Hello, u, /, Ultim, ate, We, b, Red, di, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[&amp;, gt, ;, New, conventional, nuclear, plants,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[Just, look, through, your, comment, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [https, ://, www, ., pen, medi, ain, c, ., com...  \n",
       "1     [Nice, try, ,, but, false, equivalency, is, st...  \n",
       "2     [I, ', m, slightly, torn, on, this, ,, the, id...  \n",
       "3     [*, Artic, le, cont, ents, as, always, :, *, &...  \n",
       "4     [What, is, the, current, out, look, for, ocean...  \n",
       "...                                                 ...  \n",
       "1995  [The, usual, reason, -, their, policies, most,...  \n",
       "1996  [it, ', s, not, deep, though, ,, it, is, a, su...  \n",
       "1997  [**, Hello, u, /, Ultim, ate, We, b, Red, di, ...  \n",
       "1998  [&, gt, ;, New, conventional, nuclear, plants,...  \n",
       "1999  [Just, look, through, your, comment, history, ...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to tokenize a single text\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_comments_bpe['tokens'] = df_comments_bpe['body'].apply(tokenize_text)\n",
    "\n",
    "# Display the dataframe with tokens\n",
    "df_comments_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unl_token\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, normalizers\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))\n",
    "\n",
    "# Normalize the text (optional but recommended)\n",
    "tokenizer.normalizer = normalizers.BertNormalizer()\n",
    "\n",
    "# Use a pre-tokenizer to split text into words\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define a trainer\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer on the dataframe's text column\n",
    "texts = df_comments_wp['body'].tolist()\n",
    "tokenizer.train_from_iterator(texts, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https, ://, www, ., pen, ##medi, ##ain, ##c, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[nice, try, ,, but, false, equivalency, is, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[i, ', m, slightly, torn, on, this, ,, the, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[*, article, content, ##s, as, always, :, ##*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[what, is, the, current, outl, ##oo, ##k, for,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[the, usual, reason, -, their, policies, most,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[it, ', s, not, deep, though, ,, it, is, a, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[**, hello, u, /, ultimate, ##web, ##reddit, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[&amp;, gt, ;, new, conventional, nuclear, plants,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[just, look, through, your, comment, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [https, ://, www, ., pen, ##medi, ##ain, ##c, ...  \n",
       "1     [nice, try, ,, but, false, equivalency, is, st...  \n",
       "2     [i, ', m, slightly, torn, on, this, ,, the, id...  \n",
       "3     [*, article, content, ##s, as, always, :, ##*,...  \n",
       "4     [what, is, the, current, outl, ##oo, ##k, for,...  \n",
       "...                                                 ...  \n",
       "1995  [the, usual, reason, -, their, policies, most,...  \n",
       "1996  [it, ', s, not, deep, though, ,, it, is, a, su...  \n",
       "1997  [**, hello, u, /, ultimate, ##web, ##reddit, #...  \n",
       "1998  [&, gt, ;, new, conventional, nuclear, plants,...  \n",
       "1999  [just, look, through, your, comment, history, ...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to tokenize a single text\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_comments_wp['tokens'] = df_comments_wp['body'].apply(tokenize_text)\n",
    "\n",
    "# Display the dataframe with tokens\n",
    "df_comments_wp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmahner/dlss-project24/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define a custom tokenizer function\n",
    "def custom_tokenizer(nlp):\n",
    "    def tokenizer(text):\n",
    "        tokens = bert_tokenizer.tokenize(text)\n",
    "        return Doc(nlp.vocab, words=tokens)\n",
    "    return tokenizer\n",
    "\n",
    "# Create a spaCy pipeline and replace its tokenizer with the custom tokenizer\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a single text\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_comments_spacy['tokens'] = df_comments_spacy['body'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[https, :, /, /, www, ., pen, ##media, ##in, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[nice, try, ,, but, false, e, ##qui, ##vale, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[i, ', m, slightly, torn, on, this, ,, the, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[*, article, contents, as, always, :, *, &amp;, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[what, is, the, current, outlook, for, ocean, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[the, usual, reason, -, their, policies, most,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[it, ', s, not, deep, though, ,, it, is, a, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[*, *, hello, u, /, ultimate, ##we, ##bre, ##d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[&amp;, gt, ;, new, conventional, nuclear, plants,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[just, look, through, your, comment, history, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [https, :, /, /, www, ., pen, ##media, ##in, #...  \n",
       "1     [nice, try, ,, but, false, e, ##qui, ##vale, #...  \n",
       "2     [i, ', m, slightly, torn, on, this, ,, the, id...  \n",
       "3     [*, article, contents, as, always, :, *, &, am...  \n",
       "4     [what, is, the, current, outlook, for, ocean, ...  \n",
       "...                                                 ...  \n",
       "1995  [the, usual, reason, -, their, policies, most,...  \n",
       "1996  [it, ', s, not, deep, though, ,, it, is, a, su...  \n",
       "1997  [*, *, hello, u, /, ultimate, ##we, ##bre, ##d...  \n",
       "1998  [&, gt, ;, new, conventional, nuclear, plants,...  \n",
       "1999  [just, look, through, your, comment, history, ...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=text_data.txt --model_prefix=unigram --vocab_size=2000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_data.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text_data.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4376 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 6632 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 1 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1375685\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9511% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=96\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999511\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 6565 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=684687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 50173 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 6565\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 30462\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 30462 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19236 obj=12.0883 num_tokens=81624 num_tokens/piece=4.24329\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17062 obj=9.81572 num_tokens=82203 num_tokens/piece=4.8179\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12793 obj=9.85112 num_tokens=87085 num_tokens/piece=6.80724\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12778 obj=9.81085 num_tokens=87111 num_tokens/piece=6.81726\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9583 obj=10.0075 num_tokens=94407 num_tokens/piece=9.85151\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9582 obj=9.95592 num_tokens=94398 num_tokens/piece=9.8516\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7186 obj=10.2281 num_tokens=102879 num_tokens/piece=14.3166\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7186 obj=10.1698 num_tokens=102886 num_tokens/piece=14.3176\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5389 obj=10.4885 num_tokens=112280 num_tokens/piece=20.835\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5389 obj=10.428 num_tokens=112263 num_tokens/piece=20.8319\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4041 obj=10.7758 num_tokens=121506 num_tokens/piece=30.0683\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4041 obj=10.7113 num_tokens=121504 num_tokens/piece=30.0678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3030 obj=11.1013 num_tokens=130515 num_tokens/piece=43.0743\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3030 obj=11.0314 num_tokens=130516 num_tokens/piece=43.0746\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2272 obj=11.4782 num_tokens=139842 num_tokens/piece=61.5502\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2272 obj=11.401 num_tokens=139849 num_tokens/piece=61.5533\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=11.4476 num_tokens=141023 num_tokens/piece=64.1014\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=11.4394 num_tokens=141029 num_tokens/piece=64.1041\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Save text data to a file as required by sentencepiece\n",
    "with open('text_data.txt', 'w') as f:\n",
    "    for text in df_comments_unigram['body']:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "# Train a Unigram tokenizer\n",
    "spm.SentencePieceTrainer.Train('--input=text_data.txt --model_prefix=unigram --vocab_size=2000 --model_type=unigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Load the trained Unigram model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"unigram.model\")\n",
    "\n",
    "# Define a custom tokenizer function\n",
    "def custom_tokenizer(nlp):\n",
    "    def tokenizer_func(text):\n",
    "        tokens = sp.encode_as_pieces(text)\n",
    "        return Doc(nlp.vocab, words=tokens)\n",
    "    return tokenizer_func\n",
    "\n",
    "# Create a spaCy pipeline and replace its tokenizer with the custom tokenizer\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>f1bk4k3</td>\n",
       "      <td>https://www.penmediainc.com/2019/09/23/climate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[▁http, s, ://, www, ., p, en, me, di, ain, c,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>frqfnw7</td>\n",
       "      <td>Nice try, but false equivalency is still false...</td>\n",
       "      <td>nice try but false equivalency is still false ...</td>\n",
       "      <td>nice try but false equivalency be still false ...</td>\n",
       "      <td>[▁N, ice, ▁try, ,, ▁but, ▁false, ▁e, q, u, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dh9dp98</td>\n",
       "      <td>I'm slightly torn on this, the idea of nations...</td>\n",
       "      <td>i slightly torn on this the idea of nations tr...</td>\n",
       "      <td>I slightly tear on this the idea of nation try...</td>\n",
       "      <td>[▁I, ', m, ▁, s, l, ight, ly, ▁to, r, n, ▁on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i1vluk4</td>\n",
       "      <td>*Article contents as always:*\\n\\n&amp;amp;#x200B;\\...</td>\n",
       "      <td>article contents as always south australian pr...</td>\n",
       "      <td>article content as always south australian pre...</td>\n",
       "      <td>[▁*, A, r, tic, le, ▁, content, s, ▁as, ▁alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fr9nzyu</td>\n",
       "      <td>What is the current outlook for ocean acidific...</td>\n",
       "      <td>what is the current outlook for ocean acidific...</td>\n",
       "      <td>what be the current outlook for ocean acidific...</td>\n",
       "      <td>[▁What, ▁is, ▁the, ▁current, ▁out, l, ook, ▁fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>h8859gb</td>\n",
       "      <td>The usual reason - their policies most closely...</td>\n",
       "      <td>the usual reason their policies most closely a...</td>\n",
       "      <td>the usual reason their policy most closely ali...</td>\n",
       "      <td>[▁The, ▁us, u, al, ▁reason, ▁-, ▁their, ▁polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>fqo3dlb</td>\n",
       "      <td>it's not deep though, it is a surface level ob...</td>\n",
       "      <td>it not deep though it is a surface level obser...</td>\n",
       "      <td>it not deep though it be a surface level obser...</td>\n",
       "      <td>[▁it, ', s, ▁not, ▁deep, ▁though, ,, ▁it, ▁is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>fhhct09</td>\n",
       "      <td>**Hello u/UltimateWebRedditor, unfortunately y...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>hello unfortunately your submission do blame c...</td>\n",
       "      <td>[▁**, H, el, lo, ▁, u, /, U, l, t, im, ate, W,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>h4s5us0</td>\n",
       "      <td>&amp;gt; New conventional nuclear plants... exorbi...</td>\n",
       "      <td>new conventional nuclear plants exorbitant cos...</td>\n",
       "      <td>new conventional nuclear plant exorbitant cost...</td>\n",
       "      <td>[▁&amp;, gt, ;, ▁New, ▁con, ve, n, tion, al, ▁nucl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>fzvvvcq</td>\n",
       "      <td>Just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>just look through your comment history real qu...</td>\n",
       "      <td>[▁Just, ▁look, ▁through, ▁your, ▁comment, ▁his...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                               body  \\\n",
       "0              0  f1bk4k3  https://www.penmediainc.com/2019/09/23/climate...   \n",
       "1              1  frqfnw7  Nice try, but false equivalency is still false...   \n",
       "2              2  dh9dp98  I'm slightly torn on this, the idea of nations...   \n",
       "3              3  i1vluk4  *Article contents as always:*\\n\\n&amp;#x200B;\\...   \n",
       "4              4  fr9nzyu  What is the current outlook for ocean acidific...   \n",
       "...          ...      ...                                                ...   \n",
       "1995        1995  h8859gb  The usual reason - their policies most closely...   \n",
       "1996        1996  fqo3dlb  it's not deep though, it is a surface level ob...   \n",
       "1997        1997  fhhct09  **Hello u/UltimateWebRedditor, unfortunately y...   \n",
       "1998        1998  h4s5us0  &gt; New conventional nuclear plants... exorbi...   \n",
       "1999        1999  fzvvvcq  Just look through your comment history real qu...   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency is still false ...   \n",
       "2     i slightly torn on this the idea of nations tr...   \n",
       "3     article contents as always south australian pr...   \n",
       "4     what is the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policies most closely a...   \n",
       "1996  it not deep though it is a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plants exorbitant cos...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0                                                   NaN   \n",
       "1     nice try but false equivalency be still false ...   \n",
       "2     I slightly tear on this the idea of nation try...   \n",
       "3     article content as always south australian pre...   \n",
       "4     what be the current outlook for ocean acidific...   \n",
       "...                                                 ...   \n",
       "1995  the usual reason their policy most closely ali...   \n",
       "1996  it not deep though it be a surface level obser...   \n",
       "1997  hello unfortunately your submission do blame c...   \n",
       "1998  new conventional nuclear plant exorbitant cost...   \n",
       "1999  just look through your comment history real qu...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [▁http, s, ://, www, ., p, en, me, di, ain, c,...  \n",
       "1     [▁N, ice, ▁try, ,, ▁but, ▁false, ▁e, q, u, i, ...  \n",
       "2     [▁I, ', m, ▁, s, l, ight, ly, ▁to, r, n, ▁on, ...  \n",
       "3     [▁*, A, r, tic, le, ▁, content, s, ▁as, ▁alway...  \n",
       "4     [▁What, ▁is, ▁the, ▁current, ▁out, l, ook, ▁fo...  \n",
       "...                                                 ...  \n",
       "1995  [▁The, ▁us, u, al, ▁reason, ▁-, ▁their, ▁polic...  \n",
       "1996  [▁it, ', s, ▁not, ▁deep, ▁though, ,, ▁it, ▁is,...  \n",
       "1997  [▁**, H, el, lo, ▁, u, /, U, l, t, im, ate, W,...  \n",
       "1998  [▁&, gt, ;, ▁New, ▁con, ve, n, tion, al, ▁nucl...  \n",
       "1999  [▁Just, ▁look, ▁through, ▁your, ▁comment, ▁his...  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df_comments_unigram['tokens'] = df_comments_unigram['body'].apply(tokenize_text)\n",
    "\n",
    "# Display the dataframe with tokens\n",
    "df_comments_unigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=text_data.txt --model_prefix=unigram --vocab_size=2000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text_data.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text_data.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (4376 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 6632 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 1 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1375685\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9511% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=96\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999511\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 6565 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=684687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 50173 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 6565\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 30462\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 30462 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19236 obj=12.0883 num_tokens=81624 num_tokens/piece=4.24329\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17062 obj=9.81572 num_tokens=82203 num_tokens/piece=4.8179\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12793 obj=9.85112 num_tokens=87085 num_tokens/piece=6.80724\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12778 obj=9.81085 num_tokens=87111 num_tokens/piece=6.81726\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9583 obj=10.0075 num_tokens=94407 num_tokens/piece=9.85151\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9582 obj=9.95592 num_tokens=94398 num_tokens/piece=9.8516\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7186 obj=10.2281 num_tokens=102879 num_tokens/piece=14.3166\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7186 obj=10.1698 num_tokens=102886 num_tokens/piece=14.3176\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5389 obj=10.4885 num_tokens=112280 num_tokens/piece=20.835\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5389 obj=10.428 num_tokens=112263 num_tokens/piece=20.8319\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4041 obj=10.7758 num_tokens=121506 num_tokens/piece=30.0683\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4041 obj=10.7113 num_tokens=121504 num_tokens/piece=30.0678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3030 obj=11.1013 num_tokens=130515 num_tokens/piece=43.0743\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3030 obj=11.0314 num_tokens=130516 num_tokens/piece=43.0746\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2272 obj=11.4782 num_tokens=139842 num_tokens/piece=61.5502\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2272 obj=11.401 num_tokens=139849 num_tokens/piece=61.5533\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=11.4476 num_tokens=141023 num_tokens/piece=64.1014\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=11.4394 num_tokens=141029 num_tokens/piece=64.1041\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenizer  Vocab Size\n",
      "0        BPE       10000\n",
      "1    Unigram        2000\n",
      "2  WordPiece       30522\n",
      "3      spaCy         764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabianmahner/dlss-project24/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer\n",
    "import spacy\n",
    "\n",
    "# Sample dataframe\n",
    "df_comments\n",
    "\n",
    "# BPE Tokenizer\n",
    "tokenizer_bpe = Tokenizer(models.BPE())\n",
    "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer_bpe = trainers.BpeTrainer(vocab_size=10000, min_frequency=2, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n",
    "texts = df_comments['body'].tolist()\n",
    "tokenizer_bpe.train_from_iterator(texts, trainer_bpe)\n",
    "vocab_bpe = tokenizer_bpe.get_vocab()\n",
    "vocab_size_bpe = len(vocab_bpe)\n",
    "\n",
    "# Unigram Tokenizer\n",
    "with open('text_data.txt', 'w') as f:\n",
    "    for text in df_comments['body']:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=text_data.txt --model_prefix=unigram --vocab_size=2000 --model_type=unigram')\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.Load(\"unigram.model\")\n",
    "vocab_size_unigram = len(sp_unigram)\n",
    "\n",
    "# WordPiece Tokenizer\n",
    "tokenizer_wordpiece = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_wordpiece = tokenizer_wordpiece.get_vocab()\n",
    "vocab_size_wordpiece = len(vocab_wordpiece)\n",
    "\n",
    "# spaCy Tokenizer\n",
    "nlp_spacy = spacy.blank(\"en\")\n",
    "vocab_size_spacy = len(nlp_spacy.vocab)\n",
    "\n",
    "# Collect the results\n",
    "results = {\n",
    "    \"Tokenizer\": [\"BPE\", \"Unigram\", \"WordPiece\", \"spaCy\"],\n",
    "    \"Vocab Size\": [vocab_size_bpe, vocab_size_unigram, vocab_size_wordpiece, vocab_size_spacy]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
