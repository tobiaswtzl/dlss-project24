{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOxw6kzt4p5FKYin1odCYK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiaswtzl/dlss-project24/blob/main/code/accuracy_and_fscore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48F79bnMohK5",
        "outputId": "2e635b22-c600-4214-c91a-5d9daa45d72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow\n",
            "Successfully installed pyarrow-14.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: ray 2.34.0 does not provide the extra 'debug'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    ## this is necessary to get ray running on colab\n",
        "    !pip uninstall -y -q pyarrow\n",
        "    !pip install pyarrow==14.0.1\n",
        "    !pip install -q -U ray[tune]\n",
        "    !pip install -q ray[debug]\n",
        "    ## force crash as restart is necessary due to the installed packages\n",
        "    import os\n",
        "    os._exit(0)\n",
        "\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "## check if on colab\n",
        "try:\n",
        "    import google.colab\n",
        "    in_colab = True\n",
        "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
        "    google.colab.drive.mount('/content/drive')\n",
        "\n",
        "except ImportError:\n",
        "    in_colab = False\n",
        "    ## get current directory\n",
        "    current_wd = os.getcwd()\n",
        "    ## move one up to go to main directory\n",
        "    local_path = os.path.dirname(current_wd) + \"/\"\n",
        "\n",
        "print(\"CWD: \", local_path)\n",
        "\n",
        "import ray\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.train import Checkpoint\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "\n",
        "\n",
        "\n",
        "#### Continuous bag of words model #####\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        # Embedding layer for word indices\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Linear layer for mapping embeddings to vocab size\n",
        "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Get embeddings for context words\n",
        "        embeds = self.embeddings(context)\n",
        "        # Average embeddings to get a single vector\n",
        "        combined = torch.mean(embeds, dim=1)\n",
        "        # Apply dropout and pass through linear layer\n",
        "        out = self.linear1(self.dropout(combined))\n",
        "        # Compute log probabilities\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, target):\n",
        "        embeds = self.embeddings(target)\n",
        "        out = self.linear1(self.dropout(embeds))\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "\n",
        "def filter_corpus(corpus, vocab_set):\n",
        "    return [[word for word in doc if word in vocab_set] for doc in corpus]\n",
        "\n",
        "# Function to create context-target pairs\n",
        "def create_context_target_pairs_cbow(text, context_size):\n",
        "    pairs = []\n",
        "    for sentence in text:\n",
        "        for i in range(context_size, len(sentence) - context_size):\n",
        "            context = sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
        "            target = sentence[i]\n",
        "            pairs.append((context, target))\n",
        "    return pairs\n",
        "\n",
        "# Function to create context-target pairs for Skip-gram\n",
        "def create_context_target_pairs_skipgram(text, context_size):\n",
        "    pairs = []\n",
        "    for sentence in text:\n",
        "        for i in range(len(sentence)):\n",
        "            target = sentence[i]\n",
        "            context = sentence[max(0, i - context_size):i] + sentence[i + 1:i + context_size + 1]\n",
        "            for ctx in context:\n",
        "                pairs.append((target, ctx))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Dataset and DataLoader definition\n",
        "class Word2VecDataset_cbow(Dataset):\n",
        "    def __init__(self, pairs, word_to_index):\n",
        "        self.pairs = pairs\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.pairs[idx]\n",
        "        context_idxs = torch.tensor([self.word_to_index[word] for word in context], dtype=torch.long)\n",
        "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
        "        return context_idxs, target_idx\n",
        "\n",
        "class Word2VecDataset_skipgram(Dataset):\n",
        "    def __init__(self, pairs, word_to_index):\n",
        "        self.pairs = pairs\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target, context = self.pairs[idx]\n",
        "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
        "        context_idx = torch.tensor(self.word_to_index[context], dtype=torch.long)\n",
        "        return target_idx, context_idx\n",
        "\n",
        "def prepare_data_for_model_training(file_name = \"str\", min_count = int, data_sample_name = str):\n",
        "    #### Parameters to choose:\n",
        "    ## get data\n",
        "    comments = pd.read_csv(local_path +f\"/data/preprocessed/{file_name}.csv\")\n",
        "\n",
        "    # Splitting the data into train, validation, and test sets\n",
        "    train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    #Adding all comments for generating the vocabulary. If not an error occurs when tokens missing\n",
        "    total_comments_list = comments[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "\n",
        "    train_list = train_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "    val_list = val_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "    test_list = test_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "\n",
        "    # Ensure each entry is a string and split each sentence into words\n",
        "    total_corpus = [doc.split() for doc in total_comments_list]\n",
        "    corpus_train = [doc.split() for doc in train_list]\n",
        "    corpus_val = [doc.split() for doc in val_list]\n",
        "    corpus_test = [doc.split() for doc in test_list]\n",
        "\n",
        "    # Create a vocabulary: count occurrences of each word\n",
        "    vocab = defaultdict(int)\n",
        "    for sentence in total_corpus:\n",
        "        for word in sentence:\n",
        "            vocab[word] += 1\n",
        "\n",
        "    # Remove infrequent words from the vocabulary\n",
        "    vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
        "\n",
        "    # Create word to index and index to word mappings\n",
        "    word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
        "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "    # Create DataFrame from vocabulary\n",
        "    vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
        "\n",
        "    vocab_set = set(vocab.keys())\n",
        "\n",
        "    filtered_total_corpus = filter_corpus(total_corpus, vocab_set)\n",
        "    filtered_corpus_train = filter_corpus(corpus_train, vocab_set)\n",
        "    filtered_corpus_val = filter_corpus(corpus_val, vocab_set)\n",
        "    filtered_corpus_test = filter_corpus(corpus_test, vocab_set)\n",
        "    return filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Class that implements early stopping to halt training when the validation loss stops improving.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    patience : int\n",
        "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
        "    min_delta : float\n",
        "        Minimum change in the validation loss to qualify as an improvement.\n",
        "\n",
        "    Methods:\n",
        "    ----------\n",
        "    __call__(val_loss, model)\n",
        "        Checks if the validation loss has improved and updates the state of early stopping.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    patience : int\n",
        "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
        "    min_delta : float\n",
        "        Minimum change in the validation loss to qualify as an improvement.\n",
        "    counter : int\n",
        "        Counter for the number of epochs since the last improvement.\n",
        "    best_loss : float or None\n",
        "        Best recorded validation loss.\n",
        "    early_stop : bool\n",
        "        Indicating whether training should be stopped early.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience= int, min_delta= float):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        ## for the first training iteration\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            ## check if the loss decreased, if not:\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            ## if loss decrease (more than the defined delta): save model parameters, reset counter and update best loss\n",
        "        else:\n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                self.counter = 0\n",
        "\n",
        "\n",
        "def train_model_cbow(config, data):\n",
        "    \"\"\"\n",
        "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    config : dict\n",
        "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
        "    data : tuple\n",
        "        Tuple containing training and validation datasets.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    dict\n",
        "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
        "        and lists of validation and training losses across epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
        "\n",
        "    train_pairs = create_context_target_pairs_cbow(filtered_corpus_train, config[\"context_size\"])\n",
        "    val_pairs = create_context_target_pairs_cbow(filtered_corpus_val, config[\"context_size\"])\n",
        "    train_dataset = Word2VecDataset_cbow(train_pairs, word_to_index)\n",
        "    val_dataset = Word2VecDataset_cbow(val_pairs, word_to_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "\n",
        "    ## set seed to replicate the model\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    ## empty lists to store loss\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    ## initialise model\n",
        "    model = CBOW(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
        "\n",
        "    ## loss criterion\n",
        "    loss_criterion = nn.NLLLoss()\n",
        "\n",
        "    ## choose optimiser\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    ## adapt learning rate with scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
        "\n",
        "    #### Early Stopper ####\n",
        "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
        "\n",
        "    #### Training ####\n",
        "    ## each epoch iterates through the whole dataset\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        ## train model on training set\n",
        "        model.train()\n",
        "        ## set loss and r2 to zero again so we start fresh\n",
        "        train_loss = 0\n",
        "        ## iterate through batches of the training data (data is the features and target the target)\n",
        "        for context_idxs, target_idx in train_loader:\n",
        "            ## send tensors to gpu\n",
        "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "            ## reset gradient to 0,start fresh again\n",
        "            optimizer.zero_grad()\n",
        "            ## predict target\n",
        "            log_probs = model(context_idxs)\n",
        "            ## caculate loss\n",
        "            loss = loss_criterion(log_probs, target_idx)\n",
        "            ## caculate gradients\n",
        "            loss.backward()\n",
        "            ## update weights\n",
        "            optimizer.step()\n",
        "            ## sum loss for all batches together\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "        #### Validation ####\n",
        "        ## check performance on validation set\n",
        "        model.eval()\n",
        "        ## set loss to zero again so we start fresh\n",
        "        val_loss_sum = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        ## as we test on the validation set, we do not want to update our weights now\n",
        "        with torch.no_grad():\n",
        "            for context_idxs, target_idx in val_loader:\n",
        "                ## send tensors to gpu\n",
        "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "                log_probs = model(context_idxs)\n",
        "                ## caculate loss\n",
        "                loss = loss_criterion(log_probs, target_idx)\n",
        "                ## sum loss for whole epoch\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                correct += (predicted_idx == target_idx).sum().item()\n",
        "                total += context_idxs.size(0)\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        ## adapt learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
        "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
        "          ##save checkpoint\n",
        "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "\n",
        "          ## report mertrics and save checkpoint\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      },\n",
        "                  checkpoint=Checkpoint.from_directory(\".\")\n",
        "                  )\n",
        "        else:\n",
        "          ##report only metrics\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      }\n",
        "                  )\n",
        "\n",
        "        #### Early stopping ####\n",
        "        # check if loss decreases more than defined threshold\n",
        "        early_stopper(val_loss, model)\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "    ## last checkpoint\n",
        "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "    ray.train.report(\n",
        "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
        "        checkpoint=Checkpoint.from_directory(\".\")\n",
        "        )\n",
        "\n",
        "    #return train_losses, val_losses, val_r2s\n",
        "    return {\n",
        "        \"loss\": round(early_stopper.best_loss, 2),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"val_loss_list\": val_losses,\n",
        "        \"train_loss_list\": train_losses\n",
        "        }\n",
        "\n",
        "def train_model_skipgram(config, data):\n",
        "    \"\"\"\n",
        "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    config : dict\n",
        "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
        "    data : tuple\n",
        "        Tuple containing training and validation datasets.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    dict\n",
        "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
        "        and lists of validation and training losses across epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
        "\n",
        "    train_pairs = create_context_target_pairs_skipgram(filtered_corpus_train, config[\"context_size\"])\n",
        "    val_pairs = create_context_target_pairs_skipgram(filtered_corpus_val, config[\"context_size\"])\n",
        "    train_dataset = Word2VecDataset_skipgram(train_pairs, word_to_index)\n",
        "    val_dataset = Word2VecDataset_skipgram(val_pairs, word_to_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "\n",
        "    ## set seed to replicate the model\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    ## empty lists to store loss\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    ## initialise model\n",
        "    model = SkipGram(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
        "\n",
        "    ## loss criterion\n",
        "    loss_criterion = nn.NLLLoss()\n",
        "\n",
        "    ## choose optimiser\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    ## adapt learning rate with scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
        "\n",
        "    #### Early Stopper ####\n",
        "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
        "\n",
        "    #### Training ####\n",
        "    ## each epoch iterates through the whole dataset\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        ## train model on training set\n",
        "        model.train()\n",
        "        ## set loss and r2 to zero again so we start fresh\n",
        "        train_loss = 0\n",
        "        ## iterate through batches of the training data (data is the features and target the target)\n",
        "        for context_idxs, target_idx in train_loader:\n",
        "            ## send tensors to gpu\n",
        "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "            ## reset gradient to 0,start fresh again\n",
        "            optimizer.zero_grad()\n",
        "            ## predict target\n",
        "            log_probs = model(target_idx)\n",
        "            ## caculate loss\n",
        "            loss = loss_criterion(log_probs, context_idxs)\n",
        "            ## caculate gradients\n",
        "            loss.backward()\n",
        "            ## update weights\n",
        "            optimizer.step()\n",
        "            ## sum loss for all batches together\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "        #### Validation ####\n",
        "        ## check performance on validation set\n",
        "        model.eval()\n",
        "        ## set loss to zero again so we start fresh\n",
        "        val_loss_sum = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        ## as we test on the validation set, we do not want to update our weights now\n",
        "        with torch.no_grad():\n",
        "            for context_idxs, target_idx in val_loader:\n",
        "                ## send tensors to gpu\n",
        "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "                log_probs = model(target_idx)\n",
        "                ## caculate loss\n",
        "                loss = loss_criterion(log_probs, context_idxs)\n",
        "                ## sum loss for whole epoch\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                correct += (predicted_idx == context_idxs).sum().item()\n",
        "                total += context_idxs.size(0)\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        ## adapt learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
        "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
        "          ##save checkpoint\n",
        "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "\n",
        "          ## report mertrics and save checkpoint\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      },\n",
        "                  checkpoint=Checkpoint.from_directory(\".\")\n",
        "                  )\n",
        "        else:\n",
        "          ##report only metrics\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      }\n",
        "                  )\n",
        "\n",
        "        #### Early stopping ####\n",
        "        # check if loss decreases more than defined threshold\n",
        "        early_stopper(val_loss, model)\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "    ## last checkpoint\n",
        "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "    ray.train.report(\n",
        "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
        "        checkpoint=Checkpoint.from_directory(\".\")\n",
        "        )\n",
        "\n",
        "    #return train_losses, val_losses, val_r2s\n",
        "    return {\n",
        "        \"loss\": round(early_stopper.best_loss, 2),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"val_loss_list\": val_losses,\n",
        "        \"train_loss_list\": train_losses\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "#### Tuning ####\n",
        "## Custom function to shorten ray file path names\n",
        "def short_dirname(trial) -> str:\n",
        "    \"\"\"\n",
        "    Function that shortens path names created by Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    trial : ray.tune.Trial\n",
        "        The Ray trial object for which the directory name is being created.\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    str\n",
        "        A shortened file path in the format 'trial_<trial_id>'.\n",
        "    \"\"\"\n",
        "    return \"trial_\" + str(trial.trial_id)\n",
        "\n",
        "\n",
        "## actual tuning\n",
        "def tune_parameters(training_function, num_samples, train_corpus, val_corpus, word_to_index, max_num_epochs, parameter_space, resources, local_path):\n",
        "    \"\"\"\n",
        "    Function that tunes the hyperparameters for a DL model using ASHA scheduling and saves the best model and tuning results locally.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    training_function : function\n",
        "        The function used for training the model during hyperparameter tuning.\n",
        "    num_samples : int\n",
        "        The number of hyperparameter samples to try.\n",
        "    train_dataset : object\n",
        "        Training dataset object.\n",
        "    val_dataset : object\n",
        "        Validation dataset object.\n",
        "    max_num_epochs : int\n",
        "        The maximum number of epochs for training each model.\n",
        "    parameter_space : dict\n",
        "        Dictionary defining the hyperparameter search space.\n",
        "    resources : dict\n",
        "        Resources configuration for training.\n",
        "    local_path : str\n",
        "        Local path to save tuning results and best model.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    pandas.DataFrame\n",
        "        DataFrame containing the tuning results, sorted by loss.\n",
        "        \"\"\"\n",
        "\n",
        "    ## because min number of epochs in sampling range is 50\n",
        "    #assert max_num_epochs > 50\n",
        "\n",
        "    ## Hyperparameters to sample from\n",
        "    ## ASHA scheduler to increase efficiency and stop inefficient training configs\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=3,\n",
        "        reduction_factor=2\n",
        "    )\n",
        "\n",
        "    ## tuning function, choose resources\n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(\n",
        "            tune.with_parameters(\n",
        "                training_function,\n",
        "                data = (train_corpus, val_corpus, word_to_index)),\n",
        "                resources= resources\n",
        "        ),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "            trial_dirname_creator=short_dirname\n",
        "        ),\n",
        "        param_space= parameter_space,\n",
        "        run_config = ray.train.RunConfig(storage_path = local_path, name=\"run_\" + datetime.now().strftime(\"%m-%d_%H_%M\"))\n",
        "    )\n",
        "\n",
        "    results = tuner.fit()\n",
        "\n",
        "\n",
        "    #### Best Model ####\n",
        "\n",
        "    ## create folder\n",
        "    os.makedirs(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}', exist_ok=True)\n",
        "\n",
        "    ## get best model\n",
        "    best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "    ## save info about best model\n",
        "    with open(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_result_info_' + parameter_space[\"model\"] + '.pkl', 'wb') as file:\n",
        "        pickle.dump(best_result, file)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Best trial config: {}\".format(best_result.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
        "\n",
        "    ## get path to that best model\n",
        "    best_checkpoint_path = best_result.get_best_checkpoint(metric = \"loss\", mode = \"min\").path + \"/checkpoint_\"+ parameter_space[\"model\"] + \".pt\"\n",
        "    ## save path to model as txt\n",
        "    #with open(local_path + f\"/best_models_{parameter_space[\"folder_ending\"]}//path_best_model_\" + parameter_space[\"model\"] + \".txt\", \"w\") as file:\n",
        "    #    file.write(best_checkpoint_path)\n",
        "\n",
        "    ##save model parameters\n",
        "    best_checkpoint = torch.load(best_checkpoint_path)\n",
        "\n",
        "    ## create new model\n",
        "    if parameter_space[\"model\"] == \"CBOW\":\n",
        "        model_final = CBOW(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "    else:\n",
        "        model_final = SkipGram(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    ## load parameteres of best checkpoint\n",
        "    model_final.load_state_dict(best_checkpoint)\n",
        "\n",
        "    torch.save(model_final.state_dict(), local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_model_parameters_' + parameter_space[\"model\"] + '.pt')\n",
        "\n",
        "    #### Tuning Overview ####\n",
        "    ## Get results as df\n",
        "    df_tuning_results = results.get_dataframe()\n",
        "    ## Rename cols\n",
        "    df_tuning_results.columns = [col.replace('config/', '') for col in df_tuning_results.columns]\n",
        "    ## sort by loss\n",
        "    df_tuning_results.sort_values(\"loss\", inplace = True)\n",
        "    ## Save only relevant cols\n",
        "    df_tuning_results = df_tuning_results[['loss', \"accuracy\", \"context_size\", 'lr', 'batch_size', 'epochs',\n",
        "                                           'patience', 'min_delta', \"gamma\", \"step_size\",\n",
        "                                           \"dropout\", 'time_total_s', \"val_loss_list\", \"train_loss_list\"]]\n",
        "    ## Save as csv\n",
        "    df_tuning_results.to_csv(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/df_tuning_results' + parameter_space[\"model\"] + '.csv')\n",
        "\n",
        "    return df_tuning_results\n",
        "\n",
        "\n",
        " #### Replication ####\n",
        "def load_best_model(model_type = str, local_path = str, data_sample_name = str):\n",
        "    \"\"\"\n",
        "    Function that loads the best model based on the specified model type.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model to load (\"CNN\" or other).\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    torch.nn.Module\n",
        "        The best pre-trained model loaded on the device and ready for evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    ## get best config\n",
        "    with open(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl', 'rb') as file:\n",
        "    # Use pickle.dump() to write the data object to file\n",
        "        best_result = pickle.load(file)\n",
        "\n",
        "    #with open(f\"{local_path}/tuning_results/best_models/path_best_model_{model_type}.txt\") as file:\n",
        "    #    path_best_file = file.read()\n",
        "\n",
        "    ## load parameters of best model\n",
        "    best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n",
        "\n",
        "    ## create new model\n",
        "    if model_type == \"CBOW\":\n",
        "        model_final = CBOW(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "    else:\n",
        "        model_final = SkipGram(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = 500 ## hardcode to minimise code change. reason: had to stop tuning due to resource constraints\n",
        "            # thus, the model infos were not properly saved\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    ## load parameteres of best checkpoint\n",
        "    model_final.load_state_dict(best_checkpoint)\n",
        "    ## model into evaluation mode\n",
        "    model_final.eval()\n",
        "    model_final.to(device)\n",
        "\n",
        "    return model_final\n",
        "\n",
        "\n",
        "def plot_loss_curve(model_type = str, local_path = str, data_sample_name = str):\n",
        "\n",
        "   \"\"\"\n",
        "    Function that plots the training and validation loss curves of the best model.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model whose loss curve to plot (\"CNN\" or other).\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "   ## get file with loss data\n",
        "   with open(local_path + f\"/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl\", 'rb') as file:\n",
        "       best_result = pickle.load(file)\n",
        "\n",
        "   ## get respective tuning data\n",
        "   val_losses = best_result.metrics[\"val_loss_list\"]\n",
        "   ## i forgot to divide the train loss by n in the training function\n",
        "   ## and repeating that takes 8 hours, so I have to do it like this now\n",
        "   train_losses = best_result.metrics[\"train_loss_list\"]\n",
        "\n",
        "   ## create plot\n",
        "   plt.plot(train_losses, label='Training Loss')\n",
        "   plt.plot(val_losses, label='Validation Loss')\n",
        "   plt.title(f\"{model_type} loss curves\")\n",
        "   plt.xlabel('Epochs')\n",
        "   plt.ylabel('Loss')\n",
        "   plt.legend()\n",
        "   ## save\n",
        "   plt.savefig(local_path + f\"plots/loss_curve_{model_type}_{data_sample_name}.png\")\n",
        "   plt.show()\n",
        "\n",
        "\n",
        "def classify(model_type, dataloader, index_to_word, data_sample_name = str, include_true_vals=True):\n",
        "    model = load_best_model(model_type, local_path=local_path, data_sample_name = data_sample_name)\n",
        "\n",
        "    if model_type == \"CBOW\":\n",
        "        with torch.no_grad():\n",
        "            predictions, true_vals = [], []\n",
        "            for context_idx, target_idx in dataloader:\n",
        "                context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
        "                log_probs = model(context_idx)\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
        "\n",
        "                if include_true_vals:\n",
        "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in target_idx.cpu().numpy()])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            predictions, true_vals = [], []\n",
        "            for context_idx, target_idx in dataloader:\n",
        "                context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
        "                log_probs = model(target_idx)\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
        "\n",
        "                if include_true_vals:\n",
        "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in context_idx.cpu().numpy()])\n",
        "\n",
        "\n",
        "\n",
        "    if include_true_vals:\n",
        "        return predictions, true_vals\n",
        "    else:\n",
        "        return predictions\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_classification(model_type, prediction_val, prediction_test, y_val, y_test):\n",
        "    \"\"\"\n",
        "    Function that evaluates a classification model by computing accuracy, recall, precision, and F-score for the training and validation data, and optionally for the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model being evaluated.\n",
        "    prediction_val : array\n",
        "        Predictions made by the model on the validation dataset.\n",
        "    prediction_test : array\n",
        "        Predictions made by the model on the test dataset.\n",
        "    y_val : array\n",
        "        Actual target values in the validation dataset.\n",
        "    y_test : array\n",
        "        Actual target values in the test dataset.\n",
        "    final_testing : bool, default=False\n",
        "        Flag indicating whether to evaluate the model on the test dataset.\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(f\"\\n--------------------------\\n{model_type} Classification Evaluation \\n--------------------------\")\n",
        "\n",
        "\n",
        "    print(\"\\nValidation set\")\n",
        "    print(\"--------------\")\n",
        "\n",
        "    print(f\"F1 Score: {f1_score(y_true = y_val, y_pred = prediction_val, average = 'micro'):.2f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true = y_val, y_pred = prediction_val):.2f}\")\n",
        "\n",
        "    #print(classification_report(y_true = y_val, y_pred = prediction_val, digits=2, zero_division=0))\n",
        "    print(\"\\nTest set\")\n",
        "    print(\"--------\\n\")\n",
        "    print(f\"F1 Score: {f1_score(y_true = y_test, y_pred = prediction_test, average = 'micro'):.2f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true = y_test, y_pred = prediction_test):.2f}\")\n",
        "\n",
        "    #print(classification_report(y_true = y_test, y_pred = prediction_test, digits=2, zero_division=0))\n",
        "\n",
        "\n",
        "def extract_embeddings(model_type = str, local_path = str, data_sample_name = str, index_to_word = dict):\n",
        "    model = load_best_model(model_type = model_type, local_path=local_path, data_sample_name = data_sample_name)\n",
        "    embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "    # Create a DataFrame\n",
        "    embeddings_df = pd.DataFrame(embeddings)\n",
        "    embeddings_df.insert(0, 'word', [index_to_word[i] for i in range(len(embeddings_df))])\n",
        "    embeddings_df.to_csv(local_path + f\"data/embeddings/embeddings_{model_type}_{data_sample_name}.csv\")\n",
        "    return embeddings_df\n",
        "\n",
        "  ## use the gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUYq3QW4r_7G",
        "outputId": "4496b166-0127-454e-d127-ca0393ef8ded"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CWD:  /content/drive/MyDrive/DLSS/\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW"
      ],
      "metadata": {
        "id": "gGdPMJP5xEvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name = prepare_data_for_model_training(\n",
        "    file_name = \"total_posts\",\n",
        "    min_count = 40,\n",
        "    data_sample_name = \"total_posts\"\n",
        "    )\n",
        "\n",
        "context_size = 2\n",
        "\n",
        "val_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_val, context_size)\n",
        "test_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_test, context_size)\n",
        "\n",
        "val_dataset_cbow = Word2VecDataset_cbow(val_pairs_cbow, word_to_index)\n",
        "test_dataset_cbow = Word2VecDataset_cbow(test_pairs_cbow, word_to_index)\n",
        "\n",
        "val_loader_cbow = DataLoader(val_dataset_cbow, shuffle=False, batch_size = 50000)\n",
        "test_loader_cbow = DataLoader(test_dataset_cbow, shuffle=False, batch_size = 50000)\n",
        "\n",
        "prediction_val_cbow, true_val_cbow = classify(\n",
        "    model_type=\"CBOW\",\n",
        "    dataloader = val_loader_cbow,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name = data_sample_name\n",
        "    )\n",
        "prediction_test_cbow, true_test_cbow = classify(\n",
        "    model_type=\"CBOW\",\n",
        "    dataloader = test_loader_cbow,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name = data_sample_name\n",
        "    )\n",
        "\n",
        "evaluate_classification(\n",
        "    model_type = \"CBOW\",\n",
        "    prediction_val = prediction_val_cbow,\n",
        "    prediction_test = prediction_test_cbow,\n",
        "    y_val = true_val_cbow,\n",
        "    y_test = true_test_cbow\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "timh-68DoySp",
        "outputId": "b64f7d42-0f96-4d2e-9b9c-f03a688849fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------\n",
            "CBOW Classification Evaluation \n",
            "--------------------------\n",
            "\n",
            "Validation set\n",
            "--------------\n",
            "F1 Score: 0.18\n",
            "Accuracy: 0.18\n",
            "\n",
            "Test set\n",
            "--------\n",
            "\n",
            "F1 Score: 0.18\n",
            "Accuracy: 0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skipgram"
      ],
      "metadata": {
        "id": "SDuNYCrIxBgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_val, context_size)\n",
        "test_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_test, context_size)\n",
        "\n",
        "val_dataset_skipgram= Word2VecDataset_skipgram(val_pairs_skipgram, word_to_index)\n",
        "test_dataset_skipgram = Word2VecDataset_skipgram(test_pairs_skipgram, word_to_index)\n",
        "\n",
        "val_loader_skipgram = DataLoader(val_dataset_skipgram, shuffle=False, batch_size = 50000)\n",
        "test_loader_skipgram = DataLoader(test_dataset_skipgram, shuffle=False, batch_size = 50000)\n",
        "\n",
        "prediction_val_skipgram, true_val_skipgram = classify(\n",
        "    model_type=\"skipgram\",\n",
        "    dataloader = val_loader_skipgram,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name = data_sample_name\n",
        "    )\n",
        "prediction_test_skipgram, true_test_skipgram = classify(\n",
        "    model_type=\"skipgram\",\n",
        "    dataloader = test_loader_skipgram,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name = data_sample_name\n",
        "    )\n",
        "\n",
        "evaluate_classification(\n",
        "    model_type = \"skipgram\",\n",
        "    prediction_val = prediction_val_skipgram,\n",
        "    prediction_test = prediction_test_skipgram,\n",
        "    y_val = true_val_skipgram,\n",
        "    y_test = true_test_skipgram\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrVjgPS6xJvB",
        "outputId": "139a7ff7-d021-4531-9eac-f618fa3601a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------\n",
            "skipgram Classification Evaluation \n",
            "--------------------------\n",
            "\n",
            "Validation set\n",
            "--------------\n",
            "F1 Score: 0.06\n",
            "Accuracy: 0.06\n",
            "\n",
            "Test set\n",
            "--------\n",
            "\n",
            "F1 Score: 0.06\n",
            "Accuracy: 0.06\n"
          ]
        }
      ]
    }
  ]
}