{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate Accuracy and F-Score for CBOW and Skipgram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiaswtzl/dlss-project24/blob/main/code/accuracy_and_fscore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48F79bnMohK5",
        "outputId": "2e635b22-c600-4214-c91a-5d9daa45d72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow\n",
            "Successfully installed pyarrow-14.0.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: ray 2.34.0 does not provide the extra 'debug'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    # Uninstall the current version of 'pyarrow', this is necessary to get ray running on colab\n",
        "    !pip uninstall -y -q pyarrow\n",
        "    \n",
        "    # Install 'pyarrow' version 14.0.1\n",
        "    !pip install pyarrow==14.0.1\n",
        "    \n",
        "    # Install the Ray package with Tune and Debug options\n",
        "    !pip install -q -U ray[tune]\n",
        "    !pip install -q ray[debug]\n",
        "    \n",
        "    # Force a crash to restart the environment (required for package changes to take effect)\n",
        "    import os\n",
        "    os._exit(0)\n",
        "\n",
        "except:\n",
        "    # If not in Google Colab or another error occurs, do nothing\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## Check if running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    in_colab = True  # Flag to indicate if running in Colab\n",
        "    local_path = \"/content/drive/MyDrive/DLSS/\"  # Set the path to the desired directory on Google Drive\n",
        "    google.colab.drive.mount('/content/drive')  # Mount Google Drive to access files\n",
        "\n",
        "except ImportError:\n",
        "    in_colab = False  # Flag to indicate if running locally\n",
        "    current_wd = os.getcwd()  # Get the current working directory\n",
        "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"  # Set path to the main directory\n",
        "\n",
        "print(\"CWD: \", local_path)  # Print the current working directory\n",
        "\n",
        "\n",
        "## Import necessary libraries for data processing, modeling, and visualization\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "## Import Ray for distributed computing and hyperparameter tuning\n",
        "import ray\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.train import Checkpoint\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "## Import metrics for model evaluation\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Continuous bag of words model #####\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        # Embedding layer for word indices\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Linear layer for mapping embeddings to vocab size\n",
        "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, context):\n",
        "        # Get embeddings for context words\n",
        "        embeds = self.embeddings(context)\n",
        "        # Average embeddings to get a single vector\n",
        "        combined = torch.mean(embeds, dim=1)\n",
        "        # Apply dropout and pass through linear layer\n",
        "        out = self.linear1(self.dropout(combined))\n",
        "        # Compute log probabilities\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        # Define an embedding layer that will learn word representations\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Define a linear layer that maps from the embedding space to the vocabulary space\n",
        "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
        "        # Define a dropout layer to prevent overfitting during training\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, target):\n",
        "        # Get the embeddings for the target words\n",
        "        embeds = self.embeddings(target)\n",
        "        # Apply dropout to the embeddings and pass them through the linear layer\n",
        "        out = self.linear1(self.dropout(embeds))\n",
        "        # Apply log softmax to the output to get log-probabilities over the vocabulary\n",
        "        log_probs = torch.log_softmax(out, dim=1)\n",
        "        return log_probs  # Return the log-probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_corpus(corpus, vocab_set):\n",
        "    \"\"\"\n",
        "    Filters the corpus by keeping only the words that are present in the provided vocabulary set.\n",
        "\n",
        "    Args:\n",
        "        corpus (list of list of str): A list where each element is a list of words representing a document.\n",
        "        vocab_set (set): A set containing the vocabulary words to be retained in the corpus.\n",
        "\n",
        "    Returns:\n",
        "        list of list of str: The filtered corpus with only words present in `vocab_set`.\n",
        "    \"\"\"\n",
        "    # Iterate through each document in the corpus and retain only the words that are in vocab_set\n",
        "    return [[word for word in doc if word in vocab_set] for doc in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_context_target_pairs_cbow(text, context_size):\n",
        "    \"\"\"\n",
        "    Creates context-target pairs for training a Continuous Bag of Words (CBOW) model.\n",
        "\n",
        "    In a CBOW model, the context consists of words surrounding a target word, and the model \n",
        "    aims to predict the target word given its context.\n",
        "\n",
        "    Args:\n",
        "        text (list of list of str): A list of sentences, where each sentence is a list of words.\n",
        "        context_size (int): The number of words to consider on each side of the target word for the context.\n",
        "\n",
        "    Returns:\n",
        "        list of tuple: A list of tuples where each tuple contains a context (list of words) and a target word (str).\n",
        "    \"\"\"\n",
        "    pairs = []  # Initialize an empty list to store context-target pairs\n",
        "\n",
        "    # Iterate over each sentence in the text\n",
        "    for sentence in text:\n",
        "        # Iterate over each word in the sentence, ignoring the edges where the full context can't be formed\n",
        "        for i in range(context_size, len(sentence) - context_size):\n",
        "            # Create the context by taking 'context_size' words before and after the target word\n",
        "            context = sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
        "            target = sentence[i]  # The target word is the current word in the sentence\n",
        "            pairs.append((context, target))  # Append the context-target pair to the list\n",
        "\n",
        "    return pairs  # Return the list of context-target pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_context_target_pairs_skipgram(text, context_size):\n",
        "    \"\"\"\n",
        "    Creates context-target pairs for training a Skip-gram model.\n",
        "\n",
        "    In a Skip-gram model, the target word is used to predict the surrounding context words.\n",
        "\n",
        "    Args:\n",
        "        text (list of list of str): A list of sentences, where each sentence is a list of words.\n",
        "        context_size (int): The number of words to consider on each side of the target word for the context.\n",
        "\n",
        "    Returns:\n",
        "        list of tuple: A list of tuples where each tuple contains a target word (str) and a context word (str).\n",
        "    \"\"\"\n",
        "    pairs = []  # Initialize an empty list to store context-target pairs\n",
        "\n",
        "    # Iterate over each sentence in the text\n",
        "    for sentence in text:\n",
        "        # Iterate over each word in the sentence\n",
        "        for i in range(len(sentence)):\n",
        "            target = sentence[i]  # The target word is the current word in the sentence\n",
        "\n",
        "            # Define the context window, making sure it doesn't go out of sentence bounds\n",
        "            context = sentence[max(0, i - context_size):i] + sentence[i + 1:i + context_size + 1]\n",
        "            \n",
        "            # Create a pair for each word in the context with the target word\n",
        "            for ctx in context:\n",
        "                pairs.append((target, ctx))  # Append the target-context pair to the list\n",
        "\n",
        "    return pairs  # Return the list of target-context pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Word2VecDataset_cbow(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset class for the CBOW (Continuous Bag of Words) model.\n",
        "\n",
        "    This dataset prepares context-target pairs for training the CBOW model, where\n",
        "    the context is a list of words and the target is a single word.\n",
        "\n",
        "    Args:\n",
        "        pairs (list of tuples): A list of context-target pairs where each context is a list of words and the target is a word.\n",
        "        word_to_index (dict): A dictionary mapping words to their corresponding indices.\n",
        "\n",
        "    Returns:\n",
        "        context_idxs (torch.Tensor): A tensor of indices representing the context words.\n",
        "        target_idx (torch.Tensor): A tensor representing the index of the target word.\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs, word_to_index):\n",
        "        self.pairs = pairs  # Store the context-target pairs\n",
        "        self.word_to_index = word_to_index  # Store the word-to-index mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)  # Return the number of pairs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the context and target from the pairs at the given index\n",
        "        context, target = self.pairs[idx]\n",
        "        \n",
        "        # Convert context words to indices using the word_to_index dictionary\n",
        "        context_idxs = torch.tensor([self.word_to_index[word] for word in context], dtype=torch.long)\n",
        "        \n",
        "        # Convert target word to its index\n",
        "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
        "        \n",
        "        return context_idxs, target_idx  # Return the context and target indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Word2VecDataset_skipgram(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset class for the Skip-gram model.\n",
        "\n",
        "    This dataset prepares target-context pairs for training the Skip-gram model, where\n",
        "    the target is a single word and the context is a single word.\n",
        "\n",
        "    Args:\n",
        "        pairs (list of tuples): A list of target-context pairs where each target and context is a word.\n",
        "        word_to_index (dict): A dictionary mapping words to their corresponding indices.\n",
        "\n",
        "    Returns:\n",
        "        target_idx (torch.Tensor): A tensor representing the index of the target word.\n",
        "        context_idx (torch.Tensor): A tensor representing the index of the context word.\n",
        "    \"\"\"\n",
        "    def __init__(self, pairs, word_to_index):\n",
        "        self.pairs = pairs  # Store the target-context pairs\n",
        "        self.word_to_index = word_to_index  # Store the word-to-index mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)  # Return the number of pairs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the target and context from the pairs at the given index\n",
        "        target, context = self.pairs[idx]\n",
        "        \n",
        "        # Convert target word to its index\n",
        "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
        "        \n",
        "        # Convert context word to its index\n",
        "        context_idx = torch.tensor(self.word_to_index[context], dtype=torch.long)\n",
        "        \n",
        "        return target_idx, context_idx  # Return the target and context indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data_for_model_training(file_name=\"str\", min_count=int, data_sample_name=str):\n",
        "    \"\"\"\n",
        "    Prepares data for training a word embedding model by loading, processing, and filtering the dataset.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Loads the data from a CSV file.\n",
        "    2. Splits the data into training, validation, and test sets.\n",
        "    3. Processes the text data into a corpus of words.\n",
        "    4. Creates a vocabulary based on word frequency and filters out infrequent words.\n",
        "    5. Maps words to indices and vice versa.\n",
        "    6. Filters the corpus based on the created vocabulary.\n",
        "\n",
        "    Args:\n",
        "        file_name (str): The name of the CSV file containing the preprocessed comments (without extension).\n",
        "        min_count (int): The minimum number of occurrences a word must have to be included in the vocabulary.\n",
        "        data_sample_name (str): A label to identify the sample of data being processed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the filtered training, validation, and test corpora, \n",
        "               the word-to-index mapping, the index-to-word mapping, and the data sample name.\n",
        "    \"\"\"\n",
        "\n",
        "    #### Parameters to choose:\n",
        "    ## Load data from CSV file\n",
        "    comments = pd.read_csv(local_path + f\"/data/preprocessed/{file_name}.csv\")\n",
        "\n",
        "    # Splitting the data into train, validation, and test sets\n",
        "    train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 50% val and 50% test\n",
        "\n",
        "    # Combine all comments for generating the vocabulary to avoid missing tokens during training\n",
        "    total_comments_list = comments[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "\n",
        "    # Prepare lists for training, validation, and test sets\n",
        "    train_list = train_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "    val_list = val_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "    test_list = test_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
        "\n",
        "    # Convert sentences into lists of words (corpus)\n",
        "    total_corpus = [doc.split() for doc in total_comments_list]\n",
        "    corpus_train = [doc.split() for doc in train_list]\n",
        "    corpus_val = [doc.split() for doc in val_list]\n",
        "    corpus_test = [doc.split() for doc in test_list]\n",
        "\n",
        "    # Create a vocabulary by counting occurrences of each word in the total corpus\n",
        "    vocab = defaultdict(int)\n",
        "    for sentence in total_corpus:\n",
        "        for word in sentence:\n",
        "            vocab[word] += 1\n",
        "\n",
        "    # Remove infrequent words from the vocabulary\n",
        "    vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
        "\n",
        "    # Create word-to-index and index-to-word mappings\n",
        "    word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
        "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "    # Create DataFrame from vocabulary for potential analysis or inspection\n",
        "    vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
        "\n",
        "    # Create a set of vocabulary words for quick lookup\n",
        "    vocab_set = set(vocab.keys())\n",
        "\n",
        "    # Filter the corpus to include only words present in the vocabulary\n",
        "    filtered_total_corpus = filter_corpus(total_corpus, vocab_set)\n",
        "    filtered_corpus_train = filter_corpus(corpus_train, vocab_set)\n",
        "    filtered_corpus_val = filter_corpus(corpus_val, vocab_set)\n",
        "    filtered_corpus_test = filter_corpus(corpus_test, vocab_set)\n",
        "\n",
        "    # Return the filtered corpora and the word mappings\n",
        "    return filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Class that implements early stopping to halt training when the validation loss stops improving.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    patience : int\n",
        "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
        "    min_delta : float\n",
        "        Minimum change in the validation loss to qualify as an improvement.\n",
        "\n",
        "    Methods:\n",
        "    ----------\n",
        "    __call__(val_loss, model)\n",
        "        Checks if the validation loss has improved and updates the state of early stopping.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    patience : int\n",
        "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
        "    min_delta : float\n",
        "        Minimum change in the validation loss to qualify as an improvement.\n",
        "    counter : int\n",
        "        Counter for the number of epochs since the last improvement.\n",
        "    best_loss : float or None\n",
        "        Best recorded validation loss.\n",
        "    early_stop : bool\n",
        "        Indicating whether training should be stopped early.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience= int, min_delta= float):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        ## for the first training iteration\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            ## check if the loss decreased, if not:\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "            ## if loss decrease (more than the defined delta): save model parameters, reset counter and update best loss\n",
        "        else:\n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_cbow(config, data):\n",
        "    \"\"\"\n",
        "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    config : dict\n",
        "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
        "    data : tuple\n",
        "        Tuple containing training and validation datasets.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    dict\n",
        "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
        "        and lists of validation and training losses across epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
        "\n",
        "    train_pairs = create_context_target_pairs_cbow(filtered_corpus_train, config[\"context_size\"])\n",
        "    val_pairs = create_context_target_pairs_cbow(filtered_corpus_val, config[\"context_size\"])\n",
        "    train_dataset = Word2VecDataset_cbow(train_pairs, word_to_index)\n",
        "    val_dataset = Word2VecDataset_cbow(val_pairs, word_to_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "\n",
        "    ## set seed to replicate the model\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    ## empty lists to store loss\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    ## initialise model\n",
        "    model = CBOW(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
        "\n",
        "    ## loss criterion\n",
        "    loss_criterion = nn.NLLLoss()\n",
        "\n",
        "    ## choose optimiser\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    ## adapt learning rate with scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
        "\n",
        "    #### Early Stopper ####\n",
        "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
        "\n",
        "    #### Training ####\n",
        "    ## each epoch iterates through the whole dataset\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        ## train model on training set\n",
        "        model.train()\n",
        "        ## set loss and r2 to zero again so we start fresh\n",
        "        train_loss = 0\n",
        "        ## iterate through batches of the training data (data is the features and target the target)\n",
        "        for context_idxs, target_idx in train_loader:\n",
        "            ## send tensors to gpu\n",
        "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "            ## reset gradient to 0,start fresh again\n",
        "            optimizer.zero_grad()\n",
        "            ## predict target\n",
        "            log_probs = model(context_idxs)\n",
        "            ## caculate loss\n",
        "            loss = loss_criterion(log_probs, target_idx)\n",
        "            ## caculate gradients\n",
        "            loss.backward()\n",
        "            ## update weights\n",
        "            optimizer.step()\n",
        "            ## sum loss for all batches together\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "        #### Validation ####\n",
        "        ## check performance on validation set\n",
        "        model.eval()\n",
        "        ## set loss to zero again so we start fresh\n",
        "        val_loss_sum = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        ## as we test on the validation set, we do not want to update our weights now\n",
        "        with torch.no_grad():\n",
        "            for context_idxs, target_idx in val_loader:\n",
        "                ## send tensors to gpu\n",
        "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "                log_probs = model(context_idxs)\n",
        "                ## caculate loss\n",
        "                loss = loss_criterion(log_probs, target_idx)\n",
        "                ## sum loss for whole epoch\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                correct += (predicted_idx == target_idx).sum().item()\n",
        "                total += context_idxs.size(0)\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        ## adapt learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
        "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
        "          ##save checkpoint\n",
        "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "\n",
        "          ## report mertrics and save checkpoint\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      },\n",
        "                  checkpoint=Checkpoint.from_directory(\".\")\n",
        "                  )\n",
        "        else:\n",
        "          ##report only metrics\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      }\n",
        "                  )\n",
        "\n",
        "        #### Early stopping ####\n",
        "        # check if loss decreases more than defined threshold\n",
        "        early_stopper(val_loss, model)\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "    ## last checkpoint\n",
        "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "    ray.train.report(\n",
        "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
        "        checkpoint=Checkpoint.from_directory(\".\")\n",
        "        )\n",
        "\n",
        "    #return train_losses, val_losses, val_r2s\n",
        "    return {\n",
        "        \"loss\": round(early_stopper.best_loss, 2),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"val_loss_list\": val_losses,\n",
        "        \"train_loss_list\": train_losses\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_skipgram(config, data):\n",
        "    \"\"\"\n",
        "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    config : dict\n",
        "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
        "    data : tuple\n",
        "        Tuple containing training and validation datasets.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    dict\n",
        "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
        "        and lists of validation and training losses across epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
        "\n",
        "    train_pairs = create_context_target_pairs_skipgram(filtered_corpus_train, config[\"context_size\"])\n",
        "    val_pairs = create_context_target_pairs_skipgram(filtered_corpus_val, config[\"context_size\"])\n",
        "    train_dataset = Word2VecDataset_skipgram(train_pairs, word_to_index)\n",
        "    val_dataset = Word2VecDataset_skipgram(val_pairs, word_to_index)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "\n",
        "    ## set seed to replicate the model\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    ## empty lists to store loss\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    ## initialise model\n",
        "    model = SkipGram(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
        "\n",
        "    ## loss criterion\n",
        "    loss_criterion = nn.NLLLoss()\n",
        "\n",
        "    ## choose optimiser\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "    ## adapt learning rate with scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
        "\n",
        "    #### Early Stopper ####\n",
        "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
        "\n",
        "    #### Training ####\n",
        "    ## each epoch iterates through the whole dataset\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        ## train model on training set\n",
        "        model.train()\n",
        "        ## set loss and r2 to zero again so we start fresh\n",
        "        train_loss = 0\n",
        "        ## iterate through batches of the training data (data is the features and target the target)\n",
        "        for context_idxs, target_idx in train_loader:\n",
        "            ## send tensors to gpu\n",
        "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "            ## reset gradient to 0,start fresh again\n",
        "            optimizer.zero_grad()\n",
        "            ## predict target\n",
        "            log_probs = model(target_idx)\n",
        "            ## caculate loss\n",
        "            loss = loss_criterion(log_probs, context_idxs)\n",
        "            ## caculate gradients\n",
        "            loss.backward()\n",
        "            ## update weights\n",
        "            optimizer.step()\n",
        "            ## sum loss for all batches together\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "        #### Validation ####\n",
        "        ## check performance on validation set\n",
        "        model.eval()\n",
        "        ## set loss to zero again so we start fresh\n",
        "        val_loss_sum = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "\n",
        "        ## as we test on the validation set, we do not want to update our weights now\n",
        "        with torch.no_grad():\n",
        "            for context_idxs, target_idx in val_loader:\n",
        "                ## send tensors to gpu\n",
        "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
        "                log_probs = model(target_idx)\n",
        "                ## caculate loss\n",
        "                loss = loss_criterion(log_probs, context_idxs)\n",
        "                ## sum loss for whole epoch\n",
        "                val_loss_sum += loss.item()\n",
        "\n",
        "                # Get the index of the max log-probability\n",
        "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "                correct += (predicted_idx == context_idxs).sum().item()\n",
        "                total += context_idxs.size(0)\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_loader)\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        ## adapt learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
        "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
        "          ##save checkpoint\n",
        "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "\n",
        "          ## report mertrics and save checkpoint\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      },\n",
        "                  checkpoint=Checkpoint.from_directory(\".\")\n",
        "                  )\n",
        "        else:\n",
        "          ##report only metrics\n",
        "          ray.train.report(\n",
        "                  {\n",
        "                      \"loss\": round(early_stopper.best_loss, 2),\n",
        "                      \"val_loss_list\": val_losses,\n",
        "                      \"train_loss_list\": train_losses,\n",
        "                      \"accuracy\": accuracy\n",
        "                      }\n",
        "                  )\n",
        "\n",
        "        #### Early stopping ####\n",
        "        # check if loss decreases more than defined threshold\n",
        "        early_stopper(val_loss, model)\n",
        "\n",
        "        if early_stopper.early_stop:\n",
        "            break\n",
        "\n",
        "    ## last checkpoint\n",
        "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
        "    ray.train.report(\n",
        "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
        "        checkpoint=Checkpoint.from_directory(\".\")\n",
        "        )\n",
        "\n",
        "    #return train_losses, val_losses, val_r2s\n",
        "    return {\n",
        "        \"loss\": round(early_stopper.best_loss, 2),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"val_loss_list\": val_losses,\n",
        "        \"train_loss_list\": train_losses\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Tuning ####\n",
        "## Custom function to shorten ray file path names\n",
        "def short_dirname(trial) -> str:\n",
        "    \"\"\"\n",
        "    Function that shortens path names created by Ray.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    trial : ray.tune.Trial\n",
        "        The Ray trial object for which the directory name is being created.\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    str\n",
        "        A shortened file path in the format 'trial_<trial_id>'.\n",
        "    \"\"\"\n",
        "    return \"trial_\" + str(trial.trial_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_parameters(training_function, num_samples, train_corpus, val_corpus, word_to_index, max_num_epochs, parameter_space, resources, local_path):\n",
        "    \"\"\"\n",
        "    Function that tunes the hyperparameters for a DL model using ASHA scheduling and saves the best model and tuning results locally.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    training_function : function\n",
        "        The function used for training the model during hyperparameter tuning.\n",
        "    num_samples : int\n",
        "        The number of hyperparameter samples to try.\n",
        "    train_dataset : object\n",
        "        Training dataset object.\n",
        "    val_dataset : object\n",
        "        Validation dataset object.\n",
        "    max_num_epochs : int\n",
        "        The maximum number of epochs for training each model.\n",
        "    parameter_space : dict\n",
        "        Dictionary defining the hyperparameter search space.\n",
        "    resources : dict\n",
        "        Resources configuration for training.\n",
        "    local_path : str\n",
        "        Local path to save tuning results and best model.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    pandas.DataFrame\n",
        "        DataFrame containing the tuning results, sorted by loss.\n",
        "        \"\"\"\n",
        "\n",
        "    ## because min number of epochs in sampling range is 50\n",
        "    #assert max_num_epochs > 50\n",
        "\n",
        "    ## Hyperparameters to sample from\n",
        "    ## ASHA scheduler to increase efficiency and stop inefficient training configs\n",
        "    scheduler = ASHAScheduler(\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=3,\n",
        "        reduction_factor=2\n",
        "    )\n",
        "\n",
        "    ## tuning function, choose resources\n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(\n",
        "            tune.with_parameters(\n",
        "                training_function,\n",
        "                data = (train_corpus, val_corpus, word_to_index)),\n",
        "                resources= resources\n",
        "        ),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "            trial_dirname_creator=short_dirname\n",
        "        ),\n",
        "        param_space= parameter_space,\n",
        "        run_config = ray.train.RunConfig(storage_path = local_path, name=\"run_\" + datetime.now().strftime(\"%m-%d_%H_%M\"))\n",
        "    )\n",
        "\n",
        "    results = tuner.fit()\n",
        "\n",
        "\n",
        "    #### Best Model ####\n",
        "\n",
        "    ## create folder\n",
        "    os.makedirs(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}', exist_ok=True)\n",
        "\n",
        "    ## get best model\n",
        "    best_result = results.get_best_result(\"loss\", \"min\")\n",
        "\n",
        "    ## save info about best model\n",
        "    with open(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_result_info_' + parameter_space[\"model\"] + '.pkl', 'wb') as file:\n",
        "        pickle.dump(best_result, file)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Best trial config: {}\".format(best_result.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
        "\n",
        "    ## get path to that best model\n",
        "    best_checkpoint_path = best_result.get_best_checkpoint(metric = \"loss\", mode = \"min\").path + \"/checkpoint_\"+ parameter_space[\"model\"] + \".pt\"\n",
        "    ## save path to model as txt\n",
        "    #with open(local_path + f\"/best_models_{parameter_space[\"folder_ending\"]}//path_best_model_\" + parameter_space[\"model\"] + \".txt\", \"w\") as file:\n",
        "    #    file.write(best_checkpoint_path)\n",
        "\n",
        "    ##save model parameters\n",
        "    best_checkpoint = torch.load(best_checkpoint_path)\n",
        "\n",
        "    ## create new model\n",
        "    if parameter_space[\"model\"] == \"CBOW\":\n",
        "        model_final = CBOW(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "    else:\n",
        "        model_final = SkipGram(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    ## load parameteres of best checkpoint\n",
        "    model_final.load_state_dict(best_checkpoint)\n",
        "\n",
        "    torch.save(model_final.state_dict(), local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_model_parameters_' + parameter_space[\"model\"] + '.pt')\n",
        "\n",
        "    #### Tuning Overview ####\n",
        "    ## Get results as df\n",
        "    df_tuning_results = results.get_dataframe()\n",
        "    ## Rename cols\n",
        "    df_tuning_results.columns = [col.replace('config/', '') for col in df_tuning_results.columns]\n",
        "    ## sort by loss\n",
        "    df_tuning_results.sort_values(\"loss\", inplace = True)\n",
        "    ## Save only relevant cols\n",
        "    df_tuning_results = df_tuning_results[['loss', \"accuracy\", \"context_size\", 'lr', 'batch_size', 'epochs',\n",
        "                                           'patience', 'min_delta', \"gamma\", \"step_size\",\n",
        "                                           \"dropout\", 'time_total_s', \"val_loss_list\", \"train_loss_list\"]]\n",
        "    ## Save as csv\n",
        "    df_tuning_results.to_csv(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/df_tuning_results' + parameter_space[\"model\"] + '.csv')\n",
        "\n",
        "    return df_tuning_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " #### Replication ####\n",
        "def load_best_model(model_type = str, local_path = str, data_sample_name = str):\n",
        "    \"\"\"\n",
        "    Function that loads the best model based on the specified model type.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model to load (\"CNN\" or other).\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    torch.nn.Module\n",
        "        The best pre-trained model loaded on the device and ready for evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    ## get best config\n",
        "    with open(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl', 'rb') as file:\n",
        "    # Use pickle.dump() to write the data object to file\n",
        "        best_result = pickle.load(file)\n",
        "\n",
        "    #with open(f\"{local_path}/tuning_results/best_models/path_best_model_{model_type}.txt\") as file:\n",
        "    #    path_best_file = file.read()\n",
        "\n",
        "    ## load parameters of best model\n",
        "    best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n",
        "\n",
        "    ## create new model\n",
        "    if model_type == \"CBOW\":\n",
        "        model_final = CBOW(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "    else:\n",
        "        model_final = SkipGram(\n",
        "            vocab_size = len(word_to_index),\n",
        "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    ## load parameteres of best checkpoint\n",
        "    model_final.load_state_dict(best_checkpoint)\n",
        "    ## model into evaluation mode\n",
        "    model_final.eval()\n",
        "    model_final.to(device)\n",
        "\n",
        "    return model_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_curve(model_type = str, local_path = str, data_sample_name = str):\n",
        "\n",
        "   \"\"\"\n",
        "    Function that plots the training and validation loss curves of the best model.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model whose loss curve to plot (\"CNN\" or other).\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "   ## get file with loss data\n",
        "   with open(local_path + f\"/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl\", 'rb') as file:\n",
        "       best_result = pickle.load(file)\n",
        "\n",
        "   ## get respective tuning data\n",
        "   val_losses = best_result.metrics[\"val_loss_list\"]\n",
        "   ## i forgot to divide the train loss by n in the training function\n",
        "   ## and repeating that takes 8 hours, so I have to do it like this now\n",
        "   train_losses = best_result.metrics[\"train_loss_list\"]\n",
        "\n",
        "   ## create plot\n",
        "   plt.plot(train_losses, label='Training Loss')\n",
        "   plt.plot(val_losses, label='Validation Loss')\n",
        "   plt.title(f\"{model_type} loss curves\")\n",
        "   plt.xlabel('Epochs')\n",
        "   plt.ylabel('Loss')\n",
        "   plt.legend()\n",
        "   ## save\n",
        "   plt.savefig(local_path + f\"plots/loss_curve_{model_type}_{data_sample_name}.png\")\n",
        "   plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify(model_type, dataloader, index_to_word, data_sample_name=str, include_true_vals=True):\n",
        "    \"\"\"\n",
        "    Classifies words using a trained CBOW or Skip-gram model and returns the predicted words.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Loads the best trained model based on the provided model type and data sample name.\n",
        "    2. Iterates through the provided data using a DataLoader.\n",
        "    3. For each batch, predicts the target word(s) based on the context (for CBOW) or predicts the context word(s) based on the target (for Skip-gram).\n",
        "    4. Optionally, returns the true values along with the predictions.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): The type of the model used ('CBOW' or 'Skip-gram').\n",
        "        dataloader (torch.utils.data.DataLoader): DataLoader object that provides batches of data for classification.\n",
        "        index_to_word (dict): A dictionary mapping indices to words.\n",
        "        data_sample_name (str): The name of the data sample used for loading the model.\n",
        "        include_true_vals (bool): Whether to return the true target/context values along with predictions.\n",
        "\n",
        "    Returns:\n",
        "        If include_true_vals is True:\n",
        "            tuple: Two lists containing predicted words and true words.\n",
        "        If include_true_vals is False:\n",
        "            list: A list of predicted words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the best trained model based on the model type and data sample name\n",
        "    model = load_best_model(model_type, local_path=local_path, data_sample_name=data_sample_name)\n",
        "\n",
        "    predictions, true_vals = [], []  # Initialize lists to store predictions and true values\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate through the dataloader to get batches of data\n",
        "        for context_idx, target_idx in dataloader:\n",
        "            # Move data to the appropriate device (CPU or GPU)\n",
        "            context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
        "            \n",
        "            # For CBOW, predict the target word based on the context\n",
        "            if model_type == \"CBOW\":\n",
        "                log_probs = model(context_idx)\n",
        "            else:  # For Skip-gram, predict the context word based on the target\n",
        "                log_probs = model(target_idx)\n",
        "\n",
        "            # Get the index of the word with the highest log-probability (the predicted word)\n",
        "            _, predicted_idx = torch.max(log_probs, dim=1)\n",
        "            \n",
        "            # Convert the predicted indices to words and add them to the predictions list\n",
        "            predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
        "\n",
        "            if include_true_vals:\n",
        "                # Convert the true indices to words and add them to the true_vals list\n",
        "                if model_type == \"CBOW\":\n",
        "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in target_idx.cpu().numpy()])\n",
        "                else:\n",
        "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in context_idx.cpu().numpy()])\n",
        "\n",
        "    # Return predictions and true values if requested\n",
        "    if include_true_vals:\n",
        "        return predictions, true_vals\n",
        "    else:\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classification(model_type, prediction_val, prediction_test, y_val, y_test):\n",
        "    \"\"\"\n",
        "    Function that evaluates a classification model by computing accuracy, recall, precision, and F-score for the training and validation data, and optionally for the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model_type : str\n",
        "        Type of the model being evaluated.\n",
        "    prediction_val : array\n",
        "        Predictions made by the model on the validation dataset.\n",
        "    prediction_test : array\n",
        "        Predictions made by the model on the test dataset.\n",
        "    y_val : array\n",
        "        Actual target values in the validation dataset.\n",
        "    y_test : array\n",
        "        Actual target values in the test dataset.\n",
        "    final_testing : bool, default=False\n",
        "        Flag indicating whether to evaluate the model on the test dataset.\n",
        "\n",
        "    Return:\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(f\"\\n--------------------------\\n{model_type} Classification Evaluation \\n--------------------------\")\n",
        "\n",
        "\n",
        "    print(\"\\nValidation set\")\n",
        "    print(\"--------------\")\n",
        "\n",
        "    print(f\"F1 Score: {f1_score(y_true = y_val, y_pred = prediction_val, average = 'micro'):.2f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true = y_val, y_pred = prediction_val):.2f}\")\n",
        "\n",
        "    #print(classification_report(y_true = y_val, y_pred = prediction_val, digits=2, zero_division=0))\n",
        "    print(\"\\nTest set\")\n",
        "    print(\"--------\\n\")\n",
        "    print(f\"F1 Score: {f1_score(y_true = y_test, y_pred = prediction_test, average = 'micro'):.2f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true = y_test, y_pred = prediction_test):.2f}\")\n",
        "\n",
        "    #print(classification_report(y_true = y_test, y_pred = prediction_test, digits=2, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUYq3QW4r_7G",
        "outputId": "4496b166-0127-454e-d127-ca0393ef8ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CWD:  /content/drive/MyDrive/DLSS/\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "def extract_embeddings(model_type=str, local_path=str, data_sample_name=str, index_to_word=dict):\n",
        "    \"\"\"\n",
        "    Extracts word embeddings from a trained model and saves them to a CSV file.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Loads the best trained model based on the provided model type and data sample name.\n",
        "    2. Extracts the word embeddings from the model.\n",
        "    3. Creates a DataFrame containing the embeddings with their corresponding words.\n",
        "    4. Saves the embeddings to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): The type of the model used (e.g., 'cbow', 'skipgram').\n",
        "        local_path (str): The local directory path where the model and embeddings are stored.\n",
        "        data_sample_name (str): The name of the data sample used for training the model.\n",
        "        index_to_word (dict): A dictionary mapping indices to words.\n",
        "\n",
        "    Returns:\n",
        "        embeddings_df (pd.DataFrame): A DataFrame containing the word embeddings with words as the first column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the best trained model based on model type and data sample name\n",
        "    model = load_best_model(model_type=model_type, local_path=local_path, data_sample_name=data_sample_name)\n",
        "    \n",
        "    # Extract the embeddings from the model as a NumPy array\n",
        "    embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "    # Create a DataFrame from the embeddings\n",
        "    embeddings_df = pd.DataFrame(embeddings)\n",
        "    \n",
        "    # Insert a column with the corresponding words\n",
        "    embeddings_df.insert(0, 'word', [index_to_word[i] for i in range(len(embeddings_df))])\n",
        "    \n",
        "    # Save the embeddings DataFrame to a CSV file\n",
        "    embeddings_df.to_csv(local_path + f\"data/embeddings/embeddings_{model_type}_{data_sample_name}.csv\")\n",
        "    \n",
        "    return embeddings_df  # Return the embeddings DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGdPMJP5xEvh"
      },
      "source": [
        "## CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "timh-68DoySp",
        "outputId": "b64f7d42-0f96-4d2e-9b9c-f03a688849fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------\n",
            "CBOW Classification Evaluation \n",
            "--------------------------\n",
            "\n",
            "Validation set\n",
            "--------------\n",
            "F1 Score: 0.18\n",
            "Accuracy: 0.18\n",
            "\n",
            "Test set\n",
            "--------\n",
            "\n",
            "F1 Score: 0.18\n",
            "Accuracy: 0.18\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for model training by filtering the corpus and creating word-index mappings\n",
        "filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name = prepare_data_for_model_training(\n",
        "    file_name=\"total_posts\",\n",
        "    min_count=40,\n",
        "    data_sample_name=\"total_posts\"\n",
        ")\n",
        "\n",
        "# Define context size for CBOW model\n",
        "context_size = 2\n",
        "\n",
        "# Create context-target pairs for validation and test sets\n",
        "val_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_val, context_size)\n",
        "test_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_test, context_size)\n",
        "\n",
        "# Convert the context-target pairs into datasets for validation and test\n",
        "val_dataset_cbow = Word2VecDataset_cbow(val_pairs_cbow, word_to_index)\n",
        "test_dataset_cbow = Word2VecDataset_cbow(test_pairs_cbow, word_to_index)\n",
        "\n",
        "# Create DataLoader objects for batch processing during validation and testing\n",
        "val_loader_cbow = DataLoader(val_dataset_cbow, shuffle=False, batch_size=50000)\n",
        "test_loader_cbow = DataLoader(test_dataset_cbow, shuffle=False, batch_size=50000)\n",
        "\n",
        "# Classify and get predictions for the validation set using the CBOW model\n",
        "prediction_val_cbow, true_val_cbow = classify(\n",
        "    model_type=\"CBOW\",\n",
        "    dataloader=val_loader_cbow,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name=data_sample_name\n",
        ")\n",
        "\n",
        "# Classify and get predictions for the test set using the CBOW model\n",
        "prediction_test_cbow, true_test_cbow = classify(\n",
        "    model_type=\"CBOW\",\n",
        "    dataloader=test_loader_cbow,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name=data_sample_name\n",
        ")\n",
        "\n",
        "# Evaluate the classification performance for both validation and test sets\n",
        "evaluate_classification(\n",
        "    model_type=\"CBOW\",\n",
        "    prediction_val=prediction_val_cbow,\n",
        "    prediction_test=prediction_test_cbow,\n",
        "    y_val=true_val_cbow,\n",
        "    y_test=true_test_cbow\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDuNYCrIxBgI"
      },
      "source": [
        "## Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrVjgPS6xJvB",
        "outputId": "139a7ff7-d021-4531-9eac-f618fa3601a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------\n",
            "skipgram Classification Evaluation \n",
            "--------------------------\n",
            "\n",
            "Validation set\n",
            "--------------\n",
            "F1 Score: 0.06\n",
            "Accuracy: 0.06\n",
            "\n",
            "Test set\n",
            "--------\n",
            "\n",
            "F1 Score: 0.06\n",
            "Accuracy: 0.06\n"
          ]
        }
      ],
      "source": [
        "# Create context-target pairs for validation and test sets using the Skip-gram model\n",
        "val_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_val, context_size)\n",
        "test_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_test, context_size)\n",
        "\n",
        "# Convert the context-target pairs into datasets for validation and test\n",
        "val_dataset_skipgram = Word2VecDataset_skipgram(val_pairs_skipgram, word_to_index)\n",
        "test_dataset_skipgram = Word2VecDataset_skipgram(test_pairs_skipgram, word_to_index)\n",
        "\n",
        "# Create DataLoader objects for batch processing during validation and testing\n",
        "val_loader_skipgram = DataLoader(val_dataset_skipgram, shuffle=False, batch_size=50000)\n",
        "test_loader_skipgram = DataLoader(test_dataset_skipgram, shuffle=False, batch_size=50000)\n",
        "\n",
        "# Classify and get predictions for the validation set using the Skip-gram model\n",
        "prediction_val_skipgram, true_val_skipgram = classify(\n",
        "    model_type=\"skipgram\",\n",
        "    dataloader=val_loader_skipgram,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name=data_sample_name\n",
        ")\n",
        "\n",
        "# Classify and get predictions for the test set using the Skip-gram model\n",
        "prediction_test_skipgram, true_test_skipgram = classify(\n",
        "    model_type=\"skipgram\",\n",
        "    dataloader=test_loader_skipgram,\n",
        "    index_to_word=index_to_word,\n",
        "    data_sample_name=data_sample_name\n",
        ")\n",
        "\n",
        "# Evaluate the classification performance for both validation and test sets\n",
        "evaluate_classification(\n",
        "    model_type=\"skipgram\",\n",
        "    prediction_val=prediction_val_skipgram,\n",
        "    prediction_test=prediction_test_skipgram,\n",
        "    y_val=true_val_skipgram,\n",
        "    y_test=true_test_skipgram\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOxw6kzt4p5FKYin1odCYK2",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
