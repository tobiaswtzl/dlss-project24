{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ## this is necessary to get ray running on colab\n",
    "    !pip uninstall -y -q pyarrow\n",
    "    !pip install pyarrow==14.0.1\n",
    "    !pip install -q -U ray[tune]\n",
    "    !pip install -q ray[debug]\n",
    "    ## force crash as restart is necessary due to the installed packages\n",
    "    import os\n",
    "    os._exit(0)\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  d:\\dlss-project24\\code/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dlss-project24\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-22 16:46:22,002\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-22 16:46:23,441\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "    ## get current directory\n",
    "    current_wd = os.getcwd()\n",
    "    ## move one up to go to main directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"\n",
    "\n",
    "print(\"CWD: \", local_path)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import Checkpoint\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(corpus, vocab_set):\n",
    "    return [[word for word in doc if word in vocab_set] for doc in corpus]\n",
    "\n",
    "# Function to create context-target pairs\n",
    "def create_context_target_pairs_cbow(text, context_size):\n",
    "    pairs = []\n",
    "    for sentence in text:\n",
    "        for i in range(context_size, len(sentence) - context_size):\n",
    "            context = sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "            target = sentence[i]\n",
    "            pairs.append((context, target))\n",
    "    return pairs\n",
    "\n",
    "# Function to create context-target pairs for Skip-gram\n",
    "def create_context_target_pairs_skipgram(text, context_size):\n",
    "    pairs = []\n",
    "    for sentence in text:\n",
    "        for i in range(len(sentence)):\n",
    "            target = sentence[i]\n",
    "            context = sentence[max(0, i - context_size):i] + sentence[i + 1:i + context_size + 1]\n",
    "            for ctx in context:\n",
    "                pairs.append((target, ctx))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Dataset and DataLoader definition\n",
    "class Word2VecDataset_cbow(Dataset):\n",
    "    def __init__(self, pairs, word_to_index):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.pairs[idx]\n",
    "        context_idxs = torch.tensor([self.word_to_index[word] for word in context], dtype=torch.long)\n",
    "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
    "        return context_idxs, target_idx\n",
    "\n",
    "class Word2VecDataset_skipgram(Dataset):\n",
    "    def __init__(self, pairs, word_to_index):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_index = word_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
    "        context_idx = torch.tensor(self.word_to_index[context], dtype=torch.long)\n",
    "        return target_idx, context_idx\n",
    "\n",
    "def prepare_data_for_model_training(file_name = \"str\", min_count = int, data_sample_name = str):\n",
    "    #### Parameters to choose:\n",
    "    ## get data\n",
    "    comments = pd.read_csv(local_path +f\"/data/preprocessed/{file_name}.csv\")\n",
    "\n",
    "    # Splitting the data into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    #Adding all comments for generating the vocabulary. If not an error occurs when tokens missing\n",
    "    total_comments_list = comments[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "\n",
    "    train_list = train_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "    val_list = val_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "    test_list = test_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "\n",
    "    # Ensure each entry is a string and split each sentence into words\n",
    "    total_corpus = [doc.split() for doc in total_comments_list]\n",
    "    corpus_train = [doc.split() for doc in train_list]\n",
    "    corpus_val = [doc.split() for doc in val_list]\n",
    "    corpus_test = [doc.split() for doc in test_list]\n",
    "\n",
    "    # Create a vocabulary: count occurrences of each word\n",
    "    vocab = defaultdict(int)\n",
    "    for sentence in total_corpus:\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # Remove infrequent words from the vocabulary\n",
    "    vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "\n",
    "    # Create word to index and index to word mappings\n",
    "    word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "    # Create DataFrame from vocabulary\n",
    "    vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
    "\n",
    "    vocab_set = set(vocab.keys())\n",
    "\n",
    "    filtered_total_corpus = filter_corpus(total_corpus, vocab_set)\n",
    "    filtered_corpus_train = filter_corpus(corpus_train, vocab_set)\n",
    "    filtered_corpus_val = filter_corpus(corpus_val, vocab_set)\n",
    "    filtered_corpus_test = filter_corpus(corpus_test, vocab_set)\n",
    "    return filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Class that implements early stopping to halt training when the validation loss stops improving.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
    "    min_delta : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "\n",
    "    Methods:\n",
    "    ----------\n",
    "    __call__(val_loss, model)\n",
    "        Checks if the validation loss has improved and updates the state of early stopping.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
    "    min_delta : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    counter : int\n",
    "        Counter for the number of epochs since the last improvement.\n",
    "    best_loss : float or None\n",
    "        Best recorded validation loss.\n",
    "    early_stop : bool\n",
    "        Indicating whether training should be stopped early.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience= int, min_delta= float):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        ## for the first training iteration\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            ## check if the loss decreased, if not:\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            ## if loss decrease (more than the defined delta): save model parameters, reset counter and update best loss\n",
    "        else:\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "\n",
    "\n",
    "def train_model_cbow(config, data):\n",
    "    \"\"\"\n",
    "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
    "    data : tuple\n",
    "        Tuple containing training and validation datasets.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict\n",
    "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
    "        and lists of validation and training losses across epochs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
    "\n",
    "    train_pairs = create_context_target_pairs_cbow(filtered_corpus_train, config[\"context_size\"])\n",
    "    val_pairs = create_context_target_pairs_cbow(filtered_corpus_val, config[\"context_size\"])\n",
    "    train_dataset = Word2VecDataset_cbow(train_pairs, word_to_index)\n",
    "    val_dataset = Word2VecDataset_cbow(val_pairs, word_to_index)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "    ## set seed to replicate the model\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    ## empty lists to store loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    ## initialise model\n",
    "    model = CBOW(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
    "\n",
    "    ## loss criterion\n",
    "    loss_criterion = nn.NLLLoss()\n",
    "\n",
    "    ## choose optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    ## adapt learning rate with scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
    "\n",
    "    #### Early Stopper ####\n",
    "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
    "\n",
    "    #### Training ####\n",
    "    ## each epoch iterates through the whole dataset\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        ## train model on training set\n",
    "        model.train()\n",
    "        ## set loss and r2 to zero again so we start fresh\n",
    "        train_loss = 0\n",
    "        ## iterate through batches of the training data (data is the features and target the target)\n",
    "        for context_idxs, target_idx in train_loader:\n",
    "            ## send tensors to gpu\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            ## reset gradient to 0,start fresh again\n",
    "            optimizer.zero_grad()\n",
    "            ## predict target\n",
    "            log_probs = model(context_idxs)\n",
    "            ## caculate loss\n",
    "            loss = loss_criterion(log_probs, target_idx)\n",
    "            ## caculate gradients\n",
    "            loss.backward()\n",
    "            ## update weights\n",
    "            optimizer.step()\n",
    "            ## sum loss for all batches together\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "        #### Validation ####\n",
    "        ## check performance on validation set\n",
    "        model.eval()\n",
    "        ## set loss to zero again so we start fresh\n",
    "        val_loss_sum = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        ## as we test on the validation set, we do not want to update our weights now\n",
    "        with torch.no_grad():\n",
    "            for context_idxs, target_idx in val_loader:\n",
    "                ## send tensors to gpu\n",
    "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "                log_probs = model(context_idxs)\n",
    "                ## caculate loss\n",
    "                loss = loss_criterion(log_probs, target_idx)\n",
    "                ## sum loss for whole epoch\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                correct += (predicted_idx == target_idx).sum().item()\n",
    "                total += context_idxs.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        ## adapt learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
    "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
    "          ##save checkpoint\n",
    "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "\n",
    "          ## report mertrics and save checkpoint\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      },\n",
    "                  checkpoint=Checkpoint.from_directory(\".\")\n",
    "                  )\n",
    "        else:\n",
    "          ##report only metrics\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      }\n",
    "                  )\n",
    "\n",
    "        #### Early stopping ####\n",
    "        # check if loss decreases more than defined threshold\n",
    "        early_stopper(val_loss, model)\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            break\n",
    "\n",
    "    ## last checkpoint\n",
    "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "    ray.train.report(\n",
    "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
    "        checkpoint=Checkpoint.from_directory(\".\")\n",
    "        )\n",
    "\n",
    "    #return train_losses, val_losses, val_r2s\n",
    "    return {\n",
    "        \"loss\": round(early_stopper.best_loss, 2),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"val_loss_list\": val_losses,\n",
    "        \"train_loss_list\": train_losses\n",
    "        }\n",
    "\n",
    "def train_model_skipgram(config, data):\n",
    "    \"\"\"\n",
    "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
    "    data : tuple\n",
    "        Tuple containing training and validation datasets.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict\n",
    "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
    "        and lists of validation and training losses across epochs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
    "\n",
    "    train_pairs = create_context_target_pairs_skipgram(filtered_corpus_train, config[\"context_size\"])\n",
    "    val_pairs = create_context_target_pairs_skipgram(filtered_corpus_val, config[\"context_size\"])\n",
    "    train_dataset = Word2VecDataset_skipgram(train_pairs, word_to_index)\n",
    "    val_dataset = Word2VecDataset_skipgram(val_pairs, word_to_index)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "    ## set seed to replicate the model\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    ## empty lists to store loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    ## initialise model\n",
    "    model = SkipGram(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
    "\n",
    "    ## loss criterion\n",
    "    loss_criterion = nn.NLLLoss()\n",
    "\n",
    "    ## choose optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    ## adapt learning rate with scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
    "\n",
    "    #### Early Stopper ####\n",
    "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
    "\n",
    "    #### Training ####\n",
    "    ## each epoch iterates through the whole dataset\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        ## train model on training set\n",
    "        model.train()\n",
    "        ## set loss and r2 to zero again so we start fresh\n",
    "        train_loss = 0\n",
    "        ## iterate through batches of the training data (data is the features and target the target)\n",
    "        for context_idxs, target_idx in train_loader:\n",
    "            ## send tensors to gpu\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            ## reset gradient to 0,start fresh again\n",
    "            optimizer.zero_grad()\n",
    "            ## predict target\n",
    "            log_probs = model(target_idx)\n",
    "            ## caculate loss\n",
    "            loss = loss_criterion(log_probs, context_idxs)\n",
    "            ## caculate gradients\n",
    "            loss.backward()\n",
    "            ## update weights\n",
    "            optimizer.step()\n",
    "            ## sum loss for all batches together\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "        #### Validation ####\n",
    "        ## check performance on validation set\n",
    "        model.eval()\n",
    "        ## set loss to zero again so we start fresh\n",
    "        val_loss_sum = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        ## as we test on the validation set, we do not want to update our weights now\n",
    "        with torch.no_grad():\n",
    "            for context_idxs, target_idx in val_loader:\n",
    "                ## send tensors to gpu\n",
    "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "                log_probs = model(target_idx)\n",
    "                ## caculate loss\n",
    "                loss = loss_criterion(log_probs, context_idxs)\n",
    "                ## sum loss for whole epoch\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                correct += (predicted_idx == context_idxs).sum().item()\n",
    "                total += context_idxs.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        ## adapt learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
    "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
    "          ##save checkpoint\n",
    "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "\n",
    "          ## report mertrics and save checkpoint\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      },\n",
    "                  checkpoint=Checkpoint.from_directory(\".\")\n",
    "                  )\n",
    "        else:\n",
    "          ##report only metrics\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      }\n",
    "                  )\n",
    "\n",
    "        #### Early stopping ####\n",
    "        # check if loss decreases more than defined threshold\n",
    "        early_stopper(val_loss, model)\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            break\n",
    "\n",
    "    ## last checkpoint\n",
    "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "    ray.train.report(\n",
    "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
    "        checkpoint=Checkpoint.from_directory(\".\")\n",
    "        )\n",
    "\n",
    "    #return train_losses, val_losses, val_r2s\n",
    "    return {\n",
    "        \"loss\": round(early_stopper.best_loss, 2),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"val_loss_list\": val_losses,\n",
    "        \"train_loss_list\": train_losses\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "#### Tuning ####\n",
    "## Custom function to shorten ray file path names\n",
    "def short_dirname(trial) -> str:\n",
    "    \"\"\"\n",
    "    Function that shortens path names created by Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    trial : ray.tune.Trial\n",
    "        The Ray trial object for which the directory name is being created.\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    str\n",
    "        A shortened file path in the format 'trial_<trial_id>'.\n",
    "    \"\"\"\n",
    "    return \"trial_\" + str(trial.trial_id)\n",
    "\n",
    "\n",
    "## actual tuning\n",
    "def tune_parameters(training_function, num_samples, train_corpus, val_corpus, word_to_index, max_num_epochs, parameter_space, resources, local_path):\n",
    "    \"\"\"\n",
    "    Function that tunes the hyperparameters for a DL model using ASHA scheduling and saves the best model and tuning results locally.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    training_function : function\n",
    "        The function used for training the model during hyperparameter tuning.\n",
    "    num_samples : int\n",
    "        The number of hyperparameter samples to try.\n",
    "    train_dataset : object\n",
    "        Training dataset object.\n",
    "    val_dataset : object\n",
    "        Validation dataset object.\n",
    "    max_num_epochs : int\n",
    "        The maximum number of epochs for training each model.\n",
    "    parameter_space : dict\n",
    "        Dictionary defining the hyperparameter search space.\n",
    "    resources : dict\n",
    "        Resources configuration for training.\n",
    "    local_path : str\n",
    "        Local path to save tuning results and best model.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the tuning results, sorted by loss.\n",
    "        \"\"\"\n",
    "\n",
    "    ## because min number of epochs in sampling range is 50\n",
    "    #assert max_num_epochs > 50\n",
    "\n",
    "    ## Hyperparameters to sample from\n",
    "    ## ASHA scheduler to increase efficiency and stop inefficient training configs\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=3,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "\n",
    "    ## tuning function, choose resources\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                training_function,\n",
    "                data = (train_corpus, val_corpus, word_to_index)),\n",
    "                resources= resources\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            trial_dirname_creator=short_dirname\n",
    "        ),\n",
    "        param_space= parameter_space,\n",
    "        run_config = ray.train.RunConfig(storage_path = local_path, name=\"run_\" + datetime.now().strftime(\"%m-%d_%H_%M\"))\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "\n",
    "    #### Best Model ####\n",
    "\n",
    "    ## create folder\n",
    "    os.makedirs(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}', exist_ok=True)\n",
    "\n",
    "    ## get best model\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    ## save info about best model\n",
    "    with open(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_result_info_' + parameter_space[\"model\"] + '.pkl', 'wb') as file:\n",
    "        pickle.dump(best_result, file)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
    "\n",
    "    ## get path to that best model\n",
    "    best_checkpoint_path = best_result.get_best_checkpoint(metric = \"loss\", mode = \"min\").path + \"/checkpoint_\"+ parameter_space[\"model\"] + \".pt\"\n",
    "    ## save path to model as txt\n",
    "    #with open(local_path + f\"/best_models_{parameter_space[\"folder_ending\"]}//path_best_model_\" + parameter_space[\"model\"] + \".txt\", \"w\") as file:\n",
    "    #    file.write(best_checkpoint_path)\n",
    "\n",
    "    ##save model parameters\n",
    "    best_checkpoint = torch.load(best_checkpoint_path)\n",
    "\n",
    "    ## create new model\n",
    "    if parameter_space[\"model\"] == \"CBOW\":\n",
    "        model_final = CBOW(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "    else:\n",
    "        model_final = SkipGram(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    ## load parameteres of best checkpoint\n",
    "    model_final.load_state_dict(best_checkpoint)\n",
    "\n",
    "    torch.save(model_final.state_dict(), local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_model_parameters_' + parameter_space[\"model\"] + '.pt')\n",
    "\n",
    "    #### Tuning Overview ####\n",
    "    ## Get results as df\n",
    "    df_tuning_results = results.get_dataframe()\n",
    "    ## Rename cols\n",
    "    df_tuning_results.columns = [col.replace('config/', '') for col in df_tuning_results.columns]\n",
    "    ## sort by loss\n",
    "    df_tuning_results.sort_values(\"loss\", inplace = True)\n",
    "    ## Save only relevant cols\n",
    "    df_tuning_results = df_tuning_results[['loss', \"accuracy\", \"context_size\", 'lr', 'batch_size', 'epochs',\n",
    "                                           'patience', 'min_delta', \"gamma\", \"step_size\",\n",
    "                                           \"dropout\", 'time_total_s', \"val_loss_list\", \"train_loss_list\"]]\n",
    "    ## Save as csv\n",
    "    df_tuning_results.to_csv(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/df_tuning_results' + parameter_space[\"model\"] + '.csv')\n",
    "\n",
    "    return df_tuning_results\n",
    "\n",
    "\n",
    " #### Replication ####\n",
    "def load_best_model(model_type = str, local_path = str, data_sample_name = str):\n",
    "    \"\"\"\n",
    "    Function that loads the best model based on the specified model type.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model to load (\"CNN\" or other).\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    torch.nn.Module\n",
    "        The best pre-trained model loaded on the device and ready for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get best config\n",
    "    with open(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl', 'rb') as file:\n",
    "    # Use pickle.dump() to write the data object to file\n",
    "        best_result = pickle.load(file)\n",
    "\n",
    "    #with open(f\"{local_path}/tuning_results/best_models/path_best_model_{model_type}.txt\") as file:\n",
    "    #    path_best_file = file.read()\n",
    "\n",
    "    ## load parameters of best model\n",
    "    best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n",
    "\n",
    "    ## create new model\n",
    "    if model_type == \"CBOW\":\n",
    "        model_final = CBOW(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "    else:\n",
    "        model_final = SkipGram(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    ## load parameteres of best checkpoint\n",
    "    model_final.load_state_dict(best_checkpoint)\n",
    "    ## model into evaluation mode\n",
    "    model_final.eval()\n",
    "    model_final.to(device)\n",
    "\n",
    "    return model_final\n",
    "\n",
    "\n",
    "def plot_loss_curve(model_type = str, local_path = str, data_sample_name = str):\n",
    "\n",
    "   \"\"\"\n",
    "    Function that plots the training and validation loss curves of the best model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model whose loss curve to plot (\"CNN\" or other).\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "   ## get file with loss data\n",
    "   with open(local_path + f\"/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl\", 'rb') as file:\n",
    "       best_result = pickle.load(file)\n",
    "\n",
    "   ## get respective tuning data\n",
    "   val_losses = best_result.metrics[\"val_loss_list\"]\n",
    "   ## i forgot to divide the train loss by n in the training function\n",
    "   ## and repeating that takes 8 hours, so I have to do it like this now\n",
    "   train_losses = best_result.metrics[\"train_loss_list\"]\n",
    "\n",
    "   ## create plot\n",
    "   plt.plot(train_losses, label='Training Loss')\n",
    "   plt.plot(val_losses, label='Validation Loss')\n",
    "   plt.title(f\"{model_type} loss curves\")\n",
    "   plt.xlabel('Epochs')\n",
    "   plt.ylabel('Loss')\n",
    "   plt.legend()\n",
    "   ## save\n",
    "   plt.savefig(local_path + f\"plots/loss_curve_{model_type}_{data_sample_name}.png\")\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "def classify(model_type, dataloader, index_to_word, data_sample_name = str, include_true_vals=True):\n",
    "    model = load_best_model(model_type, local_path=local_path, data_sample_name = data_sample_name)\n",
    "\n",
    "    if model_type == \"CBOW\":\n",
    "        with torch.no_grad():\n",
    "            predictions, true_vals = [], []\n",
    "            for context_idx, target_idx in dataloader:\n",
    "                context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
    "                log_probs = model(context_idx)\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
    "\n",
    "                if include_true_vals:\n",
    "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in target_idx.cpu().numpy()])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            predictions, true_vals = [], []\n",
    "            for context_idx, target_idx in dataloader:\n",
    "                context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
    "                log_probs = model(target_idx)\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
    "\n",
    "                if include_true_vals:\n",
    "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in context_idx.cpu().numpy()])\n",
    "\n",
    "\n",
    "\n",
    "    if include_true_vals:\n",
    "        return predictions, true_vals\n",
    "    else:\n",
    "        return predictions\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_classification(model_type, prediction_val, prediction_test, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Function that evaluates a classification model by computing accuracy, recall, precision, and F-score for the training and validation data, and optionally for the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model being evaluated.\n",
    "    prediction_val : array\n",
    "        Predictions made by the model on the validation dataset.\n",
    "    prediction_test : array\n",
    "        Predictions made by the model on the test dataset.\n",
    "    y_val : array\n",
    "        Actual target values in the validation dataset.\n",
    "    y_test : array\n",
    "        Actual target values in the test dataset.\n",
    "    final_testing : bool, default=False\n",
    "        Flag indicating whether to evaluate the model on the test dataset.\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\n--------------------------\\n{model_type} Classification Evaluation \\n--------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\nValidation set\")\n",
    "    print(\"--------------\")\n",
    "\n",
    "    print(f\"F1 Score: {f1_score(y_true = y_val, y_pred = prediction_val, average = 'micro'):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true = y_val, y_pred = prediction_val):.2f}\")\n",
    "\n",
    "    #print(classification_report(y_true = y_val, y_pred = prediction_val, digits=2, zero_division=0))\n",
    "    print(\"\\nTest set\")\n",
    "    print(\"--------\\n\")\n",
    "    print(f\"F1 Score: {f1_score(y_true = y_test, y_pred = prediction_test, average = 'micro'):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true = y_test, y_pred = prediction_test):.2f}\")\n",
    "\n",
    "    #print(classification_report(y_true = y_test, y_pred = prediction_test, digits=2, zero_division=0))\n",
    "\n",
    "\n",
    "def extract_embeddings(model_type = str, local_path = str, data_sample_name = str, index_to_word = dict):\n",
    "    model = load_best_model(model_type = model_type, local_path=local_path, data_sample_name = data_sample_name)\n",
    "    embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "    # Create a DataFrame\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "    embeddings_df.insert(0, 'word', [index_to_word[i] for i in range(len(embeddings_df))])\n",
    "    embeddings_df.to_csv(local_path + f\"data/embeddings/embeddings_{model_type}_{data_sample_name}.csv\")\n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Continuous bag of words model #####\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embedding layer for word indices\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer for mapping embeddings to vocab size\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Get embeddings for context words\n",
    "        embeds = self.embeddings(context)\n",
    "        # Average embeddings to get a single vector\n",
    "        combined = torch.mean(embeds, dim=1)\n",
    "        # Apply dropout and pass through linear layer\n",
    "        out = self.linear1(self.dropout(combined))\n",
    "        # Compute log probabilities\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, target):\n",
    "        embeds = self.embeddings(target)\n",
    "        out = self.linear1(self.dropout(embeds))\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd:\\\\dlss-project24\\\\code//data/preprocessed/total_posts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_for_model_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_posts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_sample_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_posts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mprepare_data_for_model_training\u001b[1;34m(file_name, min_count, data_sample_name)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data_for_model_training\u001b[39m(file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m, data_sample_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m#### Parameters to choose:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m## get data\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     comments \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/preprocessed/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Splitting the data into train, validation, and test sets\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     train_df, temp_df \u001b[38;5;241m=\u001b[39m train_test_split(comments, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32md:\\dlss-project24\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\dlss-project24\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\dlss-project24\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\dlss-project24\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\dlss-project24\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd:\\\\dlss-project24\\\\code//data/preprocessed/total_posts.csv'"
     ]
    }
   ],
   "source": [
    "filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name = prepare_data_for_model_training(\n",
    "    file_name = \"total_posts\",\n",
    "    min_count = 40,\n",
    "    data_sample_name = \"total_posts\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#### Tuning ####\n",
    "## choose sample size and max epoch\n",
    "n_samples = 1\n",
    "epochs = 120\n",
    "resources = {\"cpu\": 24, \"gpu\": 1}\n",
    "\n",
    "## use the gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-22 13:28:59</td></tr>\n",
       "<tr><td>Running for: </td><td>03:46:24.76        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.3/31.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 96.000: -5.73 | Iter 48.000: -5.74 | Iter 24.000: -5.76 | Iter 12.000: -5.78 | Iter 6.000: -5.78 | Iter 3.000: -5.79<br>Logical resource usage: 24.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_cbow_18858_00000</td><td>TERMINATED</td><td>127.0.0.1:50492</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         13580.1</td><td style=\"text-align: right;\">  5.72</td><td style=\"text-align: right;\">  0.182337</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000000)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000001)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000002)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000003)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000004)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000005)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000006)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000007)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000008)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000009)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000010)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000011)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000012)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000013)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000014)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000015)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000016)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000017)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000018)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000019)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000020)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000021)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000022)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000023)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000024)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000025)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000026)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000027)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000028)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000029)\n",
      "\u001b[36m(train_model_cbow pid=50492)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42/trial_18858_00000/checkpoint_000030)\n",
      "2024-08-22 13:28:59,415\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_09_42' in 0.0060s.\n",
      "2024-08-22 13:28:59,419\tINFO tune.py:1041 -- Total run time: 13584.78 seconds (13584.75 seconds for the tuning loop).\n",
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_42496\\3456389537.py:579: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(best_checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'model': 'CBOW', 'data_sample_name': 'total_posts', 'context_size': 2, 'dropout': 0.2, 'lr': 0.009159544393144516, 'batch_size': 512, 'epochs': 120, 'patience': 10, 'min_delta': 0.0006816366988585599, 'gamma': 0.9, 'step_size': 5, 'weight_decay': 0.00011013699407057048, 'embedding_dim': 100}\n",
      "Best trial final validation loss: 5.72\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to cbow_model.pth\n",
      "Model saved as a pickle file to cbow_model.pkl\n",
      "Model loaded from .pth file and ready for inference\n",
      "Model loaded from pickle file and ready for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_42496\\844724310.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cbow_model.pth\"))\n",
      "c:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_42496\\3456389537.py:642: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n"
     ]
    }
   ],
   "source": [
    "## clear gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "## tune\n",
    "tune_parameters(\n",
    "    train_model_cbow,\n",
    "    num_samples=n_samples,\n",
    "    train_corpus = filtered_corpus_train,\n",
    "    val_corpus = filtered_corpus_val,\n",
    "    word_to_index= word_to_index,\n",
    "    max_num_epochs=epochs,\n",
    "    resources = resources,\n",
    "parameter_space = {\n",
    "    \"model\": \"CBOW\",\n",
    "    \"data_sample_name\": \"total_posts\",\n",
    "    \"context_size\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 0.009159544393144516,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 120,\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 0.0006816366988585599,\n",
    "    \"gamma\": 0.9,\n",
    "    \"step_size\": 5,\n",
    "    \"weight_decay\": 0.00011013699407057048,\n",
    "    \"embedding_dim\": 100\n",
    "},\n",
    "    local_path = local_path + \"/tuning_results\"\n",
    "    )\n",
    "\n",
    "plot_loss_curve(\"CBOW\", local_path = local_path, data_sample_name = data_sample_name)\n",
    "\n",
    "# Save the Trained Model as .pth file\n",
    "model_path = \"cbow_model.pth\"\n",
    "model = CBOW(vocab_size=len(word_to_index), embedding_dim=100).to(device)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the Trained Model as a Pickle File\n",
    "pickle_model_path = \"cbow_model.pkl\"\n",
    "with open(pickle_model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"Model saved as a pickle file to {pickle_model_path}\")\n",
    "\n",
    "# Load the Model from .pth file for Future Use\n",
    "model = CBOW(vocab_size=len(word_to_index), embedding_dim=100)\n",
    "model.load_state_dict(torch.load(\"cbow_model.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded from .pth file and ready for inference\")\n",
    "\n",
    "# Load the Model from Pickle File for Future Use\n",
    "with open(pickle_model_path, 'rb') as f:\n",
    "    loaded_pickle_model = pickle.load(f)\n",
    "loaded_pickle_model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded from pickle file and ready for inference\")\n",
    "\n",
    "embeddings_cbow_all_years = extract_embeddings(\n",
    "     model_type = \"CBOW\",\n",
    "     local_path=local_path,\n",
    "     data_sample_name = data_sample_name,\n",
    "     index_to_word = index_to_word\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_42496\\3456389537.py:642: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 74.65 GiB. GPU 0 has a total capacity of 15.99 GiB of which 10.87 GiB is free. Of the allocated memory 3.83 GiB is allocated by PyTorch, and 8.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m val_loader_cbow \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset_cbow, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_dataset_cbow), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m test_loader_cbow \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset_cbow, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_dataset_cbow), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m prediction_val_cbow, true_val_cbow \u001b[38;5;241m=\u001b[39m classify(\n\u001b[0;32m     13\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m val_loader_cbow,\n\u001b[0;32m     15\u001b[0m     index_to_word\u001b[38;5;241m=\u001b[39mindex_to_word,\n\u001b[0;32m     16\u001b[0m     data_sample_name \u001b[38;5;241m=\u001b[39m data_sample_name\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m prediction_test_cbow, true_test_cbow \u001b[38;5;241m=\u001b[39m classify(\n\u001b[0;32m     19\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m test_loader_cbow,\n\u001b[0;32m     21\u001b[0m     index_to_word\u001b[38;5;241m=\u001b[39mindex_to_word,\n\u001b[0;32m     22\u001b[0m     data_sample_name \u001b[38;5;241m=\u001b[39m data_sample_name\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     25\u001b[0m evaluate_classification(\n\u001b[0;32m     26\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     prediction_val \u001b[38;5;241m=\u001b[39m prediction_val_cbow,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m true_test_cbow\n\u001b[0;32m     31\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[10], line 712\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(model_type, dataloader, index_to_word, data_sample_name, include_true_vals)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context_idx, target_idx \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    711\u001b[0m     context_idx, target_idx \u001b[38;5;241m=\u001b[39m context_idx\u001b[38;5;241m.\u001b[39mto(device), target_idx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 712\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m model(context_idx)\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;66;03m# Get the index of the max log-probability\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     _, predicted_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(log_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mCBOW.forward\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     16\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Apply dropout and pass through linear layer\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(combined))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute log probabilities\u001b[39;00m\n\u001b[0;32m     20\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.65 GiB. GPU 0 has a total capacity of 15.99 GiB of which 10.87 GiB is free. Of the allocated memory 3.83 GiB is allocated by PyTorch, and 8.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "context_size = 2\n",
    "\n",
    "val_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_val, context_size)\n",
    "test_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_test, context_size)\n",
    "\n",
    "val_dataset_cbow = Word2VecDataset_cbow(val_pairs_cbow, word_to_index)\n",
    "test_dataset_cbow = Word2VecDataset_cbow(test_pairs_cbow, word_to_index)\n",
    "\n",
    "val_loader_cbow = DataLoader(val_dataset_cbow, batch_size=len(val_dataset_cbow), shuffle=False)\n",
    "test_loader_cbow = DataLoader(test_dataset_cbow, batch_size=len(test_dataset_cbow), shuffle=False)\n",
    "\n",
    "prediction_val_cbow, true_val_cbow = classify(\n",
    "    model_type=\"CBOW\",\n",
    "    dataloader = val_loader_cbow,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name = data_sample_name\n",
    "    )\n",
    "prediction_test_cbow, true_test_cbow = classify(\n",
    "    model_type=\"CBOW\",\n",
    "    dataloader = test_loader_cbow,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name = data_sample_name\n",
    "    )\n",
    "\n",
    "evaluate_classification(\n",
    "    model_type = \"CBOW\",\n",
    "    prediction_val = prediction_val_cbow,\n",
    "    prediction_test = prediction_test_cbow,\n",
    "    y_val = true_val_cbow,\n",
    "    y_test = true_test_cbow\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
