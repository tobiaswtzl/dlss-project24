{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    # Uninstall the current version of 'pyarrow', this is necessary to get ray running on colab\n",
    "    !pip uninstall -y -q pyarrow\n",
    "    \n",
    "    # Install 'pyarrow' version 14.0.1\n",
    "    !pip install pyarrow==14.0.1\n",
    "    \n",
    "    # Install the Ray package with Tune and Debug options\n",
    "    !pip install -q -U ray[tune]\n",
    "    !pip install -q ray[debug]\n",
    "    \n",
    "    # Force a crash to restart the environment (required for package changes to take effect)\n",
    "    import os\n",
    "    os._exit(0)\n",
    "\n",
    "except:\n",
    "    # If not in Google Colab or another error occurs, do nothing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD:  c:\\Users\\wirth\\OneDrive\\Desktop/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True  # Flag to indicate if running in Colab\n",
    "    local_path = \"/content/drive/MyDrive/DLSS/\"  # Set the path to the desired directory on Google Drive\n",
    "    google.colab.drive.mount('/content/drive')  # Mount Google Drive to access files\n",
    "\n",
    "except ImportError:\n",
    "    in_colab = False  # Flag to indicate if running locally\n",
    "    current_wd = os.getcwd()  # Get the current working directory\n",
    "    local_path = os.path.dirname(os.path.dirname(current_wd)) + \"/\"  # Set path to the main directory\n",
    "\n",
    "print(\"CWD: \", local_path)  # Print the current working directory\n",
    "\n",
    "\n",
    "## Import necessary libraries for data processing, modeling, and visualization\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "## Import Ray for distributed computing and hyperparameter tuning\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import Checkpoint\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "## Import metrics for model evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(corpus, vocab_set):\n",
    "    \"\"\"\n",
    "    Filters the corpus by keeping only the words that are present in the provided vocabulary set.\n",
    "\n",
    "    Args:\n",
    "        corpus (list of list of str): A list where each element is a list of words representing a document.\n",
    "        vocab_set (set): A set containing the vocabulary words to be retained in the corpus.\n",
    "\n",
    "    Returns:\n",
    "        list of list of str: The filtered corpus with only words present in `vocab_set`.\n",
    "    \"\"\"\n",
    "    # Iterate through each document in the corpus and retain only the words that are in vocab_set\n",
    "    return [[word for word in doc if word in vocab_set] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_target_pairs_cbow(text, context_size):\n",
    "    \"\"\"\n",
    "    Creates context-target pairs for training a Continuous Bag of Words (CBOW) model.\n",
    "\n",
    "    In a CBOW model, the context consists of words surrounding a target word, and the model \n",
    "    aims to predict the target word given its context.\n",
    "\n",
    "    Args:\n",
    "        text (list of list of str): A list of sentences, where each sentence is a list of words.\n",
    "        context_size (int): The number of words to consider on each side of the target word for the context.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: A list of tuples where each tuple contains a context (list of words) and a target word (str).\n",
    "    \"\"\"\n",
    "    pairs = []  # Initialize an empty list to store context-target pairs\n",
    "\n",
    "    # Iterate over each sentence in the text\n",
    "    for sentence in text:\n",
    "        # Iterate over each word in the sentence, ignoring the edges where the full context can't be formed\n",
    "        for i in range(context_size, len(sentence) - context_size):\n",
    "            # Create the context by taking 'context_size' words before and after the target word\n",
    "            context = sentence[i - context_size:i] + sentence[i + 1:i + context_size + 1]\n",
    "            target = sentence[i]  # The target word is the current word in the sentence\n",
    "            pairs.append((context, target))  # Append the context-target pair to the list\n",
    "\n",
    "    return pairs  # Return the list of context-target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_target_pairs_skipgram(text, context_size):\n",
    "    \"\"\"\n",
    "    Creates context-target pairs for training a Skip-gram model.\n",
    "\n",
    "    In a Skip-gram model, the target word is used to predict the surrounding context words.\n",
    "\n",
    "    Args:\n",
    "        text (list of list of str): A list of sentences, where each sentence is a list of words.\n",
    "        context_size (int): The number of words to consider on each side of the target word for the context.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: A list of tuples where each tuple contains a target word (str) and a context word (str).\n",
    "    \"\"\"\n",
    "    pairs = []  # Initialize an empty list to store context-target pairs\n",
    "\n",
    "    # Iterate over each sentence in the text\n",
    "    for sentence in text:\n",
    "        # Iterate over each word in the sentence\n",
    "        for i in range(len(sentence)):\n",
    "            target = sentence[i]  # The target word is the current word in the sentence\n",
    "\n",
    "            # Define the context window, making sure it doesn't go out of sentence bounds\n",
    "            context = sentence[max(0, i - context_size):i] + sentence[i + 1:i + context_size + 1]\n",
    "            \n",
    "            # Create a pair for each word in the context with the target word\n",
    "            for ctx in context:\n",
    "                pairs.append((target, ctx))  # Append the target-context pair to the list\n",
    "\n",
    "    return pairs  # Return the list of target-context pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset_cbow(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for the CBOW (Continuous Bag of Words) model.\n",
    "\n",
    "    This dataset prepares context-target pairs for training the CBOW model, where\n",
    "    the context is a list of words and the target is a single word.\n",
    "\n",
    "    Args:\n",
    "        pairs (list of tuples): A list of context-target pairs where each context is a list of words and the target is a word.\n",
    "        word_to_index (dict): A dictionary mapping words to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "        context_idxs (torch.Tensor): A tensor of indices representing the context words.\n",
    "        target_idx (torch.Tensor): A tensor representing the index of the target word.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, word_to_index):\n",
    "        self.pairs = pairs  # Store the context-target pairs\n",
    "        self.word_to_index = word_to_index  # Store the word-to-index mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)  # Return the number of pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the context and target from the pairs at the given index\n",
    "        context, target = self.pairs[idx]\n",
    "        \n",
    "        # Convert context words to indices using the word_to_index dictionary\n",
    "        context_idxs = torch.tensor([self.word_to_index[word] for word in context], dtype=torch.long)\n",
    "        \n",
    "        # Convert target word to its index\n",
    "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
    "        \n",
    "        return context_idxs, target_idx  # Return the context and target indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset_skipgram(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for the Skip-gram model.\n",
    "\n",
    "    This dataset prepares target-context pairs for training the Skip-gram model, where\n",
    "    the target is a single word and the context is a single word.\n",
    "\n",
    "    Args:\n",
    "        pairs (list of tuples): A list of target-context pairs where each target and context is a word.\n",
    "        word_to_index (dict): A dictionary mapping words to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "        target_idx (torch.Tensor): A tensor representing the index of the target word.\n",
    "        context_idx (torch.Tensor): A tensor representing the index of the context word.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, word_to_index):\n",
    "        self.pairs = pairs  # Store the target-context pairs\n",
    "        self.word_to_index = word_to_index  # Store the word-to-index mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)  # Return the number of pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the target and context from the pairs at the given index\n",
    "        target, context = self.pairs[idx]\n",
    "        \n",
    "        # Convert target word to its index\n",
    "        target_idx = torch.tensor(self.word_to_index[target], dtype=torch.long)\n",
    "        \n",
    "        # Convert context word to its index\n",
    "        context_idx = torch.tensor(self.word_to_index[context], dtype=torch.long)\n",
    "        \n",
    "        return target_idx, context_idx  # Return the target and context indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model_training(file_name=\"str\", min_count=int, data_sample_name=str):\n",
    "    \"\"\"\n",
    "    Prepares data for training a word embedding model by loading, processing, and filtering the dataset.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the data from a CSV file.\n",
    "    2. Splits the data into training, validation, and test sets.\n",
    "    3. Processes the text data into a corpus of words.\n",
    "    4. Creates a vocabulary based on word frequency and filters out infrequent words.\n",
    "    5. Maps words to indices and vice versa.\n",
    "    6. Filters the corpus based on the created vocabulary.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the CSV file containing the preprocessed comments (without extension).\n",
    "        min_count (int): The minimum number of occurrences a word must have to be included in the vocabulary.\n",
    "        data_sample_name (str): A label to identify the sample of data being processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the filtered training, validation, and test corpora, \n",
    "               the word-to-index mapping, the index-to-word mapping, and the data sample name.\n",
    "    \"\"\"\n",
    "\n",
    "    #### Parameters to choose:\n",
    "    ## Load data from CSV file\n",
    "    comments = pd.read_csv(local_path + f\"/data/preprocessed/{file_name}.csv\")\n",
    "\n",
    "    # Splitting the data into train, validation, and test sets\n",
    "    train_df, temp_df = train_test_split(comments, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 50% val and 50% test\n",
    "\n",
    "    # Combine all comments for generating the vocabulary to avoid missing tokens during training\n",
    "    total_comments_list = comments[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "\n",
    "    # Prepare lists for training, validation, and test sets\n",
    "    train_list = train_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "    val_list = val_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "    test_list = test_df[\"title_and_text_lemmatized\"].dropna().astype(str).tolist()\n",
    "\n",
    "    # Convert sentences into lists of words (corpus)\n",
    "    total_corpus = [doc.split() for doc in total_comments_list]\n",
    "    corpus_train = [doc.split() for doc in train_list]\n",
    "    corpus_val = [doc.split() for doc in val_list]\n",
    "    corpus_test = [doc.split() for doc in test_list]\n",
    "\n",
    "    # Create a vocabulary by counting occurrences of each word in the total corpus\n",
    "    vocab = defaultdict(int)\n",
    "    for sentence in total_corpus:\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # Remove infrequent words from the vocabulary\n",
    "    vocab = {word: count for word, count in vocab.items() if count >= min_count}\n",
    "\n",
    "    # Create word-to-index and index-to-word mappings\n",
    "    word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "    # Create DataFrame from vocabulary for potential analysis or inspection\n",
    "    vocab_df = pd.DataFrame(list(vocab.items()), columns=['Word', 'Count'])\n",
    "\n",
    "    # Create a set of vocabulary words for quick lookup\n",
    "    vocab_set = set(vocab.keys())\n",
    "\n",
    "    # Filter the corpus to include only words present in the vocabulary\n",
    "    filtered_total_corpus = filter_corpus(total_corpus, vocab_set)\n",
    "    filtered_corpus_train = filter_corpus(corpus_train, vocab_set)\n",
    "    filtered_corpus_val = filter_corpus(corpus_val, vocab_set)\n",
    "    filtered_corpus_test = filter_corpus(corpus_test, vocab_set)\n",
    "\n",
    "    # Return the filtered corpora and the word mappings\n",
    "    return filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Class that implements early stopping to halt training when the validation loss stops improving.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
    "    min_delta : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "\n",
    "    Methods:\n",
    "    ----------\n",
    "    __call__(val_loss, model)\n",
    "        Checks if the validation loss has improved and updates the state of early stopping.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of epochs to wait after the last improvement in validation loss before stopping the training.\n",
    "    min_delta : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    counter : int\n",
    "        Counter for the number of epochs since the last improvement.\n",
    "    best_loss : float or None\n",
    "        Best recorded validation loss.\n",
    "    early_stop : bool\n",
    "        Indicating whether training should be stopped early.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience= int, min_delta= float):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        ## for the first training iteration\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            ## check if the loss decreased, if not:\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            ## if loss decrease (more than the defined delta): save model parameters, reset counter and update best loss\n",
    "        else:\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cbow(config, data):\n",
    "    \"\"\"\n",
    "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
    "    data : tuple\n",
    "        Tuple containing training and validation datasets.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict\n",
    "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
    "        and lists of validation and training losses across epochs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
    "\n",
    "    train_pairs = create_context_target_pairs_cbow(filtered_corpus_train, config[\"context_size\"])\n",
    "    val_pairs = create_context_target_pairs_cbow(filtered_corpus_val, config[\"context_size\"])\n",
    "    train_dataset = Word2VecDataset_cbow(train_pairs, word_to_index)\n",
    "    val_dataset = Word2VecDataset_cbow(val_pairs, word_to_index)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "    ## set seed to replicate the model\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    ## empty lists to store loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    ## initialise model\n",
    "    model = CBOW(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
    "\n",
    "    ## loss criterion\n",
    "    loss_criterion = nn.NLLLoss()\n",
    "\n",
    "    ## choose optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    ## adapt learning rate with scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
    "\n",
    "    #### Early Stopper ####\n",
    "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
    "\n",
    "    #### Training ####\n",
    "    ## each epoch iterates through the whole dataset\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        ## train model on training set\n",
    "        model.train()\n",
    "        ## set loss and r2 to zero again so we start fresh\n",
    "        train_loss = 0\n",
    "        ## iterate through batches of the training data (data is the features and target the target)\n",
    "        for context_idxs, target_idx in train_loader:\n",
    "            ## send tensors to gpu\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            ## reset gradient to 0,start fresh again\n",
    "            optimizer.zero_grad()\n",
    "            ## predict target\n",
    "            log_probs = model(context_idxs)\n",
    "            ## caculate loss\n",
    "            loss = loss_criterion(log_probs, target_idx)\n",
    "            ## caculate gradients\n",
    "            loss.backward()\n",
    "            ## update weights\n",
    "            optimizer.step()\n",
    "            ## sum loss for all batches together\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "        #### Validation ####\n",
    "        ## check performance on validation set\n",
    "        model.eval()\n",
    "        ## set loss to zero again so we start fresh\n",
    "        val_loss_sum = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        ## as we test on the validation set, we do not want to update our weights now\n",
    "        with torch.no_grad():\n",
    "            for context_idxs, target_idx in val_loader:\n",
    "                ## send tensors to gpu\n",
    "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "                log_probs = model(context_idxs)\n",
    "                ## caculate loss\n",
    "                loss = loss_criterion(log_probs, target_idx)\n",
    "                ## sum loss for whole epoch\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                correct += (predicted_idx == target_idx).sum().item()\n",
    "                total += context_idxs.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        ## adapt learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
    "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
    "          ##save checkpoint\n",
    "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "\n",
    "          ## report mertrics and save checkpoint\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      },\n",
    "                  checkpoint=Checkpoint.from_directory(\".\")\n",
    "                  )\n",
    "        else:\n",
    "          ##report only metrics\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      }\n",
    "                  )\n",
    "\n",
    "        #### Early stopping ####\n",
    "        # check if loss decreases more than defined threshold\n",
    "        early_stopper(val_loss, model)\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            break\n",
    "\n",
    "    ## last checkpoint\n",
    "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "    ray.train.report(\n",
    "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
    "        checkpoint=Checkpoint.from_directory(\".\")\n",
    "        )\n",
    "\n",
    "    #return train_losses, val_losses, val_r2s\n",
    "    return {\n",
    "        \"loss\": round(early_stopper.best_loss, 2),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"val_loss_list\": val_losses,\n",
    "        \"train_loss_list\": train_losses\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_skipgram(config, data):\n",
    "    \"\"\"\n",
    "    Function that trains a model using the specified configuration and data, implements early stopping based on validation loss improvement, and reports training progress and results to Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    config : dict\n",
    "        Dictionary containing hyperparameters and settings for the model, training, and early stopping.\n",
    "    data : tuple\n",
    "        Tuple containing training and validation datasets.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict\n",
    "        A dictionary containing the final training loss, validation loss, accuracy, the epoch at which training stopped,\n",
    "        and lists of validation and training losses across epochs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    filtered_corpus_train, filtered_corpus_val, word_to_index = data\n",
    "\n",
    "    train_pairs = create_context_target_pairs_skipgram(filtered_corpus_train, config[\"context_size\"])\n",
    "    val_pairs = create_context_target_pairs_skipgram(filtered_corpus_val, config[\"context_size\"])\n",
    "    train_dataset = Word2VecDataset_skipgram(train_pairs, word_to_index)\n",
    "    val_dataset = Word2VecDataset_skipgram(val_pairs, word_to_index)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, generator = torch.Generator().manual_seed(1234))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "    ## set seed to replicate the model\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    ## empty lists to store loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    ## initialise model\n",
    "    model = SkipGram(len(word_to_index), config[\"embedding_dim\"]).to(device)\n",
    "\n",
    "    ## loss criterion\n",
    "    loss_criterion = nn.NLLLoss()\n",
    "\n",
    "    ## choose optimiser\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    ## adapt learning rate with scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
    "\n",
    "    #### Early Stopper ####\n",
    "    early_stopper = EarlyStopping(patience= config[\"patience\"], min_delta = config[\"min_delta\"])\n",
    "\n",
    "    #### Training ####\n",
    "    ## each epoch iterates through the whole dataset\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        ## train model on training set\n",
    "        model.train()\n",
    "        ## set loss and r2 to zero again so we start fresh\n",
    "        train_loss = 0\n",
    "        ## iterate through batches of the training data (data is the features and target the target)\n",
    "        for context_idxs, target_idx in train_loader:\n",
    "            ## send tensors to gpu\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            ## reset gradient to 0,start fresh again\n",
    "            optimizer.zero_grad()\n",
    "            ## predict target\n",
    "            log_probs = model(target_idx)\n",
    "            ## caculate loss\n",
    "            loss = loss_criterion(log_probs, context_idxs)\n",
    "            ## caculate gradients\n",
    "            loss.backward()\n",
    "            ## update weights\n",
    "            optimizer.step()\n",
    "            ## sum loss for all batches together\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "\n",
    "        #### Validation ####\n",
    "        ## check performance on validation set\n",
    "        model.eval()\n",
    "        ## set loss to zero again so we start fresh\n",
    "        val_loss_sum = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        ## as we test on the validation set, we do not want to update our weights now\n",
    "        with torch.no_grad():\n",
    "            for context_idxs, target_idx in val_loader:\n",
    "                ## send tensors to gpu\n",
    "                context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "                log_probs = model(target_idx)\n",
    "                ## caculate loss\n",
    "                loss = loss_criterion(log_probs, context_idxs)\n",
    "                ## sum loss for whole epoch\n",
    "                val_loss_sum += loss.item()\n",
    "\n",
    "                # Get the index of the max log-probability\n",
    "                _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "                correct += (predicted_idx == context_idxs).sum().item()\n",
    "                total += context_idxs.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        ## adapt learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        ## save checkpoints only if loss decreased and the epoch is larger than the patience (to save less checkpoints) but always report metrics to ray\n",
    "        if epoch > 0 and early_stopper.best_loss - config[\"min_delta\"]  > val_loss:\n",
    "          ##save checkpoint\n",
    "          torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "\n",
    "          ## report mertrics and save checkpoint\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      },\n",
    "                  checkpoint=Checkpoint.from_directory(\".\")\n",
    "                  )\n",
    "        else:\n",
    "          ##report only metrics\n",
    "          ray.train.report(\n",
    "                  {\n",
    "                      \"loss\": round(early_stopper.best_loss, 2),\n",
    "                      \"val_loss_list\": val_losses,\n",
    "                      \"train_loss_list\": train_losses,\n",
    "                      \"accuracy\": accuracy\n",
    "                      }\n",
    "                  )\n",
    "\n",
    "        #### Early stopping ####\n",
    "        # check if loss decreases more than defined threshold\n",
    "        early_stopper(val_loss, model)\n",
    "\n",
    "        if early_stopper.early_stop:\n",
    "            break\n",
    "\n",
    "    ## last checkpoint\n",
    "    torch.save(model.state_dict(), \"checkpoint_\" + config[\"model\"] + \".pt\")\n",
    "    ray.train.report(\n",
    "        {\"loss\": round(early_stopper.best_loss, 3), \"epoch\": int(epoch), \"accuracy\": round(accuracy, 3)},\n",
    "        checkpoint=Checkpoint.from_directory(\".\")\n",
    "        )\n",
    "\n",
    "    #return train_losses, val_losses, val_r2s\n",
    "    return {\n",
    "        \"loss\": round(early_stopper.best_loss, 2),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"val_loss_list\": val_losses,\n",
    "        \"train_loss_list\": train_losses\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tuning ####\n",
    "## Custom function to shorten ray file path names\n",
    "def short_dirname(trial) -> str:\n",
    "    \"\"\"\n",
    "    Function that shortens path names created by Ray.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    trial : ray.tune.Trial\n",
    "        The Ray trial object for which the directory name is being created.\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    str\n",
    "        A shortened file path in the format 'trial_<trial_id>'.\n",
    "    \"\"\"\n",
    "    return \"trial_\" + str(trial.trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_parameters(training_function, num_samples, train_corpus, val_corpus, word_to_index, max_num_epochs, parameter_space, resources, local_path):\n",
    "    \"\"\"\n",
    "    Function that tunes the hyperparameters for a DL model using ASHA scheduling and saves the best model and tuning results locally.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    training_function : function\n",
    "        The function used for training the model during hyperparameter tuning.\n",
    "    num_samples : int\n",
    "        The number of hyperparameter samples to try.\n",
    "    train_dataset : object\n",
    "        Training dataset object.\n",
    "    val_dataset : object\n",
    "        Validation dataset object.\n",
    "    max_num_epochs : int\n",
    "        The maximum number of epochs for training each model.\n",
    "    parameter_space : dict\n",
    "        Dictionary defining the hyperparameter search space.\n",
    "    resources : dict\n",
    "        Resources configuration for training.\n",
    "    local_path : str\n",
    "        Local path to save tuning results and best model.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the tuning results, sorted by loss.\n",
    "        \"\"\"\n",
    "\n",
    "    ## because min number of epochs in sampling range is 50\n",
    "    #assert max_num_epochs > 50\n",
    "\n",
    "    ## Hyperparameters to sample from\n",
    "    ## ASHA scheduler to increase efficiency and stop inefficient training configs\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=3,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "\n",
    "    ## tuning function, choose resources\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                training_function,\n",
    "                data = (train_corpus, val_corpus, word_to_index)),\n",
    "                resources= resources\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            trial_dirname_creator=short_dirname\n",
    "        ),\n",
    "        param_space= parameter_space,\n",
    "        run_config = ray.train.RunConfig(storage_path = local_path, name=\"run_\" + datetime.now().strftime(\"%m-%d_%H_%M\"))\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "\n",
    "    #### Best Model ####\n",
    "\n",
    "    ## create folder\n",
    "    os.makedirs(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}', exist_ok=True)\n",
    "\n",
    "    ## get best model\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    ## save info about best model\n",
    "    with open(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_result_info_' + parameter_space[\"model\"] + '.pkl', 'wb') as file:\n",
    "        pickle.dump(best_result, file)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"loss\"]))\n",
    "\n",
    "    ## get path to that best model\n",
    "    best_checkpoint_path = best_result.get_best_checkpoint(metric = \"loss\", mode = \"min\").path + \"/checkpoint_\"+ parameter_space[\"model\"] + \".pt\"\n",
    "    ## save path to model as txt\n",
    "    #with open(local_path + f\"/best_models_{parameter_space[\"folder_ending\"]}//path_best_model_\" + parameter_space[\"model\"] + \".txt\", \"w\") as file:\n",
    "    #    file.write(best_checkpoint_path)\n",
    "\n",
    "    ##save model parameters\n",
    "    best_checkpoint = torch.load(best_checkpoint_path)\n",
    "\n",
    "    ## create new model\n",
    "    if parameter_space[\"model\"] == \"CBOW\":\n",
    "        model_final = CBOW(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "    else:\n",
    "        model_final = SkipGram(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    ## load parameteres of best checkpoint\n",
    "    model_final.load_state_dict(best_checkpoint)\n",
    "\n",
    "    torch.save(model_final.state_dict(), local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/best_model_parameters_' + parameter_space[\"model\"] + '.pt')\n",
    "\n",
    "    #### Tuning Overview ####\n",
    "    ## Get results as df\n",
    "    df_tuning_results = results.get_dataframe()\n",
    "    ## Rename cols\n",
    "    df_tuning_results.columns = [col.replace('config/', '') for col in df_tuning_results.columns]\n",
    "    ## sort by loss\n",
    "    df_tuning_results.sort_values(\"loss\", inplace = True)\n",
    "    ## Save only relevant cols\n",
    "    df_tuning_results = df_tuning_results[['loss', \"accuracy\", \"context_size\", 'lr', 'batch_size', 'epochs',\n",
    "                                           'patience', 'min_delta', \"gamma\", \"step_size\",\n",
    "                                           \"dropout\", 'time_total_s', \"val_loss_list\", \"train_loss_list\"]]\n",
    "    ## Save as csv\n",
    "    df_tuning_results.to_csv(local_path + f'/best_models_{parameter_space[\"data_sample_name\"]}/df_tuning_results' + parameter_space[\"model\"] + '.csv')\n",
    "\n",
    "    return df_tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### Replication ####\n",
    "def load_best_model(model_type = str, local_path = str, data_sample_name = str):\n",
    "    \"\"\"\n",
    "    Function that loads the best model based on the specified model type.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model to load (\"CNN\" or other).\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    torch.nn.Module\n",
    "        The best pre-trained model loaded on the device and ready for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get best config\n",
    "    with open(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl', 'rb') as file:\n",
    "    # Use pickle.dump() to write the data object to file\n",
    "        best_result = pickle.load(file)\n",
    "\n",
    "    #with open(f\"{local_path}/tuning_results/best_models/path_best_model_{model_type}.txt\") as file:\n",
    "    #    path_best_file = file.read()\n",
    "\n",
    "    ## load parameters of best model\n",
    "    best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n",
    "\n",
    "    ## create new model\n",
    "    if model_type == \"CBOW\":\n",
    "        model_final = CBOW(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "    else:\n",
    "        model_final = SkipGram(\n",
    "            vocab_size = len(word_to_index),\n",
    "            embedding_dim = best_result.metrics[\"config\"][\"embedding_dim\"]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    ## load parameteres of best checkpoint\n",
    "    model_final.load_state_dict(best_checkpoint)\n",
    "    ## model into evaluation mode\n",
    "    model_final.eval()\n",
    "    model_final.to(device)\n",
    "\n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(model_type = str, local_path = str, data_sample_name = str):\n",
    "\n",
    "   \"\"\"\n",
    "    Function that plots the training and validation loss curves of the best model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model whose loss curve to plot (\"CNN\" or other).\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "   ## get file with loss data\n",
    "   with open(local_path + f\"/tuning_results/best_models_{data_sample_name}/best_result_info_{model_type}.pkl\", 'rb') as file:\n",
    "       best_result = pickle.load(file)\n",
    "\n",
    "   ## get respective tuning data\n",
    "   val_losses = best_result.metrics[\"val_loss_list\"]\n",
    "   ## i forgot to divide the train loss by n in the training function\n",
    "   ## and repeating that takes 8 hours, so I have to do it like this now\n",
    "   train_losses = best_result.metrics[\"train_loss_list\"]\n",
    "\n",
    "   ## create plot\n",
    "   plt.plot(train_losses, label='Training Loss')\n",
    "   plt.plot(val_losses, label='Validation Loss')\n",
    "   plt.title(f\"{model_type} loss curves\")\n",
    "   plt.xlabel('Epochs')\n",
    "   plt.ylabel('Loss')\n",
    "   plt.legend()\n",
    "   ## save\n",
    "   plt.savefig(local_path + f\"plots/loss_curve_{model_type}_{data_sample_name}.png\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model_type, dataloader, index_to_word, data_sample_name=str, include_true_vals=True):\n",
    "    \"\"\"\n",
    "    Classifies words using a trained CBOW or Skip-gram model and returns the predicted words.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the best trained model based on the provided model type and data sample name.\n",
    "    2. Iterates through the provided data using a DataLoader.\n",
    "    3. For each batch, predicts the target word(s) based on the context (for CBOW) or predicts the context word(s) based on the target (for Skip-gram).\n",
    "    4. Optionally, returns the true values along with the predictions.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): The type of the model used ('CBOW' or 'Skip-gram').\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader object that provides batches of data for classification.\n",
    "        index_to_word (dict): A dictionary mapping indices to words.\n",
    "        data_sample_name (str): The name of the data sample used for loading the model.\n",
    "        include_true_vals (bool): Whether to return the true target/context values along with predictions.\n",
    "\n",
    "    Returns:\n",
    "        If include_true_vals is True:\n",
    "            tuple: Two lists containing predicted words and true words.\n",
    "        If include_true_vals is False:\n",
    "            list: A list of predicted words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the best trained model based on the model type and data sample name\n",
    "    model = load_best_model(model_type, local_path=local_path, data_sample_name=data_sample_name)\n",
    "\n",
    "    predictions, true_vals = [], []  # Initialize lists to store predictions and true values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through the dataloader to get batches of data\n",
    "        for context_idx, target_idx in dataloader:\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            context_idx, target_idx = context_idx.to(device), target_idx.to(device)\n",
    "            \n",
    "            # For CBOW, predict the target word based on the context\n",
    "            if model_type == \"CBOW\":\n",
    "                log_probs = model(context_idx)\n",
    "            else:  # For Skip-gram, predict the context word based on the target\n",
    "                log_probs = model(target_idx)\n",
    "\n",
    "            # Get the index of the word with the highest log-probability (the predicted word)\n",
    "            _, predicted_idx = torch.max(log_probs, dim=1)\n",
    "            \n",
    "            # Convert the predicted indices to words and add them to the predictions list\n",
    "            predictions.extend([index_to_word.get(word_id.item()) for word_id in predicted_idx.cpu().numpy()])\n",
    "\n",
    "            if include_true_vals:\n",
    "                # Convert the true indices to words and add them to the true_vals list\n",
    "                if model_type == \"CBOW\":\n",
    "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in target_idx.cpu().numpy()])\n",
    "                else:\n",
    "                    true_vals.extend([index_to_word.get(word_id.item()) for word_id in context_idx.cpu().numpy()])\n",
    "\n",
    "    # Return predictions and true values if requested\n",
    "    if include_true_vals:\n",
    "        return predictions, true_vals\n",
    "    else:\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model_type, prediction_val, prediction_test, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Function that evaluates a classification model by computing accuracy, recall, precision, and F-score for the training and validation data, and optionally for the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Type of the model being evaluated.\n",
    "    prediction_val : array\n",
    "        Predictions made by the model on the validation dataset.\n",
    "    prediction_test : array\n",
    "        Predictions made by the model on the test dataset.\n",
    "    y_val : array\n",
    "        Actual target values in the validation dataset.\n",
    "    y_test : array\n",
    "        Actual target values in the test dataset.\n",
    "    final_testing : bool, default=False\n",
    "        Flag indicating whether to evaluate the model on the test dataset.\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\n--------------------------\\n{model_type} Classification Evaluation \\n--------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\nValidation set\")\n",
    "    print(\"--------------\")\n",
    "\n",
    "    print(f\"F1 Score: {f1_score(y_true = y_val, y_pred = prediction_val, average = 'micro'):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true = y_val, y_pred = prediction_val):.2f}\")\n",
    "\n",
    "    #print(classification_report(y_true = y_val, y_pred = prediction_val, digits=2, zero_division=0))\n",
    "    print(\"\\nTest set\")\n",
    "    print(\"--------\\n\")\n",
    "    print(f\"F1 Score: {f1_score(y_true = y_test, y_pred = prediction_test, average = 'micro'):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true = y_test, y_pred = prediction_test):.2f}\")\n",
    "\n",
    "    #print(classification_report(y_true = y_test, y_pred = prediction_test, digits=2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model_type=str, local_path=str, data_sample_name=str, index_to_word=dict):\n",
    "    \"\"\"\n",
    "    Extracts word embeddings from a trained model and saves them to a CSV file.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the best trained model based on the provided model type and data sample name.\n",
    "    2. Extracts the word embeddings from the model.\n",
    "    3. Creates a DataFrame containing the embeddings with their corresponding words.\n",
    "    4. Saves the embeddings to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): The type of the model used (e.g., 'cbow', 'skipgram').\n",
    "        local_path (str): The local directory path where the model and embeddings are stored.\n",
    "        data_sample_name (str): The name of the data sample used for training the model.\n",
    "        index_to_word (dict): A dictionary mapping indices to words.\n",
    "\n",
    "    Returns:\n",
    "        embeddings_df (pd.DataFrame): A DataFrame containing the word embeddings with words as the first column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the best trained model based on model type and data sample name\n",
    "    model = load_best_model(model_type=model_type, local_path=local_path, data_sample_name=data_sample_name)\n",
    "    \n",
    "    # Extract the embeddings from the model as a NumPy array\n",
    "    embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "    # Create a DataFrame from the embeddings\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "    \n",
    "    # Insert a column with the corresponding words\n",
    "    embeddings_df.insert(0, 'word', [index_to_word[i] for i in range(len(embeddings_df))])\n",
    "    \n",
    "    # Save the embeddings DataFrame to a CSV file\n",
    "    embeddings_df.to_csv(local_path + f\"data/embeddings/embeddings_{model_type}_{data_sample_name}.csv\")\n",
    "    \n",
    "    return embeddings_df  # Return the embeddings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Continuous bag of words model #####\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embedding layer for word indices\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer for mapping embeddings to vocab size\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Get embeddings for context words\n",
    "        embeds = self.embeddings(context)\n",
    "        # Average embeddings to get a single vector\n",
    "        combined = torch.mean(embeds, dim=1)\n",
    "        # Apply dropout and pass through linear layer\n",
    "        out = self.linear1(self.dropout(combined))\n",
    "        # Compute log probabilities\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        # Define an embedding layer that will learn word representations\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define a linear layer that maps from the embedding space to the vocabulary space\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Define a dropout layer to prevent overfitting during training\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, target):\n",
    "        # Get the embeddings for the target words\n",
    "        embeds = self.embeddings(target)\n",
    "        # Apply dropout to the embeddings and pass them through the linear layer\n",
    "        out = self.linear1(self.dropout(embeds))\n",
    "        # Apply log softmax to the output to get log-probabilities over the vocabulary\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs  # Return the log-probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training\n",
    "filtered_corpus_train, filtered_corpus_val, filtered_corpus_test, word_to_index, index_to_word, data_sample_name = prepare_data_for_model_training(\n",
    "    file_name = \"total_posts\", # train on total_posts\n",
    "    min_count = 40, # set min_count to 40 to reduce vocab size\n",
    "    data_sample_name = \"total_posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the sample size and the maximum number of epochs for training\n",
    "n_samples = 6  # Number of samples to use for tuning (can be adjusted based on needs)\n",
    "epochs = 80  # Maximum number of epochs for training\n",
    "\n",
    "# Define the computational resources to allocate for the tuning process\n",
    "resources = {\"cpu\": 24, \"gpu\": 1}  # Number of CPUs and GPUs to use\n",
    "\n",
    "# Set the device to GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # Print the device being used (either 'cuda' for GPU or 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-22 06:05:30</td></tr>\n",
       "<tr><td>Running for: </td><td>05:41:56.20        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.1/31.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 48.000: -5.915 | Iter 24.000: -5.93 | Iter 12.000: -5.94 | Iter 6.000: -5.96 | Iter 3.000: -6.105<br>Logical resource usage: 24.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  context_size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  embedding_dim</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  gamma</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  min_delta</th><th style=\"text-align: right;\">  patience</th><th style=\"text-align: right;\">  step_size</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_cbow_00f84_00000</td><td>TERMINATED</td><td>127.0.0.1:48784</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">             2</td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">            100</td><td style=\"text-align: right;\">      50</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">0.00429471 </td><td style=\"text-align: right;\">0.000152078</td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.000209604</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">        6037.85 </td><td style=\"text-align: right;\">  5.97</td><td style=\"text-align: right;\"> 0.159981 </td></tr>\n",
       "<tr><td>train_model_cbow_00f84_00001</td><td>TERMINATED</td><td>127.0.0.1:32700</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">      0.3</td><td style=\"text-align: right;\">            500</td><td style=\"text-align: right;\">     140</td><td style=\"text-align: right;\">   0.5 </td><td style=\"text-align: right;\">0.000152302</td><td style=\"text-align: right;\">0.00415651 </td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">   0.000131698</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">        3984.81 </td><td style=\"text-align: right;\">  5.93</td><td style=\"text-align: right;\"> 0.156316 </td></tr>\n",
       "<tr><td>train_model_cbow_00f84_00002</td><td>TERMINATED</td><td>127.0.0.1:44768</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">            500</td><td style=\"text-align: right;\">     120</td><td style=\"text-align: right;\">   0.5 </td><td style=\"text-align: right;\">0.00167671 </td><td style=\"text-align: right;\">0.00620937 </td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.00102048 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         608.013</td><td style=\"text-align: right;\">  6.71</td><td style=\"text-align: right;\"> 0.116948 </td></tr>\n",
       "<tr><td>train_model_cbow_00f84_00003</td><td>TERMINATED</td><td>127.0.0.1:2656 </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">             4</td><td style=\"text-align: right;\">      0.5</td><td style=\"text-align: right;\">           1000</td><td style=\"text-align: right;\">      90</td><td style=\"text-align: right;\">   0.5 </td><td style=\"text-align: right;\">0.00222392 </td><td style=\"text-align: right;\">0.00336192 </td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">   0.00991097 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         419.431</td><td style=\"text-align: right;\">  8.3 </td><td style=\"text-align: right;\"> 0.0478248</td></tr>\n",
       "<tr><td>train_model_cbow_00f84_00004</td><td>TERMINATED</td><td>127.0.0.1:33840</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">             2</td><td style=\"text-align: right;\">      0.5</td><td style=\"text-align: right;\">           1000</td><td style=\"text-align: right;\">      60</td><td style=\"text-align: right;\">   0.9 </td><td style=\"text-align: right;\">0.00282287 </td><td style=\"text-align: right;\">0.00360756 </td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">   0.000174904</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">        8812.31 </td><td style=\"text-align: right;\">  5.86</td><td style=\"text-align: right;\"> 0.17229  </td></tr>\n",
       "<tr><td>train_model_cbow_00f84_00005</td><td>TERMINATED</td><td>127.0.0.1:42784</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">             3</td><td style=\"text-align: right;\">      0.5</td><td style=\"text-align: right;\">           1000</td><td style=\"text-align: right;\">     100</td><td style=\"text-align: right;\">   0.75</td><td style=\"text-align: right;\">0.000433449</td><td style=\"text-align: right;\">0.00413283 </td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">   0.000260873</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         625.272</td><td style=\"text-align: right;\">  6.2 </td><td style=\"text-align: right;\"> 0.135876 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000000)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000001)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000002)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000003)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000004)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000005)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000006)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000007)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000008)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000009)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000010)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000011)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000012)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000013)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000014)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000015)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000016)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000017)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000018)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000019)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000020)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000021)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000022)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000023)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000024)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000025)\n",
      "\u001b[36m(train_model_cbow pid=48784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00000/checkpoint_000026)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000000)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000001)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000002)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000003)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000004)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000005)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000006)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000007)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000008)\n",
      "\u001b[36m(train_model_cbow pid=32700)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00001/checkpoint_000009)\n",
      "\u001b[36m(train_model_cbow pid=2656)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00003/checkpoint_000000)\n",
      "\u001b[36m(train_model_cbow pid=33840)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00004/checkpoint_000000)\n",
      "\u001b[36m(train_model_cbow pid=33840)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00004/checkpoint_000001)\n",
      "\u001b[36m(train_model_cbow pid=33840)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00004/checkpoint_000002)\n",
      "\u001b[36m(train_model_cbow pid=33840)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00004/checkpoint_000003)\n",
      "\u001b[36m(train_model_cbow pid=33840)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00004/checkpoint_000004)\n",
      "\u001b[36m(train_model_cbow pid=42784)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23/trial_00f84_00005/checkpoint_000000)\n",
      "2024-08-22 06:05:30,591\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'c:/Users/wirth/OneDrive/Desktop/tuning_results/run_08-22_00_23' in 0.0090s.\n",
      "2024-08-22 06:05:30,596\tINFO tune.py:1041 -- Total run time: 20516.21 seconds (20516.19 seconds for the tuning loop).\n",
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_37088\\3456389537.py:579: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(best_checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'model': 'CBOW', 'data_sample_name': 'total_posts', 'context_size': 2, 'dropout': 0.5, 'lr': 0.0028228658248271535, 'batch_size': 512, 'epochs': 60, 'patience': 15, 'min_delta': 0.0036075632797252636, 'gamma': 0.9, 'step_size': 10, 'weight_decay': 0.00017490364688456408, 'embedding_dim': 1000}\n",
      "Best trial final validation loss: 5.86\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_37088\\3456389537.py:642: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n"
     ]
    }
   ],
   "source": [
    "## Clear GPU memory to ensure no residual memory is being used from previous operations\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "## Perform hyperparameter tuning\n",
    "tune_parameters(\n",
    "    train_model_cbow,  # Function to train the CBOW model\n",
    "    num_samples=n_samples,  # Number of hyperparameter configurations to try\n",
    "    train_corpus=filtered_corpus_train,  # Training corpus to use for model training\n",
    "    val_corpus=filtered_corpus_val,  # Validation corpus to use for evaluating model performance\n",
    "    word_to_index=word_to_index,  # Dictionary mapping words to their corresponding indices\n",
    "    max_num_epochs=epochs,  # Maximum number of epochs for each hyperparameter configuration\n",
    "    resources=resources,  # Computational resources to allocate (CPU/GPU)\n",
    "    \n",
    "    # Define the hyperparameter space to explore during tuning\n",
    "    parameter_space={\n",
    "        \"model\": \"CBOW\",  # Type of model being tuned\n",
    "        \"data_sample_name\": data_sample_name,  # Name of the data sample\n",
    "        \"context_size\": tune.choice([2, 3, 4, 5]),  # Different context sizes to try\n",
    "        \"dropout\": tune.choice([0.1, 0.2, 0.3, 0.4, 0.5]),  # Dropout rates to prevent overfitting\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-2),  # Learning rate sampled logarithmically between 1e-4 and 1e-2\n",
    "        \"batch_size\": tune.choice([64, 128, 256, 512, 1024]),  # Different batch sizes to try\n",
    "        \"epochs\": tune.choice(list(range(50, 150, 10))),  # Number of epochs, ranging from 50 to 140 in steps of 10\n",
    "        \"patience\": tune.choice([5, 10, 15]),  # Early stopping patience values to try\n",
    "        \"min_delta\": tune.loguniform(0.01, 0.0001),  # Minimum change to qualify as an improvement for early stopping\n",
    "        \"gamma\": tune.choice([0.1, 0.25, 0.5, 0.75, 0.9]),  # Different values for the learning rate decay factor\n",
    "        \"step_size\": tune.choice([5, 10, 20]),  # Step sizes for learning rate scheduler\n",
    "        \"weight_decay\": tune.loguniform(1e-4, 1e-2),  # Weight decay (L2 regularization) values to try\n",
    "        \"embedding_dim\": tune.choice([100, 500, 1000])  # Different embedding dimensions to try\n",
    "    },\n",
    "    \n",
    "    local_path=local_path + \"/tuning_results\"  # Path where the tuning results will be saved\n",
    ")\n",
    "\n",
    "# Plot the loss curve for the CBOW model based on the tuning results\n",
    "plot_loss_curve(\"CBOW\", local_path=local_path, data_sample_name=data_sample_name)\n",
    "\n",
    "# Extract and save the embeddings from the best CBOW model\n",
    "embeddings_cbow_all_years = extract_embeddings(\n",
    "    model_type=\"CBOW\",  # Specify the type of model used (CBOW)\n",
    "    local_path=local_path,  # Path where the model and embeddings are stored\n",
    "    data_sample_name=data_sample_name,  # Name of the data sample\n",
    "    index_to_word=index_to_word  # Dictionary mapping indices back to words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wirth\\AppData\\Local\\Temp\\ipykernel_37088\\3456389537.py:642: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_checkpoint = torch.load(f'{local_path}/tuning_results/best_models_{data_sample_name}/best_model_parameters_{model_type}.pt')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 29.89 GiB. GPU 0 has a total capacity of 15.99 GiB of which 14.42 GiB is free. Of the allocated memory 307.82 MiB is allocated by PyTorch, and 14.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m val_loader_cbow \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset_cbow, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_dataset_cbow), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m test_loader_cbow \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset_cbow, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_dataset_cbow), shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m prediction_val_cbow, true_val_cbow \u001b[38;5;241m=\u001b[39m classify(\n\u001b[0;32m     13\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m val_loader_cbow,\n\u001b[0;32m     15\u001b[0m     index_to_word\u001b[38;5;241m=\u001b[39mindex_to_word,\n\u001b[0;32m     16\u001b[0m     data_sample_name \u001b[38;5;241m=\u001b[39m data_sample_name\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m prediction_test_cbow, true_test_cbow \u001b[38;5;241m=\u001b[39m classify(\n\u001b[0;32m     19\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m test_loader_cbow,\n\u001b[0;32m     21\u001b[0m     index_to_word\u001b[38;5;241m=\u001b[39mindex_to_word,\n\u001b[0;32m     22\u001b[0m     data_sample_name \u001b[38;5;241m=\u001b[39m data_sample_name\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     25\u001b[0m evaluate_classification(\n\u001b[0;32m     26\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBOW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     prediction_val \u001b[38;5;241m=\u001b[39m prediction_val_cbow,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m true_test_cbow\n\u001b[0;32m     31\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[51], line 712\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(model_type, dataloader, index_to_word, data_sample_name, include_true_vals)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m context_idx, target_idx \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    711\u001b[0m     context_idx, target_idx \u001b[38;5;241m=\u001b[39m context_idx\u001b[38;5;241m.\u001b[39mto(device), target_idx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 712\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m model(context_idx)\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;66;03m# Get the index of the max log-probability\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     _, predicted_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(log_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 14\u001b[0m, in \u001b[0;36mCBOW.forward\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, context):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get embeddings for context words\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(context)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Average embeddings to get a single vector\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\Users\\wirth\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 29.89 GiB. GPU 0 has a total capacity of 15.99 GiB of which 14.42 GiB is free. Of the allocated memory 307.82 MiB is allocated by PyTorch, and 14.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Set the context size for the CBOW model\n",
    "context_size = 2\n",
    "\n",
    "# Generate context-target pairs for the validation and test datasets using the CBOW model\n",
    "val_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_val, context_size)\n",
    "test_pairs_cbow = create_context_target_pairs_cbow(filtered_corpus_test, context_size)\n",
    "\n",
    "# Create datasets for validation and test data using the generated pairs\n",
    "val_dataset_cbow = Word2VecDataset_cbow(val_pairs_cbow, word_to_index)\n",
    "test_dataset_cbow = Word2VecDataset_cbow(test_pairs_cbow, word_to_index)\n",
    "\n",
    "# Create DataLoaders for the validation and test datasets\n",
    "# Batch size is set to the length of the dataset to process the entire dataset at once\n",
    "val_loader_cbow = DataLoader(val_dataset_cbow, batch_size=len(val_dataset_cbow), shuffle=False)\n",
    "test_loader_cbow = DataLoader(test_dataset_cbow, batch_size=len(test_dataset_cbow), shuffle=False)\n",
    "\n",
    "# Perform classification on the validation data using the CBOW model\n",
    "prediction_val_cbow, true_val_cbow = classify(\n",
    "    model_type=\"CBOW\",\n",
    "    dataloader=val_loader_cbow,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name=data_sample_name\n",
    ")\n",
    "\n",
    "# Perform classification on the test data using the CBOW model\n",
    "prediction_test_cbow, true_test_cbow = classify(\n",
    "    model_type=\"CBOW\",\n",
    "    dataloader=test_loader_cbow,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name=data_sample_name\n",
    ")\n",
    "\n",
    "# Evaluate the classification results by comparing predictions to the true values\n",
    "evaluate_classification(\n",
    "    model_type=\"CBOW\",\n",
    "    prediction_val=prediction_val_cbow,\n",
    "    prediction_test=prediction_test_cbow,\n",
    "    y_val=true_val_cbow,\n",
    "    y_test=true_test_cbow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clear GPU memory to ensure no residual memory is being used from previous operations\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "## Perform hyperparameter tuning for the Skip-gram model\n",
    "tune_parameters(\n",
    "    train_model_skipgram,  # Function to train the Skip-gram model\n",
    "    num_samples=n_samples,  # Number of hyperparameter configurations to try\n",
    "    train_corpus=filtered_corpus_train,  # Training corpus to use for model training\n",
    "    val_corpus=filtered_corpus_val,  # Validation corpus to use for evaluating model performance\n",
    "    word_to_index=word_to_index,  # Dictionary mapping words to their corresponding indices\n",
    "    max_num_epochs=epochs,  # Maximum number of epochs for each hyperparameter configuration\n",
    "    resources=resources,  # Computational resources to allocate (CPU/GPU)\n",
    "    \n",
    "    # Define the hyperparameter space to explore during tuning\n",
    "    parameter_space={\n",
    "        \"model\": \"skipgram\",  # Specify the model type as Skip-gram\n",
    "        \"data_sample_name\": data_sample_name,  # Name of the data sample\n",
    "        \"context_size\": tune.choice([2, 3, 4, 5]),  # Different context sizes to try\n",
    "        \"dropout\": tune.choice([0.1, 0.2, 0.3, 0.4, 0.5]),  # Dropout rates to prevent overfitting\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-2),  # Learning rate sampled logarithmically between 1e-4 and 1e-2\n",
    "        \"batch_size\": tune.choice([64, 128, 256, 512, 1024]),  # Different batch sizes to try\n",
    "        \"epochs\": tune.choice(list(range(50, 150, 10))),  # Number of epochs, ranging from 50 to 140 in steps of 10\n",
    "        \"patience\": tune.choice([5, 10, 15]),  # Early stopping patience values to try\n",
    "        \"min_delta\": tune.loguniform(0.01, 0.0001),  # Minimum change to qualify as an improvement for early stopping\n",
    "        \"gamma\": tune.choice([0.1, 0.25, 0.5, 0.75, 0.9]),  # Different values for the learning rate decay factor\n",
    "        \"step_size\": tune.choice([5, 10, 20]),  # Step sizes for learning rate scheduler\n",
    "        \"weight_decay\": tune.loguniform(1e-4, 1e-2),  # Weight decay (L2 regularization) values to try\n",
    "        \"embedding_dim\": tune.choice([100, 500, 1000])  # Different embedding dimensions to try\n",
    "    },\n",
    "    \n",
    "    local_path=local_path + \"/tuning_results\"  # Path where the tuning results will be saved\n",
    ")\n",
    "\n",
    "# Plot the loss curve for the Skip-gram model based on the tuning results\n",
    "plot_loss_curve(\"skipgram\", local_path=local_path, data_sample_name=data_sample_name)\n",
    "\n",
    "# Extract and save the embeddings from the best Skip-gram model\n",
    "embeddings_skipgram_all_years = extract_embeddings(\n",
    "    model_type=\"skipgram\",  # Specify the type of model used (Skip-gram)\n",
    "    local_path=local_path,  # Path where the model and embeddings are stored\n",
    "    data_sample_name=data_sample_name,  # Name of the data sample\n",
    "    index_to_word=index_to_word  # Dictionary mapping indices back to words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the context size for the CBOW model\n",
    "context_size = 2\n",
    "\n",
    "# Generate context-target pairs for the validation and test datasets using the skipigram model\n",
    "val_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_val, context_size)\n",
    "test_pairs_skipgram = create_context_target_pairs_skipgram(filtered_corpus_test, context_size)\n",
    "\n",
    "# Create datasets for validation and test data using the generated pairs\n",
    "val_dataset_skipgram = Word2VecDataset_skipgram(val_pairs_skipgram, word_to_index)\n",
    "test_dataset_skipgram = Word2VecDataset_skipgram(test_pairs_skipgram, word_to_index)\n",
    "\n",
    "# Create DataLoaders for the validation and test datasets\n",
    "# Batch size is set to the length of the dataset to process the entire dataset at once\n",
    "val_loader_skipgram = DataLoader(val_dataset_skipgram, batch_size=len(val_dataset_cbow), shuffle=False)\n",
    "test_loader_skipgram = DataLoader(test_dataset_skipgram, batch_size=len(test_dataset_skipgram), shuffle=False)\n",
    "\n",
    "# Perform classification on the validation data using the skipgram model\n",
    "prediction_val_skipgram, true_val_skipgram = classify(\n",
    "    model_type=\"skipgram\",\n",
    "    dataloader = val_loader_skipgram,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name = data_sample_name\n",
    "    )\n",
    "\n",
    "# Perform classification on the test data using the skipgram model\n",
    "prediction_test_skipgram, true_test_skipgram = classify(\n",
    "    model_type=\"skipgram\",\n",
    "    dataloader = test_loader_skipgram,\n",
    "    index_to_word=index_to_word,\n",
    "    data_sample_name = data_sample_name\n",
    "    )\n",
    "\n",
    "# Evaluate the classification results by comparing predictions to the true values\n",
    "evaluate_classification(\n",
    "    model_type = \"skipgram\",\n",
    "    prediction_val = prediction_val_skipgram,\n",
    "    prediction_test = prediction_test_skipgram,\n",
    "    y_val = true_val_skipgram,\n",
    "    y_test = true_test_skipgram\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
